{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMItyxg2a2C2+sZOOO/U9sg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/blob/main/TareaSemana6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![MAIA banner](https://raw.githubusercontent.com/MAIA4361-Aprendizaje-refuerzo-profundo/Notebooks_Tareas/main/Images/Aprendizaje_refuerzo_profundo_Banner_V1.png)\n",
        "\n",
        "# <h1><center>Tarea Tutorial - Semana 6 <a href=\"https://colab.research.google.com/github/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/blob/main/TareaSemana6.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"140\" align=\"center\"/></a></center></h1>\n",
        "\n",
        "<center><h1>PPO y TRPO</h1></center>\n",
        "\n",
        "Este tutorial sigue con el estudio de algoritmos de gradiente de política. Concretamente, se revisarán 2 algoritmos: Trust Region Policy Optimization (TRPO) y Proximal Policy Optimization (PPO). Se utilizará la implementación de ambos algoritmos encontrada dentro de <a href=\"https://stable-baselines3.readthedocs.io/en/master/index.html\">Stable Baselines3</a>, y los usaremos para resolver dos problemas de robótica basados en el entorno de simulación MuJoCo: <a href=\"https://gymnasium.farama.org/environments/mujoco/hopper/\">Hopper</a> y <a href=\"https://gymnasium.farama.org/environments/mujoco/ant/\">Ant</a>. Este notebook tutorial se divide en las siguientes secciones:\n",
        "\n",
        "\n",
        "# Tabla de Contenidos\n",
        "1. [Objetivos de Aprendizaje](#scrollTo=Objetivos_de_Aprendizaje)  \n",
        "2. [Marco Teórico](#scrollTo=Marco_Te_rico)  \n",
        "3. [Instalación de Librerías](#scrollTo=Instalaci_n_de_Librer_as)  \n",
        "4. [Familiarización con los Entornos de Gym](#scrollTo=Familiarizaci_n_con_los_Entornos_de_Gym)  \n",
        "5. [Hopper](#scrollTo=Hopper)  \n",
        "6. [Ant](#scrollTo=Ant)  \n",
        "7. [Reflexiones Finales](#scrollTo=Reflexiones_Finales)  \n",
        "8. [Referencias](#scrollTo=Referencias)"
      ],
      "metadata": {
        "id": "oblzmhF6SCZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objetivos de Aprendizaje  \n",
        "  \n",
        "* Comprender las bases teóricas detrás de algoritmos de gradiente de política basados en optimización restringida.\n",
        "* Solucionar problemas de robótica basados en simulaciones físicas usando los algoritmos de TRPO y PPO.\n",
        "* Comparar las ventajas y desventajas que existen entre TRPO y PPO.\n"
      ],
      "metadata": {
        "id": "k8OPdsC0AxgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Marco Teórico  \n",
        "\n",
        "Antes de explicar el funcionamiento concreto de los algoritmos de Trust Region Policy Optimization (TRPO) y Proximal Policy Optimization (PPO) es importante comprender el concepto de Divergencia de Kullback-Leibler, llamada abreviadamente como Divergencia KL. La Divergencia KL es una métrica que mide cuán diferente es una distribución de probabilidad de otra. Por ejemplo, si consideramos dos distribuciones de probabilidad genéricas $P_1(x)$ y $P_2(x)$, se define la Divergencia KL de la siguiente manera:\n",
        "\n",
        "<center> $ D_{KL} = \\sum_x P_1(x) \\text{log}(\\frac{P_1(x)}{P_2(x)}) $ &emsp;&emsp;&emsp;$(1)$ </center>\n",
        "\n",
        "En la teoría del Aprendizaje por Refuerzo, la Divergencia KL se utiliza para comparar y medir la diferencia que existe entre una política antigua ($\\pi_{old}$) y una política nueva ($\\pi_{new}$). Una Divergencia KL pequeña indica que ambas políticas son parecidas, que no hubo grandes cambios en la actualización, mientras que una Divergencia KL grande indica que ambas políticas son muy diferentes entre sí.\n",
        "\n",
        "En TRPO, si se denota al retorno descontado esperado como $\\eta(\\pi)$, y $L_{\\pi_{old}}(\\pi_{new})$ una aproximación local de $\\eta$, se puede demostrar la siguiente cota:\n",
        "\n",
        "<center> $ \\eta(\\pi_{new}) \\geq L_{\\pi_{old}}(\\pi_{new}) - C \\cdot D_{KL}^{max} (\\pi_{old}, \\pi_{new})$ &emsp;$ \\text{donde} $ &emsp; $C = \\frac{4 \\epsilon \\gamma}{(1-\\gamma)^2} $&emsp;&emsp;&emsp;$(2)$ </center>\n",
        "\n",
        "La Ecuación (2) muestra que mientras que la Divergencia KL se mantenga dentro de un límite, por debajo de un umbral pequeño $\\delta$, se puede asegurar que el rendimiento no decrecerá, garantizando una mejora o por lo menos estabilidad. En pocas palabras, el algoritmo de TRPO busca solucionar el problema de optimización restringida dado por la Ecuación (3):\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{maximize}_\\theta  & \\ \\ L_{\\theta_{old}}(\\theta) \\\\\n",
        "\\text{subject to}  & \\ \\ D_{KL}^{max} (\\theta_{old}, \\theta) \\leq \\delta \\qquad \\text{(3)}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Dado que también se puede demostrar la equivalencia $D_{KL} (\\pi_{\\theta_{old}}, \\pi_{\\theta}) = D_{KL} (\\theta_{old}, \\theta)$. Expandiendo el término $L_{\\theta_{old}}$, se puede reescribir la Ecuación (4). En esta expresión, se utiliza la notación $\\rho$ para definir la frecuencia de visitas y $\\hat{A}$ la ventaja.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{maximize}_{\\theta} \\quad & \\sum_s \\rho_{\\theta_{\\text{old}}}(s) \\sum_a \\pi_{\\theta}(a|s) \\hat{A}_{\\theta_{\\text{old}}}(s, a) \\\\\n",
        "\\text{subject to} \\quad & \\bar{D}^{\\rho^{\\theta_{\\text{old}}}}_{\\mathrm{KL}}(\\theta_{\\text{old}}, \\theta) \\leq \\delta \\qquad \\text{(4)}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "TRPO es en teoría fiable y robusto, pero en la práctica resulta difícil de implementar debido a este problema de optimización restringida, especialmente cuando la arquitectura de las redes nueronales se hace compleja. PPO ofrece una implementación más sencilla e igual o más estable que TRPO.\n",
        "\n",
        "Primero, se define $r_t(\\theta)$ como una razón de probabilidad:\n",
        "\n",
        "<center> $ r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t, s_t)}{\\pi_{\\theta_{old}}(a_t, s_t)} $ &emsp;&emsp;&emsp;$(5) $ </center>\n",
        "\n",
        "Con esta definición, capaz de medir el cambio de probabilidad de una política respecto a la anterior, se puede también definir un objetivo \"sustituto\" de TRPO:\n",
        "\n",
        "<center> $ L(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\frac{\\pi_{\\theta}(a_t \\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)} \\hat{A}_t \\right] = \\hat{\\mathbb{E}}_t \\left[ r_t(\\theta) \\hat{A}_t \\right] $ &emsp;&emsp;&emsp;$(6) $ </center>\n",
        "\n",
        "La maximización del término $L(\\theta)$ en la Ecuación (6) llevaría a una actualización demasiado grande de la política. Por ello, en PPO se considera cómo modificar este objetivo para penalizar cambios que muevan a $r_t(\\theta)$ lejos de 1. La solución es aplicar un <i>clipping</i>, como se muestra en la Ecuación (7).\n",
        "\n",
        "<center> $ L^{\\text{CLIP}}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t,\\ \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right] $ &emsp;&emsp;&emsp;$(7) $ </center>\n",
        "\n",
        "Donde $\\epsilon$ es un hiperparámetro utilizado para modificar el rango de <i>clipping</i>. El primer término dentro de la función de minimización es la función $L(\\theta)$ original de la Ecuación (6), y el segundo término modifica este valor recortando el rango de probabilidad, removiendo el incentivo de mover la razón $r_t(\\theta)$ fuera del rango $[1-\\epsilon,1+\\epsilon]$. Al tomar el mínimo entre el objetivo original y recortado se obtiene un límite inferior más pesimista y estable. De esta manera, los cambios en la razón de probabilidades se ignora cuando el objetivo realmente mejora, y se tiene en cuenta cuando empeora. En resumen, el cambio es penalizado, no se sobreestima el beneficio y se tiene una aproximación pesimista más estable gracias al <i>clipping</i>."
      ],
      "metadata": {
        "id": "9lCj3GovPOcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación de Librerías  \n",
        "\n",
        "Corra el siguiente bloque de código para instalar las librerías requeridas en el tutorial. Se encuentra incluído el módulo oficial de Stable Baselines3 que ejecuta A2C y SAC, y los ambientes de Gymnasium que incluyen Hopper y Ant. Esta celda puede tardar un par de minutos en ejecutarse completamente la primera vez.\n"
      ],
      "metadata": {
        "id": "chWp-3WSPQlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Limpia los registros generados\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Todas las librerías han sido instaladas correctamente.\")"
      ],
      "metadata": {
        "id": "Lt7lZo5UB-Nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8cf65ab-5f6b-4c42-b43c-52a59bc6810a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todas las librerías han sido instaladas correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Familiarización con los Entornos de Gym\n",
        "\n",
        "Los siguientes ambientes de Gymnasium son entornos basados en MuJoCo de Google Deep Mind. MuJoCo (Multi-Joint dynamics with Contact) es un simulador físico utilizado para modelar y estudiar el comportamiento de sistemas dinámicos complejos, muy utilizado en robótica.\n",
        "\n",
        "## Hopper\n",
        "\n",
        "El ambiente de <a href=\"https://gymnasium.farama.org/environments/mujoco/hopper/\">Hopper</a> consiste en una pierna en dos dimensiones que tiene como objetivo moverse hacia adelante modificando el torque de sus motores. Concretamente, la pierna se compone de cuatro partes: un torso en la parte superior, un muslo, una pantorrilla y un pie. Y cuenta con tres motores, uno para el muslo ubicado en la cadera, uno en la rodilla, y uno en el tobillo. Un diagrama esquemático del cuerpo y una imágen del entorno de simulación se muestran en la Figura 1.\n",
        "\n",
        "![Action_space_hopper](https://raw.githubusercontent.com/MAIA4361-Aprendizaje-refuerzo-profundo/Notebooks_Tareas/main/Images/Action_space_hopper.png)\n",
        "\n",
        "<center>Figura 1. Descripción del ambiente de Hopper. [4]</center>\n",
        "\n",
        "Como se ve en la Figura 1, el espacio de acciones está descrito por el control de torque en los tres motores ubicados en las bisagras de la pierna. Los índices de los motores son respectivamente 0, 1 y 2, y el rango de control de cada motor se normaliza entre -1 y 1.\n",
        "\n",
        "Por otro lado, el espacio de observación está descrito por 5 posiciones (<i>qpos</i>) y 6 velocidades (<i>qvel</i>) de las partes individuales del robot.\n",
        "\n",
        "Finalmente, la recompensa es calculada ponderando 3 aspectos clave: la estabilidad del mecanismo por la cual recibe una recompensa de +1 por cada timestep en el cual la pierna no cae de cierta altura, la posición del torso que se usa para evaluar que la pierna avanza, y una penalización de control que castiga cuando se toman acciones muy grandes en los motores.  \n",
        "\n",
        "## Ant\n",
        "\n",
        "El ambiente de <a href=\"https://gymnasium.farama.org/environments/mujoco/ant/\">Ant</a> consiste en un robot cuadrúpedo formado por un torso con forma esférica y cuatro piernas con 2 partes cada una. En total el robot cuenta con 8 motores, y el objetivo de esta \"hormiga\" es moverse hacia adelante (dirección derecha de la simulación).\n",
        "\n",
        "![Action_space_hopper](https://raw.githubusercontent.com/MAIA4361-Aprendizaje-refuerzo-profundo/Notebooks_Tareas/main/Images/Action_space_ant.png)\n",
        "\n",
        "<center>Figura 2. Descripción del ambiente de Ant. [5]</center>\n",
        "\n",
        "De forma similar a Hopper, en Ant el espacio de acciones está dado por el control de torque entre -1 y 1 de los 8 motores del sistema dinámico de acuerdo con los índices de la Figura 2.\n",
        "\n",
        "En este caso, el espacio de observación está descrito por 13 posiciones (<i>qpos</i>) y 14 velocidades (<i>qvel</i>) de las partes del robot, y 78 elementos adicionales relacionados con el centro de masa y fuerzas externas al sistema.\n",
        "\n",
        "La recompensa es calculada ponderando 4 aspectos. Al igual que en Hopper, en Ant se le da una recompensa en cada timestep por conservar la estabilidad, también se recompensa que el robot avance en la dirección deseada, y se penalizan acciones muy grandes. El cuarto elemento involucrado en este ambiente corresponde a un castigo si las fuerzas de contacto externas son demasiado grandes."
      ],
      "metadata": {
        "id": "xdQ5A4KbPSOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETAR ===========================================\n",
        "#\n",
        "\n",
        "# ====================================================="
      ],
      "metadata": {
        "id": "jUyuk4lSmL0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflexiones Finales\n",
        "\n",
        "Teniendo en cuenta los resultados observado en ambos ambientes con PPO y TRPO, reflexione sobre las siguientes preguntas:\n",
        "\n",
        "\n",
        "*   ¿?\n",
        "\n",
        "*   ¿?\n",
        "\n",
        "*   ¿?\n",
        "\n",
        "*   ¿?"
      ],
      "metadata": {
        "id": "G5ZVj1xuPWoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias\n",
        "\n",
        "[1] Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. The MIT Press, second edition.\n",
        "\n",
        "[2] Schulman, J., Levine S., Moritz P., Jordan M. and Abbeel P. (2015). Trust Region Policy Optimization. cite: arXiv:1502.05477\n",
        "\n",
        "[3] Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and O. Klimov. (2017). Proximal Policy Optimization Algorithms. cite: arXiv:1707.06347\n",
        "\n",
        "[4] Gym Documentation, Hopper. `https://gymnasium.farama.org/environments/mujoco/hopper/`\n",
        "\n",
        "[5] Gym Documentation, Ant. `https://gymnasium.farama.org/environments/mujoco/ant/`\n",
        "\n",
        "[6] Stable Baselines3 Contrib Documentation, TRPO. `https://sb3-contrib.readthedocs.io/en/master/modules/trpo.html`\n",
        "\n",
        "[7] Stable Baselines3 Documentation, PPO. `https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html`"
      ],
      "metadata": {
        "id": "HhVGfkzU6KRH"
      }
    }
  ]
}