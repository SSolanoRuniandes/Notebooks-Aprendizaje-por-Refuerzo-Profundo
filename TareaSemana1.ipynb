{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+0MeaKefQdCA+0rCsUCas",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/blob/main/TareaSemana1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![MAIA banner](https://raw.githubusercontent.com/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/main/Images/Aprendizaje_refuerzo_profundo_Banner_V1.png)\n",
        "\n",
        "# <h1><center>Tarea Tutorial - Semana 1 <a href=\"https://colab.research.google.com/github/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/blob/main/TareaSemana1.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"140\" align=\"center\"/></a></center></h1>\n",
        "\n",
        "<center><h1>Aproximación de Funciones</h1></center>\n",
        "\n",
        "En este tutorial aprenderás acerca de las ventajas que tiene utilizar aproximación de funciones en problemas de aprendizaje por refuerzo profundo. Ilustraremos el proceso con el problema de <a href=\"https://gymnasium.farama.org/environments/classic_control/cart_pole/\">Cart Pole</a>, incluido en las librerías de Gym, e implementaremos redes nuronales con el framework <a href=\"https://github.com/inarikami/keras-rl2/blob/master/examples/dqn_cartpole.py\">keras-rl2</a>, que ya incluye varios algoritmos de aprendizaje por refuerzo. El objetivo principal será comparar el desempeño de distintos algoritmos en su versión tabular y su versión con redes neuronales. Este notebook tutorial se divide en las siguientes secciones:\n",
        "\n",
        "\n",
        "# Tabla de Contenidos\n",
        "1. [Objetivos de Aprendizaje](#scrollTo=Objetivos_de_Aprendizaje)  \n",
        "2. [Marco Teórico](#scrollTo=Marco_Te_rico)  \n",
        "3. [Instalación de Librerías](#scrollTo=Instalaci_n_de_Librer_as)  \n",
        "4. [Familiarización con el Entorno de Gym](#scrollTo=Familiarizaci_n_con_el_Entorno_de_Gym)  \n",
        "5. [Métodos Tabulares](#scrollTo=M_todos_Tabulares)  \n",
        "6. [Métodos con Redes Neuronales](#scrollTo=M_todos_con_Redes_Neuronales)  \n",
        "7. [Comparación](#scrollTo=Comparaci_n)  \n",
        "8. [Conclusiones](#scrollTo=Conclusiones)  \n",
        "9. [Referencias](#scrollTo=Referencias)"
      ],
      "metadata": {
        "id": "oblzmhF6SCZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objetivos de Aprendizaje  \n",
        "  \n",
        "* Implementar métodos de aprendizaje por refuerzo de forma existosa con redes neuronales (Deep SARSA y Deep Q-Networks).\n",
        "* Familiarizarse con los entornos de simulación de Gym.\n",
        "* Identificar las ventajas de implementar aproximación funciones en comparación a utilizar una versión tabular de los algoritmos.\n"
      ],
      "metadata": {
        "id": "k8OPdsC0AxgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Marco Teórico  \n",
        "En el aprendizaje por refuerzo hay dos grandes familias de métodos para encontrar políticas de comportamiento óptimas: los métodos tabulares y los métodos por aproximación de funciones. Entre los métodos tabulares algunos de los algoritmos más utilizados son los de SARSA y Q-Learning. En la aproximación de funciones mediante redes neuronales estos dos métodos encuentran su equivalente en la forma de Deep SARSA y Deep Q-Networks (DQN).\n",
        "\n",
        "\n",
        "![Interaccion_agente_ambiente](https://raw.githubusercontent.com/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/main/Images/Interaccion_agente_ambiente.png)\n",
        "\n",
        "<center>Figura 1. Interacción de un agente con el ambiente en un problema de aprendizaje por refuerzo. [1]</center>\n",
        "\n",
        "\n",
        "Recordemos que un ambiente en un problema de aprendizaje por refuerzo está definido por estados, acciones y recompensas. En casa estado $S_t$ el agente toma una acción $A_t$, y tiene cierta probabilidad de pasar a otro estado $S_{t+1}$ y recibir una recompensa $R_{t+1}$. El objetivo del aprendizaje por refuerzo es hallar una política de comportamiento tal que en cada estado el agente escoja la mejor acción posible para maximizar el retorno obtenido a largo plazo. Precisamente, esto es lo que indica la función de valor de parejas estado-acción $Q(S_t,A_t)$. La función $Q$ nos dice cuál es el retorno esperado si es un estado $S_t$ se toma una acción $A_t$ siguiendo una política específica $\\pi$. SARSA y Q-Learning son métodos que iterativamente calculan esta función $Q$ de acuerdo con las siguientes reglas de actualización:\n",
        "\n",
        "<center> $Q(S_t,A_t) \\ ← \\ Q(S_t,A_t)+α[R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]$ &emsp;&emsp;&emsp;$(1)$ </center>\n",
        "\n",
        "<center> $Q(S_t,A_t) \\ ← \\ Q(S_t,A_t)+α[R_{t+1}+\\gamma \\underset{a}{\\max} Q(S_{t+1},a)-Q(S_t,A_t)]$ &emsp;&emsp;&emsp;$(2)$ </center>\n",
        "\n",
        "La Ecuación (1) es la regla de actualización que usa SARSA y la Ecuación (2) es la regla de actualización que utiliza Q-Learning. Note que la única diferencia entre ambas expresiones es un término de maximización sobre acciones que aparece en la Ecuación (2). Esto deriva de que SARSA es un algoritmo <i>on-policy</i> mientras que Q-Learning es <i>off-policy</i>. Esto quiere decir que SARSA genera un comportamiento en el ambiente con la misma política que mejora, lo cual tiene como limitación que debe mantener exploración (<i>soft</i>). Al contrario, en Q-Learning la política que se mejora es distinta a la política que genera el comportamiento, por lo cual se puede hacer (<i>greedy</i>). Los macroalgoritmos correspondientes se muestran en la Figura 2 y en la Figura 3:\n",
        "\n",
        "\n",
        "![SARSA_tabular](https://raw.githubusercontent.com/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/main/Images/SARSA_tabular.png)\n",
        "\n",
        "<center>Figura 2. Algoritmo de SARSA para control de política (versión tabular). [1]</center>\n",
        "\n",
        "![QLearning_tabular](https://raw.githubusercontent.com/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/main/Images/QLearning_tabular.png)\n",
        "\n",
        "\n",
        "<center>Figura 3. Algoritmo de Q-Learning para control de política (versión tabular). [1]</center>\n",
        "\n",
        "Note que el problema de la versión tabular es que se necesita estimar $Q(S_t,A_t)$ para cada pareja estado-acción. Cuando el problema es muy grande, es decir, que tiene muchos estados y acciones posibles, hay muchas entradas en la tabla que deben actualizarse. Esto no solo ocupa un gran espacio en memoria, sino que también complica la actualización de muchas parejas estado-acción si la exploración no es apropiada y puede demandar una cantidad demasiado grande de cálculos, recursos computacionales y tiempo.  \n",
        "\n",
        "Por otro lado, en la versión de aproximación de funciones de estos algoritmos, primero hay que definir qué tipo de aproximación se utilizará. En [1] aparecen muchas opciones para aproximar las funciones de valor, entre las cuales se encuentran una versión de parámetros lineales, polinomios, codificación, entre otros. No obstante, la alternativa más utilizada actualmente y probablemente la más útil y robusta, es el uso de redes neuronales profundas. Las redes neuronales permiten estimar funciones modificando la salida a partir de una entrada utilizando pesos asociados a cada neurona y funciones no lineales que, combinadas, pueden reproducir casi cualquier tipo de función si se utilizan suficientes neuronas (y de ahí el término profundas).  \n",
        "\n",
        "Fue en el año 2013 que Mnih et al. [2] publicaron un paper en el cual se detalla la implementación de redes neuronales profundas para la ejecución del algoritmo de Q-Learning en el aprendizaje de juegos de Atari. En su momento, este paper impuso un nuevo estado del arte, ya que fue la primera vez que se logró trasladar la ventaja de usar redes neuronales que aprendieran de datos sensoriales crudos, como los pixeles de una imágen, al campo de aprendizaje por refuerzo con éxito. En la Figura 3 se muestra el algoritmo propuesto de Deep Q-Networks o DQN:\n",
        "\n",
        "![DQN](https://raw.githubusercontent.com/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/main/Images/DQN.png)\n",
        "\n",
        "\n",
        "<center>Figura 4. Algoritmo de DQN para control de política. [2]</center>\n",
        "\n",
        "Por otro lado, Deep SARSA es un algoritmo que, aunque utiliza redes neuronales profundas para realizar un estimativo de la función de valor, no tiene un almacenamiento en memoria de las experiencias observadas, que es una de las ventajas de DQN señaladas en [2], y también se caracteriza por ser <i>on-policy</i>."
      ],
      "metadata": {
        "id": "9lCj3GovPOcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Instalación de Librerías  \n",
        "\n",
        "Corra el siguiente bloque de código para instalar las librerías requeridas en el tutorial. Se encuentran incluídos módulos oficiales que ejecutan agentes de Deep SARSA y DQN, y el ambiente de <i>Cart-Pole</i> de <i>Gym</i>.\n"
      ],
      "metadata": {
        "id": "chWp-3WSPQlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Descarga librerías no incluidas en Colab usando pip\n",
        "!pip install tensorflow==2.12 keras==2.12 keras-rl2 #instala versión de tensorflow compatible y keras-rl2\n",
        "!pip install gymnasium #gym\n",
        "!pip install renderlab #para renderizar gym\n",
        "!pip install ale-py #ale-py\n",
        "\n",
        "#Importa estas librerías\n",
        "import gym #importa gymnasyum\n",
        "import numpy as np #importa numpy\n",
        "import tensorflow as tf #importa tensorflow\n",
        "\n",
        "from keras.models import Sequential #de keras importa el modelo secuencial de redes\n",
        "from keras.layers import Dense, Activation, Flatten, Input #de keras importa tipos de capas de la red\n",
        "from tensorflow.keras.optimizers import Adam #de keras importa Adam para solucionar el gradiente\n",
        "from rl.agents.dqn import DQNAgent #de keras rl 2 importa el agente de de DQN\n",
        "from rl.agents.sarsa import SARSAAgent #de keras rl 2 importa el agente de Deep SARSA\n",
        "from rl.policy import BoltzmannQPolicy #de keras rl2 importa la política de Boltzmann Q\n",
        "from rl.memory import SequentialMemory #de keras rl2 importa memoria secuencial\n",
        "\n",
        "import gymnasium as gym #importa la libreria de gym con las simulaciones\n",
        "import renderlab #importa renderlab para los videos\n",
        "\n",
        "#Importa otras librerías básicas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "#Limpia los registros generados\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "Lt7lZo5UB-Nh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Familiarización con el Entorno de Gym  \n",
        "Contenido de la sección 4.  \n"
      ],
      "metadata": {
        "id": "xdQ5A4KbPSOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_prueba_1 = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "env_render_1 = renderlab.RenderFrame(env_prueba_1, \"./output\")\n",
        "obs , info =env_render_1.reset()\n",
        "\n",
        "terminated = False\n",
        "truncated = False\n",
        "total_reward=0\n",
        "while not (terminated or truncated):\n",
        "  action=0\n",
        "  obs, reward, terminated, truncated , info = env_render_1.step(action)\n",
        "  total_reward+=reward\n",
        "\n",
        "print(\"Total Reward in one episode: \",total_reward)\n",
        "\n",
        "\n",
        "env_render_1.play()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "2CB_sJpPOoJZ",
        "outputId": "e6c3dbaa-bba1-476e-b8fe-1877d7da4a59"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reward in one episode:  10.0\n",
            "Moviepy - Building video temp-{start}.mp4.\n",
            "Moviepy - Writing video temp-{start}.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rt:   0%|          | 0/12 [00:00<?, ?it/s, now=None]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file ./output/1741848699.664485.mp4, 720000 bytes wanted but 0 bytes read,at frame 11/12, at time 0.37/0.37 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "                                                   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready temp-{start}.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls  >\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Métodos Tabulares  \n",
        "\n",
        "Para el análisis de los métodos tabulares, se suministra como base la Clase <i>Learning_CartPole</i>, la cual ya implementa discretización del espacio de estados y acciones, los algoritmos de SARSA y Q-Learning, y métodos para renderizar el ambiente y hacer un análisis de la recompensa por episodio de entrenamiento.\n"
      ],
      "metadata": {
        "id": "MbY4Th6pPUbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Learning_CartPole(object):\n",
        "    def __init__(self,gamma,alpha,epsilon,env_name='CartPole-v1',divisions_per_variable=(10,10,10,10)) -> None:\n",
        "        self.gamma=gamma #factor de descuento\n",
        "        self.alpha=alpha #tamaño de paso de aprendizaje\n",
        "        self.epsilon=epsilon #epsilon para políticas epsilon-greedy\n",
        "        self.env = gym.make(env_name, render_mode = \"rgb_array\") #crea un ambiente\n",
        "        self.env_render = renderlab.RenderFrame(self.env, \"./output\") #crea un ambiente por separado para renderizar\n",
        "        self.limits_space_state=[(-4.8,4.8),(-2,2),(-0.418,0.418),(-2,2)] #límites de los estados\n",
        "        self.divisions_per_variable=divisions_per_variable #discretizacion\n",
        "        self.TargetPolicy={} #inicializa política objetivo\n",
        "        self.BehaviorPolicy={} #inicializa política de comportamiento\n",
        "        self.Q={} #inicializa tabla de Q\n",
        "        self.bins = [self.create_bins(self.limits_space_state[i],self.divisions_per_variable[i]) for i in range(4)]\n",
        "\n",
        "        self.states=[]\n",
        "\n",
        "        for position in self.bins[0]:\n",
        "          for velocity in self.bins[1]:\n",
        "            for angle in self.bins[2]:\n",
        "              for a_vel in self.bins[3]:\n",
        "                self.states.append((position,velocity,angle,a_vel)) #crea un estado definiendo una tupla de 4 elementos\n",
        "\n",
        "\n",
        "\n",
        "    def create_bins(self,i,num):\n",
        "        return np.arange(num+1)*(i[1]-i[0])/num+i[0]\n",
        "\n",
        "    def discretize_bins(self, x):\n",
        "        r = tuple((np.digitize(x[i], self.bins[i]) - 1) if x[i] > self.bins[i][-1] else np.digitize(x[i], self.bins[i]) for i in range(4))\n",
        "        # print(\"r:\",r)\n",
        "        return (self.bins[0][r[0]], self.bins[1][r[1]], self.bins[2][r[2]], self.bins[3][r[3]])\n",
        "\n",
        "    def PolicyGreedyUpdate(self,state):\n",
        "      #Esta función actualiza la política target en un único estado haciéndola epsilon-greedy con respecto al estimativo de Q\n",
        "       #Parámetro state: estado donde se actualiza la política haciéndola epsilon-greedy\n",
        "       pi_policy={0:0,1:0}  #inicializa ambas probabilidades en epsilon/2\n",
        "       pi_policy[max(self.Q[state],key=self.Q[state].get)]=1 #para el índice de acción a que maximiza Q(s,a) la probabilidad cambia a 1-epsilon+epsilon/2\n",
        "       self.TargetPolicy[state]=pi_policy #se actualiza la política en estado state\n",
        "\n",
        "    def PolicyEpsilonGreedyUpdate(self,state):\n",
        "      #Esta función hace a la política greedy con respecto al estimativo de Q en todos los estados\n",
        "       pi_policy={0:self.epsilon/2,1:self.epsilon/2} #calcula prob en el estado peor\n",
        "       pi_policy[max(self.Q[state],key=self.Q[state].get)]=1-self.epsilon+self.epsilon/2 #calcula prob en el otro estado\n",
        "       self.TargetPolicy[state]=pi_policy #guarda en el estado las probabilidades\n",
        "\n",
        "    def CreateGreedyPolicy(self):\n",
        "       for s in self.states:\n",
        "          self.PolicyGreedyUpdate(s)\n",
        "\n",
        "    def CreateEpsilonGreedyPolicy(self):\n",
        "      #Esta función hace a la política epsilon-greedy con respecto al estimativo de Q en todos los estados\n",
        "      for s in range(len(self.states)): #un ciclo recorre todos los estados\n",
        "          self.PolicyEpsilonGreedyUpdate(self.states[s])  #en cada estado se llama a la función PolicyEpsilonGreedyUpdate\n",
        "\n",
        "    def ChooseEpsilonGreedyAction(self,state,q):\n",
        "      #Esta función escoge qué acción tomar en caso que sea aleatorio según una pol´pitica epsilon-greedy\n",
        "      #Parámetro state: El estado state donde debe decidir una acción\n",
        "      #Parámetro q: El estimativo de Q que se tiene en ecuenta para tomar la decisión (Integer 0 o 1)\n",
        "      #Return: La acción que toma A\n",
        "\n",
        "      aleatorio=random.uniform(0, 1) #genera un número aleatorio entre 0 y 1\n",
        "      mejor=max(q[state], key=q[state].get) #obtiene el indice de la mejor acción de acuerdo con q\n",
        "      otras=[0,1] #lista de posibles acciones\n",
        "      if(aleatorio>=self.epsilon): #si el número escogido es mayor a epsilon\n",
        "        A=mejor  #toma la mejor acción\n",
        "      else: ##si el número generado es menor a epsilon\n",
        "        A=random.choice(otras)  #escoge  aleatoriamente de la lista de acciones\n",
        "      return A #retorna un integer 0 o 1\n",
        "\n",
        "    def SARSA(self,number_of_episodes):\n",
        "      #Algoritmo de SARSA on-policy\n",
        "      #Parámetro number_of_episodes: Integer mayor a 0 que indica cuántos episodios generar para usar como muestra\n",
        "      #Return: una lista con el historial de recompensas por episodio\n",
        "\n",
        "      self.Q={s:{0:0,1:0} for s in self.states} #inicializa Q\n",
        "      training_history=[] #crea una lista vacía\n",
        "      for _ in range(number_of_episodes): #itera el número de episodios\n",
        "        obs , info=self.env.reset() #reinicia el ambiente y ve el estado\n",
        "        S=self.discretize_bins(obs) #discretiza los estados\n",
        "        A=self.ChooseEpsilonGreedyAction(S,self.Q) #elige acción tipi epsilon greedy\n",
        "        terminated = False #inicializa condición de while\n",
        "        truncated = False #inicializa condición de while\n",
        "        r=0 #recompensa inicial\n",
        "        while not (terminated or truncated): #si pierde o trucando en 500\n",
        "          obs, rew, terminated, truncated , info = self.env.step(A) #toma A, observa recompensa, siguiente estado\n",
        "          next_state = self.discretize_bins(obs) #discretiza la observación dentro de los estados de la tabla\n",
        "          R=rew #extrae variable\n",
        "          r=r+R #suma recompensa al total\n",
        "          Aprima=self.ChooseEpsilonGreedyAction(next_state,self.Q) #selecciona otra acción\n",
        "          self.Q[S][A]=self.Q[S][A]+self.alpha*(R+self.gamma*self.Q[next_state][Aprima]-self.Q[S][A]) #ecuación 1\n",
        "          S=next_state #siguiente estado\n",
        "          A=Aprima #siguiente accion\n",
        "        training_history.append(r) #por espisodio guarda la recompensa final\n",
        "      return training_history #retorna lista con recompensas por espisodio\n",
        "\n",
        "\n",
        "    def QLearning(self,number_of_episodes):\n",
        "      #Algoritmo de Q-Learning off-policy\n",
        "      #Parámetro number_of_episodes: Integer mayor a 0 que indica cuántos episodios generar para usar como muestra\n",
        "      #Return: una lista con el historial de recompensas por episodio\n",
        "\n",
        "      self.Q={s:{0:0,1:0} for s in self.states} #inicializa el estimativo de Q en 0 para todas las parejas estado-acción\n",
        "      training_history=[]  #crea una lista vacía\n",
        "      for _ in range(number_of_episodes): #itera el número de episodios\n",
        "        obs , info=self.env.reset()  #reinicia el ambiente y ve el estado\n",
        "        # print(obs)\n",
        "        terminated = False #inicializa condición de while\n",
        "        truncated = False #inicializa condición de while\n",
        "        S=self.discretize_bins(obs) #discretiza en los estados de la tabla\n",
        "        # print(S)\n",
        "        r=0 #inicializa recompensa\n",
        "        while not (terminated or truncated): #si pierde o trucando en 500\n",
        "          A=self.ChooseEpsilonGreedyAction(S,self.Q) #elige acción tipi epsilon greedy\n",
        "          obs, rew, terminated, truncated , info = self.env.step(A)  #toma A, observa recompensa, siguiente estado\n",
        "          # print(obs)\n",
        "          next_state=self.discretize_bins(obs)  #discretiza la observación dentro de los estados de la tabla\n",
        "          # print(next_state)\n",
        "          Sprima=next_state\n",
        "          R=rew #Extrae variable\n",
        "          r=r+R #suma recompensa al total\n",
        "          self.Q[S][A]=self.Q[S][A]+self.alpha*(R+self.gamma*self.Q[Sprima][max(self.Q[Sprima], key=self.Q[Sprima].get)]-self.Q[S][A]) #ecuación 2\n",
        "          S=Sprima #siguiente estado\n",
        "        training_history.append(r)  #por espisodio guarda la recompensa final\n",
        "      return training_history #retorna lista con recompensas por espisodio\n",
        "\n",
        "\n",
        "    def ExecutePolicy(self):\n",
        "        #Esta función ejecuta la polític actualmente en Target dentro del ambiente de simulación\n",
        "\n",
        "        obs , info =self.env.reset() #reinicia el ambiente\n",
        "        S=self.discretize_bins(obs) #discretiza estados\n",
        "        cumulative_gamma=1 #factor de descuento\n",
        "        R=0 #en el tiempo inicial la reocmpensa es 0\n",
        "        t=0\n",
        "        terminated = False #inicializa condición de while\n",
        "        truncated = False #inicializa condición de while\n",
        "        while not (terminated or truncated): #si pierde o trucando en 500\n",
        "          left_percentage=self.TargetPolicy[S][0] #probabilidad de escoger izquierda según pi\n",
        "          rigth_percentage=self.TargetPolicy[S][1] #probabilidad de escoger derecha según pi\n",
        "          if (left_percentage==1): #caso determinístico izquierda\n",
        "              action=0\n",
        "          elif (rigth_percentage==1): #caso determinístico derecha\n",
        "              action=1\n",
        "          else: #si no\n",
        "              decision=random.uniform(0,1) #genera un número aleatorio entre 0 y 1\n",
        "              if (decision<=left_percentage): #compara para elegir\n",
        "                action=0 #izq\n",
        "              else:\n",
        "                action=1 #der\n",
        "          t=t+1 #suma tiempo\n",
        "          obs, rew, terminated, truncated , info = self.env.step(action) #recopila la observación\n",
        "          next_state=self.discretize_bins(obs) #discretización\n",
        "          Sprima=next_state #\n",
        "          R=R+rew*cumulative_gamma #aplica descuento\n",
        "          cumulative_gamma=cumulative_gamma*self.gamma #halla retorno\n",
        "          S=Sprima #siguiente estado\n",
        "        return (R,t)\n",
        "\n",
        "    def ExecuteExperiment(self,number_of_episodes):\n",
        "      #Esta función ejecuta un número de experimentos y retorna cuántos episodios hizo y las recompensas\n",
        "      #Parámetro number_of_episodes: Integer mayor a 0 que indica cuántos episodios generar para usar como muestra\n",
        "      #Return: tupla con los episodios y lista de rewards\n",
        "\n",
        "      episodes=np.zeros(number_of_episodes) #inicializa un array en ceros con el número de episodios a correr\n",
        "      rewards=np.zeros(number_of_episodes) #inicializa array en ceros con el número de episodios a correr\n",
        "      for i in range(number_of_episodes): #ejecuta el número de episodios\n",
        "        R,t=self.ExecutePolicy() #llama a ejecutar política, observa el R y t\n",
        "        episodes[i]=i+1 #suma un episodio\n",
        "        rewards[i]=R #añade R a la lista\n",
        "      return (episodes,rewards) #retorna episodios y recompensas\n",
        "\n",
        "    def PlayVideo(self):\n",
        "        #Esta función se crea para mostrar el render de una prueba\n",
        "\n",
        "        obs , info =self.env_render.reset()\n",
        "        S=self.discretize_bins(obs)\n",
        "        cumulative_gamma=1\n",
        "        R=0\n",
        "        t=0\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        while not (terminated or truncated):\n",
        "          left_percentage=self.TargetPolicy[S][0]\n",
        "          rigth_percentage=self.TargetPolicy[S][1]\n",
        "          if (left_percentage==1):\n",
        "              action=0\n",
        "          elif (rigth_percentage==1):\n",
        "              action=1\n",
        "          else:\n",
        "              decision=random.uniform(0,1)\n",
        "              if (decision<=left_percentage):\n",
        "                action=0\n",
        "              else:\n",
        "                action=1\n",
        "          t=t+1\n",
        "          obs, rew, terminated, truncated , info = self.env_render.step(action)\n",
        "          next_state=self.discretize_bins(obs)\n",
        "          Sprima=next_state\n",
        "          R=R+rew*cumulative_gamma\n",
        "          cumulative_gamma=cumulative_gamma*self.gamma\n",
        "          S=Sprima\n",
        "        print(\"Reward: \",R)\n",
        "        self.env_render.play()\n",
        "\n",
        "\n",
        "\n",
        "    def Plot_LearningCurve(self,method,number_of_experiments,number_of_episodes,supt):\n",
        "      #Esta función toma los datos de SARSA y Q-Learning y grafica promedio, mínimo, máximo\n",
        "\n",
        "      experiments = []\n",
        "      i=1\n",
        "      if method=='SARSA':\n",
        "        for _ in range(number_of_experiments):\n",
        "          exp = self.SARSA(number_of_episodes)\n",
        "          experiments.append(exp)\n",
        "          #print(i)\n",
        "          i=i+1\n",
        "      else:\n",
        "        for _ in range(number_of_experiments):\n",
        "          exp = self.QLearning(number_of_episodes)\n",
        "          experiments.append(exp)\n",
        "          #print(i)\n",
        "          i=i+1\n",
        "\n",
        "      mean_rewards=[0] * number_of_episodes\n",
        "      max_rewards=[0] * number_of_episodes\n",
        "      min_rewards=[600] * number_of_episodes\n",
        "      for i in range(number_of_experiments):\n",
        "        for j in range(number_of_episodes):\n",
        "          mean_rewards[j]=mean_rewards[j]+experiments[i][j]\n",
        "          if(experiments[i][j]<min_rewards[j]):\n",
        "            min_rewards[j]=experiments[i][j]\n",
        "          if(experiments[i][j]>max_rewards[j]):\n",
        "            max_rewards[j]=experiments[i][j]\n",
        "      mean_rewards = [n * (1/len(experiments)) for n in mean_rewards]\n",
        "\n",
        "      listas = {\n",
        "          \"x\": [j for j in range(number_of_episodes)],\n",
        "          \"Promedio\":mean_rewards,\n",
        "          \"min\": min_rewards,\n",
        "          \"max\": max_rewards,\n",
        "      }\n",
        "      df = pd.DataFrame(listas)\n",
        "\n",
        "      fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "      axs[0].grid(alpha=0.3)\n",
        "      axs[0].plot(mean_rewards, color='red', label=\"Promedio\")\n",
        "      axs[0].set_ylabel('Recompensa obtenida (Duración del episodio)')\n",
        "      axs[0].set_xlabel('Número de episodios')\n",
        "      axs[0].legend()\n",
        "\n",
        "      axs[1].grid(alpha=0.3)\n",
        "      axs[1].plot(mean_rewards, color='red', label=\"Promedio\")\n",
        "      axs[1].fill_between(x='x',y1='min',y2='max', data=df, color='green',alpha = 0.225, label=\"Máximos y Mínimos\")\n",
        "      axs[1].set_ylabel('Recompensa obtenida (Duración del episodio)')\n",
        "      axs[1].set_xlabel('Número de episodios')\n",
        "      axs[1].legend()\n",
        "\n",
        "      plt.suptitle(supt)\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "      return mean_rewards"
      ],
      "metadata": {
        "id": "WVRcEb9OLvsm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo de uso"
      ],
      "metadata": {
        "id": "yWA-V-sOL9UA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1xf_Lr08L3fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Métodos con Redes Neuronales  \n",
        "Contenido de la sección 6.  \n"
      ],
      "metadata": {
        "id": "4FrsG3G_PVot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparación  \n",
        "Contenido de la sección 7.  \n"
      ],
      "metadata": {
        "id": "G5ZVj1xuPWoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones  \n",
        "Contenido de la sección 8."
      ],
      "metadata": {
        "id": "cI7BClVhPXxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias\n",
        "\n",
        "[1] Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. The MIT Press, second edition.\n",
        "\n",
        "[2] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing atari with deep reinforcement learning. cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013.\n"
      ],
      "metadata": {
        "id": "HhVGfkzU6KRH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YFMoZEaCFqK8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}