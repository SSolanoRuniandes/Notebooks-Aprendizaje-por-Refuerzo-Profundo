{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/blob/main/IntroduccionStableBaselines3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oblzmhF6SCZW"
      },
      "source": [
        "![MAIA banner](https://raw.githubusercontent.com/MAIA4361-Aprendizaje-refuerzo-profundo/Notebooks_Tareas/main/Images/Aprendizaje_refuerzo_profundo_Banner_V1.png)\n",
        "\n",
        "# <h1><center>Introducción a Stable-Baselines3 - Semana 3 <a href=\"https://colab.research.google.com/github/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/blob/main/IntroduccionStableBaselines3.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"140\" align=\"center\"/></a></center></h1>\n",
        "\n",
        "\n",
        "Stable-Baselines3 (SB3) es una librería basada en PyTorch que contiene la implementación de múltiples algoritmos de Aprendizaje por Refuerzo. Stable-Baselines3, versión mejorada de Stable-Baselines, es actualmente una de las librerías mejor actualizadas y documentadas en el campo de Aprendizaje por Refuerzo debido a su naturaleza open-source. Esta librería ofrece implementaciones limpias de diferentes algoritmos como PPO, A2C, SAC, DQN, entre otros, que son fáciles de utilizar con entornos de Gymnasium.\n",
        "\n",
        "En este notebook se presenta una traducción directa del tutorial oficial de Stable Baselines <a href=\"https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/1_getting_started.ipynb\">Stable Baselines3 Tutorial - Getting Started</a>. Se sugiere revisar otros tutoriales oficiales que pueden ser encontrados en <a href=\"https://github.com/araffin/rl-tutorial-jnrr19\">este repositorio de GitHub</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial de Stable Baselines3 - Primeros Pasos\n",
        "\n",
        "Repositorio de tutoriales en Github: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n",
        "\n",
        "Repositorio oficial de Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentación oficial de Stable-Baselines3: https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "Otras contribuciones de SB3: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "\n",
        "Repositorio de RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) es un marco de entrenamiento para problemas Aprendizaje por Refuerzo (RL) utilizando Stable-Baselines3. Este proporciona scripts para realizar el entrenamiento, evaluación de agentes, ajuste de hiperparámetros, gráfica de resultados y grabación de videos.  \n",
        "\n",
        "## Introducción\n",
        "\n",
        "En este notebok, aprenderás los elementos más básicos necesarios para utilizar la librería stable_basleines: cómo crear un modelo de RL, entrenarlo y evalaurlo. Debido a que todos los algoritmos comparten la misma interfaz, veremos qué tan simple es cambiar de un algoritmo a otro.\n",
        "\n",
        "## Instalar Depedencias y Stable-Baselines3 usando pip\n",
        "\n",
        "La lista completa de dependencias puede ser encontrada en el archivo [README](https://github.com/DLR-RM/stable-baselines3) del repositorio oficial.\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ],
      "metadata": {
        "id": "rkPuSbmG6dw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install ffmpeg freeglut3-dev xvfb  # Para visualización\n",
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
      ],
      "metadata": {
        "id": "0zNWCVCf6dFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "Stable-Baselines3 funciona en ambientes que siguen la [interfaz y estructura de Gym](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html).\n",
        "Puede encontrar una lista de ambientes disponibles [aquí](https://gymnasium.farama.org/environments/classic_control/).\n",
        "\n",
        "No todos los algoritmos funcionan con todos los espacios de acciones definidos; puede encontrar más información en [esta tabla](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html)"
      ],
      "metadata": {
        "id": "e0TVMzxa92lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym #SB3 trabaja con ambientes de gymnasium\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "oybFWRt9-aDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo primero que necesitamos hacer es importar el modelo de RL. Puede revisar la documentación para saber qué modelo se puede utilizar para cada problema. En este tutorial se utilizará el algoritmo de `PPO`."
      ],
      "metadata": {
        "id": "OB-v4rP6-bWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO #Se importa el algoritmo PPO de SB3"
      ],
      "metadata": {
        "id": "pXOKSeTd-aqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente paso es importar la clase de la política que será utilizada para crear las redes neuronales (para la política/funciones de valor). Este paso en realidad es opcional, ya que se pueden usar strings directamente en los contructores. Por ejemplo:\n",
        "\n",
        "```PPO('MlpPolicy', env)``` en lugar de ```PPO(MlpPolicy, env)```\n",
        "\n",
        "Note también que algunos algoritmos como `SAC` tienen su propia `MlpPolicy`, y es por eso que utilizar los strings para la política es la opción recomendada."
      ],
      "metadata": {
        "id": "gP18tEQT-tZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.ppo.policies import MlpPolicy"
      ],
      "metadata": {
        "id": "RhD-sumw-sf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crea el ambiente de Gym e instancia el agente\n",
        "\n",
        "Para este ejemplo, utilizaremos el ambiente de CartPole, un problema muy popular de control que ya fue visto anteriormente.\n",
        "\n",
        "\"Un poste es unido mediante una junta sin actuadores a un carrito, el cual se mueve por una superficie sin fricción. El sistema se conrola aplicando una fuerza de +1 o -1 al carrito. El péndulo comienza en posición erquida hacia arriba, y el objetivo es prevenir que caiga. Se recibe una recompensa de +1 por cada paso de tiempo que el poste permanece hacia arriba.\"\n",
        "\n",
        "Cartpole environment: [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
        "\n",
        "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
        "\n",
        "\n",
        "Para este caso escogemos la MlpPolicy porque el espacio de observación del CartPole es un vector de características, no imágenes.\n",
        "\n",
        "El tipo de acción que se usa (discreta/continua) será deducida de forma automática a partir del espacio de acciones del ambiente.\n",
        "\n",
        "Aquí usaremos el algoritmo de [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html), el cual es un método Actor-Crítico: usa una función de valor para mejorar el descento de gradiente de la política (recudicendo la varianza). Este algoritmo combina las ideas de [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) (disponiendo de múltiples trabajadores que usan bonos de entropía para formentar la exploración) y [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (usa una región de confianza para mejorar la estabilidad y evitar caídas catastróficas de rendimiento).\n",
        "\n",
        "PPO es un algoritmo on-policy, lo cual significa que las trayectorias utilizadas para acutalizar las redes neuronales deben ser recolectadas utilizando la última política actualizada. Es en realidad menos eficiente en términos de muestras que los algoritmos off-policy como [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) o [TD3](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html), pero es mucho más rápido en términos reales de tiempo."
      ],
      "metadata": {
        "id": "LUujm4Rm_mr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\") #Se crea el ambiente\n",
        "\n",
        "model = PPO(MlpPolicy, env, verbose=0) #Se instancia el algoritmo\n",
        "#model = PPO('MlpPolicy', env, verbose=0) #Alternativa recomendada"
      ],
      "metadata": {
        "id": "jofkTSHtCxAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos una función adicional para evaluar el agente:"
      ],
      "metadata": {
        "id": "6RRFZTT-C5uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.base_class import BaseAlgorithm\n",
        "\n",
        "def evaluate(\n",
        "    model: BaseAlgorithm,\n",
        "    num_episodes: int = 100,\n",
        "    deterministic: bool = True,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Evalua un agente de RL por 'num_episodes' episodios.\n",
        "\n",
        "    Lista de parámetros\n",
        "    :model: el agente de RL\n",
        "    :env: el ambiente de Gym\n",
        "    :num_episodes: número de episodios para evaluar el agente (100 por defecto)\n",
        "    :deterministic: si utilizar acciones determinísticas o estocásticas (True por defecto\n",
        "\n",
        "    :return: recompensa promedio en los `num_episodes` episodios (float)\n",
        "    \"\"\"\n",
        "    # Esta función funciona para un único ambiente\n",
        "    vec_env = model.get_env()\n",
        "    obs = vec_env.reset() #reinicia el ambiente\n",
        "    all_episode_rewards = [] #lista vacía para almacenar recompensas\n",
        "    for _ in range(num_episodes): #ciclo de num_episodes episodios\n",
        "        episode_rewards = [] #almacena las recompensas del episodio\n",
        "        done = False #inicializa condición de terminación\n",
        "\n",
        "        # Nota: SB3 VecEnv se reinicia automáticamente:\n",
        "        # https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\n",
        "        # obs = vec_env.reset()\n",
        "\n",
        "        while not done: #ejecuta un episodio\n",
        "            # _states son sólo útiles cuando se usan políticas LSTM\n",
        "            # `deterministic` es para usar acciones determinísticas\n",
        "            action, _states = model.predict(obs, deterministic=deterministic) #el modelo predice qué acción elegir en base al pultimo estado observado\n",
        "            # en este caso action, rewards y dones son arreglos\n",
        "            # porque se está usando un ambiente vectorizado\n",
        "            obs, reward, done, _info = vec_env.step(action) #da el paso y observa el nuevo estado y recompensa\n",
        "            episode_rewards.append(reward) #almacena recompensa\n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards)) #calcula el retorno del episodio (suma) y lo almacena en la lista\n",
        "\n",
        "    mean_episode_reward = np.mean(all_episode_rewards) #calcula la recompensa promedio\n",
        "    print(f\"Mean reward: {mean_episode_reward:.2f} - Num episodes: {num_episodes}\") #da un print con la recompensa promedio\n",
        "\n",
        "    return mean_episode_reward #retorna la recompensa promedio"
      ],
      "metadata": {
        "id": "_8Z_zXXNC9VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluemos un agente sin entrenar, este debería ser un agente aleatorio."
      ],
      "metadata": {
        "id": "yCEmhYzhEfHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agente aleatorio, antes del entrenamiento\n",
        "mean_reward_before_train = evaluate(model, num_episodes=100, deterministic=True)"
      ],
      "metadata": {
        "id": "NBWU6nbVEnRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No obstante, Stable-Baselines3 ya tiene incluida una función similar para evaluar agentes:"
      ],
      "metadata": {
        "id": "l8rtIicjEstH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy #importa la función evaluate_policy para evaluar agentes directamente"
      ],
      "metadata": {
        "id": "uYU5kpxqEsHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, warn=False) #obtiene la recompensa promedio y desviación estándar\n",
        "\n",
        "print(f\"mean_reward: {mean_reward:.2f} +/- {std_reward:.2f}\") #imprime resultados"
      ],
      "metadata": {
        "id": "K0gFmgZvE6aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenar y evaluar el agente"
      ],
      "metadata": {
        "id": "bLSZP1ruFGYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrena el agent por 10000 pasos\n",
        "model.learn(total_timesteps=10_000) #usa model.learn para entrenar el agente"
      ],
      "metadata": {
        "id": "X94EZOoAGtIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalúa el agente entrenado\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100) #obtiene la recompensa promedio y desviación estándar\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\") #imprime resultados"
      ],
      "metadata": {
        "id": "ScX_vGrmGzL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "!Aparentemente el entrenamiento funcionó porque la recompensa incrementó mucho¡"
      ],
      "metadata": {
        "id": "T5kXCG1VG_O0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparar grabación de video\n",
        "\n",
        "En este tutorial se muestra la forma de grabar un video utilizando el wrapper [VecVideoRecorder](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper. Se realiza una explicación sobre qué son los wrappers y cómo funcionan en [este segundo tutorial](https://github.com/araffin/rl-tutorial-jnrr19/blob/sb3/2_gym_wrappers_saving_loading.ipynb)."
      ],
      "metadata": {
        "id": "b3tkFQp0HLFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hay que configurar fake display; de lo contrario el redner fallará\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "def show_videos(video_path=\"\", prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Tomado de https://github.com/eleurent/highway-env\n",
        "\n",
        "    Lista de parámetros\n",
        "    :video_path: (str)Ruta a la carpeta que contiene los videos\n",
        "    :prefix: (str) Filtra el video, mostrando sólo los que inician con este prefijo\n",
        "    \"\"\"\n",
        "    html = []\n",
        "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append(\n",
        "            \"\"\"<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>\"\"\".format(\n",
        "                mp4, video_b64.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "\n",
        "\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
        "    \"\"\"\n",
        "    :param env_id: (str)\n",
        "    :param model: (RL model)\n",
        "    :param video_length: (int)\n",
        "    :param prefix: (str)\n",
        "    :param video_folder: (str)\n",
        "    \"\"\"\n",
        "    eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "    # Comienza el video en step=0 y graba 500 pasos\n",
        "    eval_env = VecVideoRecorder(\n",
        "        eval_env,\n",
        "        video_folder=video_folder,\n",
        "        record_video_trigger=lambda step: step == 0,\n",
        "        video_length=video_length,\n",
        "        name_prefix=prefix,\n",
        "    )\n",
        "\n",
        "    obs = eval_env.reset()\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "    # Cierra la grabadora de video\n",
        "    eval_env.close()"
      ],
      "metadata": {
        "id": "ZNMagT4rHe-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualiza el agente entrenado"
      ],
      "metadata": {
        "id": "8-o_pc28IkL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "record_video(\"CartPole-v1\", model, video_length=500, prefix=\"ppo-cartpole\") #graba el video"
      ],
      "metadata": {
        "id": "szaDywIDIkVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_videos(\"videos\", prefix=\"ppo\") #muestra el video"
      ],
      "metadata": {
        "id": "5bXq3CqyIzso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXTRA: Entrena un modelo de RL en una sóla línea\n",
        "\n",
        "La clase de política a usar será inferida y el ambiente será automáticamente creado. Esto funciona porque ambos se encuentran [registrados](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html)."
      ],
      "metadata": {
        "id": "mWzCExxnHUoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrena un modelo en una sola línea de código\n",
        "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
      ],
      "metadata": {
        "id": "f0DSLHRWJVRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusión\n",
        "\n",
        "In este notebook hemos aprendido:\n",
        "- Cómo definir u entrenar un modelo de RL utilizando stable_baselines3, sólo se necesita una línea de código ;)"
      ],
      "metadata": {
        "id": "c5yeRVVRJbiJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}