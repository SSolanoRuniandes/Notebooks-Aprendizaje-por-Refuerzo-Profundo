{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSEdpGG/rgEDjyVj65ckCf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/blob/main/TareaSemana2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![MAIA banner](https://raw.githubusercontent.com/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/main/Images/Aprendizaje_refuerzo_profundo_Banner_V1.png)\n",
        "\n",
        "# <h1><center>Tarea Tutorial - Semana 2 <a href=\"https://colab.research.google.com/github/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/blob/main/TareaSemana2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"140\" align=\"center\"/></a></center></h1>\n",
        "\n",
        "<center><h1>Predicción y Control On-Policy</h1></center>\n",
        "\n",
        "En este notebook tutorial vas a aprender sobre algunas técnicas <i>on-policy</i> que existen para aproximar la función de valor en un problema de aprendizaje por refuerzo y algoritmos de aprendizaje on-policy. Mostraremos el funcionamiento de estas técnica sy algoritmos utilizando el ambiente de <a href=\"https://gymnasium.farama.org/environments/classic_control/mountain_car/\">Mountain Car</a>, incluido en las librerías de Gym, e implementaremos redes nuronales con el framework <a href=\"https://github.com/inarikami/keras-rl2\">keras-rl2</a>, que incluye nuestros algoritmos de interés. La misión central de este notebook es evidenciar de forma visual cómo se puede representar matemáticamente un problema y cómo se estructura una solución cuando se utilizan técnicas <i>on-policy</i>. El tutorial se divide en:\n",
        "\n",
        "\n",
        "# Tabla de Contenidos\n",
        "1. [Objetivos de Aprendizaje](#scrollTo=Objetivos_de_Aprendizaje)  \n",
        "2. [Marco Teórico](#scrollTo=Marco_Te_rico)  \n",
        "3. [Instalación de Librerías](#scrollTo=Instalaci_n_de_Librer_as)  \n",
        "4. [Familiarización con el Entorno de Gym](#scrollTo=Familiarizaci_n_con_el_Entorno_de_Gym)  \n",
        "5. [Predicción on-policy](#scrollTo=Predicci_n_on_policy)  \n",
        "6. [Control on-policy](#scrollTo=Control_on_policy)  \n",
        "7. [Reflexiones Finales](#scrollTo=Reflexiones_Finales)  \n",
        "8. [Referencias](#scrollTo=Referencias)"
      ],
      "metadata": {
        "id": "oblzmhF6SCZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objetivos de Aprendizaje  \n",
        "  \n",
        "* Conocer algunas formas matemáticas que se utilizan para representar problemas complejos de aprendizaje por refuerzo.\n",
        "* Familiarizarse con los entornos de simulación de Gym.\n",
        "* Entrenar y validar algoritmos de aprendizaje por refuerzo <i>on-policy</i> con redes neuronales (Deep SARSA).\n",
        "\n"
      ],
      "metadata": {
        "id": "k8OPdsC0AxgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Marco Teórico  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9lCj3GovPOcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación de Librerías  \n",
        "\n",
        "Ejecute los siguientes bloques de código para instalar e importar las librerías requeridas en el tutorial. Aquí se instalan versiones compatibles de <i>keras</i>, <i>tensorflow</i> y <i>Gymnasium</i> para poder realizar los entrenamientos de SARSA con redes neuronales. La primera ejecución puede demorar un par de minutos en finalizar. Si la segunda celda le da un error, pruebe volver a ejecutar la celda."
      ],
      "metadata": {
        "id": "chWp-3WSPQlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instala el framework de keras-rl2 y requisito de tensorflow\n",
        "!pip install keras-rl2==1.0.5\n",
        "!pip install tensorflow==2.15.0\n",
        "#Instala renderlab para renderizar videos de gym\n",
        "!pip install renderlab\n",
        "\n",
        "#importa librerías básicas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import tensorflow as tf #importa tensorflow\n",
        "import renderlab #importa renderlab para videos\n",
        "import gym as gym #importa gym (usado en entrenamiento)\n",
        "import gymnasium as gymnasium #importa gymnasium (usado en render)\n",
        "\n",
        "#esta líneas evitan conflictos entre la versión de keras y tensorflow\n",
        "import tensorflow.keras as keras\n",
        "from keras import __version__\n",
        "tf.keras.__version__ = __version__\n",
        "\n",
        "#importa las herramientas de redes neuronales necesarias de keras y el agente de SARSA\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Input\n",
        "from keras.optimizers.legacy import Adam\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.agents.sarsa import SARSAAgent\n",
        "\n",
        "#limpia registros\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Todas las librerías han sido descargadas correctamente.\")"
      ],
      "metadata": {
        "id": "Lt7lZo5UB-Nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab3d29f-4028-4cc9-d31d-c72d7410ad91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todas las librerías han sido descargadas correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Familiarización con el Entorno de Gym\n",
        "\n",
        "El ambiente de Gym de <a href=\"https://gymnasium.farama.org/environments/classic_control/mountain_car/\">Mountain Car</a> consiste de un carro atrapado en el fondo de un valle, y debe acelerar a la izquierda o a la derecha para intentar ganar el impulso suficiente para salir. Puede leer más detalladamente la documentación de este ambiente en los foros oficiales de Gymnasium.\n",
        "\n",
        "![Observation_space_cartpole](https://raw.githubusercontent.com/MAIA4361-Aprendizaje-refuerzo-profundo/Notebooks_Tareas/main/Images/Observation_space_mountaincar.png)\n",
        "\n",
        "<center>Figura 5. Espacio de observación del ambiente de <i>Mountain Car</i>. [3]</center>\n",
        "\n",
        "El espacio de estados del ambiente está definido en 2 dimensiones continuas, que definen la posición en el eje x y la velocidad del vehículo. Los límites de ambas variables puede observarse en la Figura 5. Por otra parte, el espacio de acciones consiste simplemente en 3 acciones discretas y determinísticas:\n",
        "\n",
        "*   0: Acelera a la izquierda\n",
        "*   1: No acelera\n",
        "*   2: Acelera a la derecha\n",
        "\n",
        "En este ambiente, el agente recibe una recompensa cuando alcanza una meta por fuera del valle, y por cada paso de tiempo que le toma alcanzarla recibe una recompensa negativa de -1. Si el auto llega a la meta (posición mayor a 0.5), el episodio se da por terminado, mientras que si el episodio supera los 200 pasos, se da por truncado; finalizando la simulación. Esto quiere decir que la peor recompensa posible será -200.\n"
      ],
      "metadata": {
        "id": "xdQ5A4KbPSOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo\n",
        "\n",
        "En esta sección se muestra un ejemplo de simulación de un episodio del <i>Mountain Car</i>. En este caso, el carro alterna acelerar a la izquierda y a la derecha en cada paso de tiempo."
      ],
      "metadata": {
        "id": "1CxFdNryYVwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de simulación de un episodio de Mountain Car\n",
        "env_prueba_1 = gymnasium.make(\"MountainCar-v0\", render_mode=\"rgb_array\") #Se crea el ambiente. Para este tutorial, utilice gymnasium si va a renderizar.\n",
        "env_prueba_1 = renderlab.RenderFrame(env_prueba_1, \"./output\") #Se crea una copia que se pueda renderizar con renderlab\n",
        "\n",
        "obs , info = env_prueba_1.reset() #Se reinicia el estado para comenzar. En obs se almacena el estado observado (continuo, 2 dimensiones)\n",
        "terminated = False #Inicializa una condición para el loop\n",
        "truncated = False #Inicializa una condición para el loop\n",
        "total_reward=0 #Inicializa contador del retorno\n",
        "action=0 #Inicializamos una variable de acción para alternar la selección\n",
        "\n",
        "while not (terminated or truncated): #Simula hasta que el carro salga del valle o hasta que pasen 200 episodios\n",
        "\n",
        "  #Decide una acción. En este caso alterna entre izquierda y derecha\n",
        "  if(action==0):\n",
        "    action=2\n",
        "  else:\n",
        "    action=0\n",
        "\n",
        "  obs, reward, terminated, truncated , info = env_prueba_1.step(action) #Con la función step el ambiente da un paso. Se obtiene el estado, recompensa y banderas de información\n",
        "  total_reward+=reward #Llevamos una cuenta de la recompensa total\n",
        "\n",
        "print(\"Recompensa obtenida en el episodio:\",total_reward) #Se imprime la recompensa obtenida\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "env_prueba_1.play() #Con esta función se obtiene el video de la simulación"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "2CB_sJpPOoJZ",
        "outputId": "ffbde538-a576-489c-aa3c-8f31d202c86b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recompensa obtenida en el episodio: -200.0\n",
            "\n",
            "\n",
            "\n",
            "Moviepy - Building video temp-{start}.mp4.\n",
            "Moviepy - Writing video temp-{start}.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready temp-{start}.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls  >\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puede observar que el retorno obtenido fue de -200, la peor recompensa posible. Alternar inmediatamente entre izquierda y derecha no es una estrategia útil."
      ],
      "metadata": {
        "id": "cjpcdfBJbir2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio Práctico\n",
        "\n",
        "Ahora, para comprobar el entendimiento y familiarización con el ambiente, intente por unos pocos minutos generar manualmente alguna estrategia para hacer que el carro logre salir del pozo (este será el problema que solucionaremos después con redes neuronales)."
      ],
      "metadata": {
        "id": "ZQd8kQn9cYpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Genere alguna estrategia para intentar salir del pozo.\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETAR ===========================================\n",
        "#\n",
        "\n",
        "# ====================================================="
      ],
      "metadata": {
        "id": "v6MT0vr3dNe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicción on-policy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MbY4Th6pPUbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Control on-policy\n",
        "\n",
        "En este apartado se busca comprender la construcción de arquitecturas de redes neuronales profundas y luego realizar un entrenamiento con el algoritmo <i>on-policy</i> de SARSA. Se va a revisar el efecto que tienen los cambios de hiperparámetros y se busca luego visualizar la función de valor discutida en la sección anterior.\n"
      ],
      "metadata": {
        "id": "4FrsG3G_PVot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo"
      ],
      "metadata": {
        "id": "PSxQ5PPofVIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de construcción de una red neuronal y entrenamiento con SARSA\n",
        "\n",
        "########################## Creación del Ambiente ##########################\n",
        "env = gym.make('MountainCar-v0') #crea el ambiente utilizando gym.\n",
        "np.random.seed(613) #semilla aleatoria\n",
        "env.seed(613)\n",
        "nb_actions = env.action_space.n #obtiene la cantidad de acciones (3)\n",
        "\n",
        "###################### Creación de la Red Neuronal ########################\n",
        "# Modelo secuencial, una capa después de otra\n",
        "model = Sequential()\n",
        "\n",
        "#Capa de Entrada\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "#Capas \"Escondidas\". Aquí se pueden añadir varias capas con distinta cantidad de neuronas\n",
        "model.add(Dense(8,activation='relu'))\n",
        "model.add(Dense(4,activation='relu'))\n",
        "model.add(Dense(4,activation='relu'))\n",
        "\n",
        "#Capa de Salida\n",
        "model.add(Dense(nb_actions,activation='relu'))\n",
        "\n",
        "print(model.summary()) #muestra las neuronas y parámetros de la red\n",
        "\n",
        "###################### Configuración del Modelo ##########################\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1) #límite de la memoria\n",
        "policy = BoltzmannQPolicy() #política inicial\n",
        "\n",
        "#se crea el agente de SARSA\n",
        "sarsa_ejemplo_1 = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=100, policy=policy)\n",
        "sarsa_ejemplo_1.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "\n",
        "model.save_weights('model_sarsa_ejemplo_1_valoresiniciales.h5') #guarda los valores iniciales\n",
        "\n",
        "############################ Entrenamiento ###############################\n",
        "#Se entrena el agente por 10,000 pasos de tiempo y se guardan los resultados\n",
        "\n",
        "#carga los pesos iniciales para entrenar desde 0 (buena practica)\n",
        "sarsa_ejemplo_1.load_weights('model_sarsa_ejemplo_1_valoresiniciales.h5')\n",
        "\n",
        "#Comienza a entrenar\n",
        "#Guarda el historial y muestra progreso en pantalla\n",
        "historial_entrenamiento_sarsa_ejemplo_1 = sarsa_ejemplo_1.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n",
        "#guarda los pesos finales (buena practica)\n",
        "sarsa_ejemplo_1.save_weights('model_sarsa_ejemplo_1_valoresfinales.h5f', overwrite=True)"
      ],
      "metadata": {
        "id": "8-f1j-Dud33A",
        "outputId": "1f5a93db-bcde-4659-80d4-2fde483a54e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_4 (Flatten)         (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 8)                 24        \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 3)                 15        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 95 (380.00 Byte)\n",
            "Trainable params: 95 (380.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 10000 steps ...\n",
            "  200/10000: episode: 1, duration: 7.579s, episode steps: 200, steps per second:  26, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500113, mae: 0.338317, mean_q: 0.021680\n",
            "  400/10000: episode: 2, duration: 6.662s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  600/10000: episode: 3, duration: 6.648s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  800/10000: episode: 4, duration: 6.664s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 1000/10000: episode: 5, duration: 6.668s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 1200/10000: episode: 6, duration: 6.674s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 1400/10000: episode: 7, duration: 6.660s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 1600/10000: episode: 8, duration: 6.650s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 1800/10000: episode: 9, duration: 6.676s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 2000/10000: episode: 10, duration: 6.656s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 2200/10000: episode: 11, duration: 6.669s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 2400/10000: episode: 12, duration: 6.657s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 2600/10000: episode: 13, duration: 6.671s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 2800/10000: episode: 14, duration: 6.659s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 3000/10000: episode: 15, duration: 6.664s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 3200/10000: episode: 16, duration: 6.666s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 3400/10000: episode: 17, duration: 6.657s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 3600/10000: episode: 18, duration: 6.675s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 3800/10000: episode: 19, duration: 6.661s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 4000/10000: episode: 20, duration: 6.661s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 4200/10000: episode: 21, duration: 6.666s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 4400/10000: episode: 22, duration: 6.656s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 4600/10000: episode: 23, duration: 6.664s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 4800/10000: episode: 24, duration: 6.656s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 5000/10000: episode: 25, duration: 6.692s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 5200/10000: episode: 26, duration: 6.862s, episode steps: 200, steps per second:  29, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 5400/10000: episode: 27, duration: 6.874s, episode steps: 200, steps per second:  29, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 5600/10000: episode: 28, duration: 6.747s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 5800/10000: episode: 29, duration: 6.832s, episode steps: 200, steps per second:  29, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 6000/10000: episode: 30, duration: 6.710s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 6200/10000: episode: 31, duration: 6.674s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 6400/10000: episode: 32, duration: 6.755s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 6600/10000: episode: 33, duration: 6.740s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 6800/10000: episode: 34, duration: 6.912s, episode steps: 200, steps per second:  29, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 7000/10000: episode: 35, duration: 6.737s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 7200/10000: episode: 36, duration: 6.662s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 7400/10000: episode: 37, duration: 6.666s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 7600/10000: episode: 38, duration: 6.751s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 7800/10000: episode: 39, duration: 6.681s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 8000/10000: episode: 40, duration: 6.779s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 8200/10000: episode: 41, duration: 6.756s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 8400/10000: episode: 42, duration: 6.748s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 8600/10000: episode: 43, duration: 6.713s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 8800/10000: episode: 44, duration: 6.667s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 9000/10000: episode: 45, duration: 6.654s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 9200/10000: episode: 46, duration: 6.717s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 9400/10000: episode: 47, duration: 6.665s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 9600/10000: episode: 48, duration: 6.653s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 9800/10000: episode: 49, duration: 6.667s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10000/10000: episode: 50, duration: 6.680s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "done, took 335.877 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################ Validación ###############################\n",
        "sarsa_ejemplo_1.load_weights('model_sarsa_ejemplo_1_valoresfinales.h5f') #Carga los pesos del entrenamiento (buena practica)\n",
        "env.reset() #reinicia el ambiente\n",
        "test_episodes = 5  # Número de episodios de validación\n",
        "\n",
        "sarsa_ejemplo_1.test(env, nb_episodes=test_episodes, visualize=True) #Simula con la política obtenida"
      ],
      "metadata": {
        "id": "iQQpo0iKfgB3",
        "outputId": "b1400330-a540-42a7-a103-2bc5df9b2f42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -200.000, steps: 200\n",
            "Episode 2: reward: -200.000, steps: 200\n",
            "Episode 3: reward: -200.000, steps: 200\n",
            "Episode 4: reward: -200.000, steps: 200\n",
            "Episode 5: reward: -200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7814fc5de410>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################## Video #################################\n",
        "sarsa_ejemplo_1.load_weights('model_sarsa_ejemplo_1_valoresfinales.h5f') #Carga los pesos del entrenamiento (buena practica)\n",
        "\n",
        "env_test_render = gymnasium.make('MountainCar-v0', render_mode = \"rgb_array\") #crea una ambiente de prueba (con gymnasium para renderizar)\n",
        "env_test_render = renderlab.RenderFrame(env_test_render, \"./output\") #crea copia de renderlab\n",
        "observation, info = env_test_render.reset() #inicializa el ambiente\n",
        "\n",
        "while True: #simula\n",
        "  action = sarsa_ejemplo_1.forward(observation) #elige una acción de acuerdo a sarsa 4 (se usan los resultados de la primera red)\n",
        "  observation, reward, terminated, truncated, info = env_test_render.step(action) #obtiene la observación tras tomar la acción\n",
        "  if terminated or truncated: #si pierde o pasan 200 pasos\n",
        "      break #termina\n",
        "\n",
        "env_test_render.play() #muestra el video"
      ],
      "metadata": {
        "id": "Z9MQMzQVjeh0",
        "outputId": "11a0de6d-5bc8-4901-e0c2-d949be050945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video temp-{start}.mp4.\n",
            "Moviepy - Writing video temp-{start}.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready temp-{start}.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls  >\n",
              " <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAc0FtZGF0AAACrgYF//+q3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE2NCByMzE5MSA0NjEzYWMzIC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAyNCAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTMgbG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTI1IHNjZW5lY3V0PTQwIGludHJhX3JlZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAAN+GWIhAAr//7Y5/Msq1xA0DVUuHVl7uFSgaMoZ2nkvUzAAAADAAADAABo/Ot9/srZ0dc2TAAADNAlrD2ajoAyREKrGoBrmFT/jbr+uKV63H+jYfwGJbVbOn0m1S65oHQGpImqAsySWWW5d9HhpxanDBmZZ4hwTZLG5zws+xg1ZHx6j6STz4DqXbkjtj3+BkQFNtJD/LVwfkW6Q1ZyYS5LqWHgxTbDafHmxytMOgl2OiwH+ipXzwuuyhKz4aqAvG/R51rllCin0WqN1BvyulgAJvuWES6pcBaahQ8kjJcvXTs2FQNwlCiejhKF2f6DxTxMC9x7Fq8ScVJXWWN0F+ULtMhKkployw9zRywa6dFRIB4g2vIeFxXVVHY05adbe0p9zi6uVuA6oc/k/yy+4B1upje23oG8MDq9eAZdfd3mrB4453dtBWjYQUzqOIw9sbShslNlO3szVnzmG6AuyCIZRZAU8iSg97YN5LEQDNsSdhk2O2Qykyyk5mJLyrLLuVDYS8vy0y356/FQyKN2JYuyAEvSCTQ1m4ByXG3vh6APai/Td0OkoBiOh3eSz6TfauSbc1nbjgUgOUlF6f28bq27RGk5D5Ylj4wM8IChBPln/Ll6lkMX12yMNdzJnaScs97BqF9BieRGrcxYB44CzS48gzYAjntUY19oNAV9XVRn2SZPmyuVKE+hSJ+gHvDfUBuX4aoH4Y5suNMwHD+Li7dz5/8ypCFXZ9HnLq6iSie0/GA43kkydtzRBW9fwtGy6FtT6bKd+Zf5aoB3auO08n282j5/f2BNxtGEeUi++ydnUf5LPxtepOVFqIG+HUMSIlT4WJL+1FKTEiyJE1BsIud8KhvfPfE2Dj1L9xSD1UGZg1EKMgRtA+0UwtnwwDQLduZwR5kgH6Uev4ZXC4qvgpodYAAAAwAAXUMmffNUWjvKIFXMOWV5wzPloNRcdtHYt/kaJ5OJjAeSZ83W+5gZGzD9gB8zCmD8v7Iaohu7lBakLBewe+WHONcXldVAt+DE+RO7A724ySkuLyXTEtKuX1OEBXwAAjIxAAykDAvKsgdAsJTHmG4+uAkQ//nc14k1G3fG7Ks6JaqzYsMBBve0CFx5JP/oGIAA84GaUh9aP5W68aZT8YT5raZU/f7672qt9JtGNEW9IOqLr3ldRD6415UYLtJMZX3OtZOsUo6YxSbk9OcR/F0d4raBnwv/U72fb7zKKV0j5BdxDD50ApHx1UGkh0+EBCUWqUVKwn6Gx+0OBfPHhfIuyVq/gKY6UoQHMd8c6Ch3fK6ROFM94lxuL2WiWS/o2REHP+nFN6FsSW1CfHMMSwE4fe7ivJvVC6bxBa0B/2QF5f4VRcEpbiQEpgDAISEweYZoleJfeaWwUe2fvYK5EHRi0779rjDf2hzlprMlkA+NQxmYCNeVF4JIygxs1Jgab8clYBWenqXAopMCSnMgfqU/gCzuuORriGX+ZZq3UfPGlrgmle2r8zKphaMKeJOxGLI9wCQwKPWXxsUomRbGUMsi3BfhpU1DnGZK6LwPJv7UCsdVBAnePRHFkwE/BDAFcTk0IHMjrE82H4uFhUREs74eDR+LSqDKWXf1OJMcqdvXx8OuNUNZbBRN+4gRyOuDiZkAY6adXVkwita1KhioQQF+6CPZymDpt5ZQcLGZR/0XTn2uj0P0vsEIz6t8bqShu24muiQC9GDPACJUijluGbr55LQxMEXn3O49snqIGu+gPP1/q7y999lvQ/+zD7AFPH77tjQmzuQrqNMxb+xuQdk2YTo/Xk3VjpvvuPeV5V5RZpWcT56sBZfyksAfLKTt3of5LScuha0bERNOAXGBDo+51HLCkjyEVa+N40s1F5hPdRaxgNnQ2gmppL4j6YmULSCOUTYKm4TLVg1JJ3C9eEDhylu61xk2Fyc/yKE9RQzrZeRGTHLyyw5XGTfWn7XVXku/e03MXR0MBKXQSuT9jMAORL46jzCS+hO+4U+X6UvXVnPo5BXy98WtehDbSE0waE6pAoOXBTbyj2DP77BmDxCj7EEGMs3K7q9xh+fBYo2se1UV9GYt1kHkOmuM0r0q1DNC/38pyWq9CRmVscTBrYQSlFYFcZBU7EZDnAnyLdHf1cIBEdNrYjmXFKTsGzbGk12OkHJLwJjByqek9ekxH4P8ARMTGWPN2aH80hSn3QoJBP9bPEHsOgb/OR10taqjLtRl1ziX7ZkwNKlLfwPO39hp5BNPga8yC9ccOuNCarNhBSn99WloUh6HLXFzDM0xeP/70GTPFuPKnh0pGxBt2kgYX9ab2Jlyl4kcyYrHiELhsdhHOB5bA+h/EX8ZPjEvk8I3mUDfF3u4rk3hK7dzwDqw+6BgHWUBQKGTRnVhSNLNa0PjfVykyVMcLkHLDJvKqr+3a6V28b9mwB45nkvHIMmHqQ/7I5B8LxaVFQUCXCxzUvje7zXfzoUvs8/cFO6MDRfW/5KShuFVr5VvQeKOt9c/sGDjr4OgvTgH/CSF8K1HNbABIXpr09vDGaNafsCwrpDhSrxTZ3Tzi/pBXUo03z5R0Y/vK9bMp2cAE32Q8h4roWsBkMPAoBE2n7u3ogN/1R1WNYr17SBwsRawf+AbmEFfqRkAAB/C/XOMoY7iSjry0Z5KBZ7uSOaWuI92sJZM6kZzIzaVcrYbtlM4/A42BsoEaB/DxJKLF3Y+NERbCbbmAuld5Ml7IJ4b9AfAvSfKKv0Q40HYAksma+BfH3LZpcVW7dcwIZF5BJU1at9Pld412ObFx/r5Lz2aBenwLsnkplRsVkEMz240o8HZhrTABC4GT4pA1uRaqt2bemG2ATQdLuAAmo5S3eD2oy5Ut3Q+lBiXW0HPU0CzfrOlvAwgU2DQp0yiIwSbkLfAKTAOnkOSZDEid7gVGSH7x2m0JLXEhbyMZwn55WTSG8FtbBdizSnYrpszd30/irnaVbzDPSXTpRVv/ON4Ntyc2ZsiadiBFtVv4Js7S5PiBFI/e2gblfj+3aCAzFDV7rwVQyxu6oS8PaFZe1gMyAEpFc0ASMKSKVSrg6sHfnRj7cbSikZN87zW51f1Yz2Lpc06yEBZSD3VMJ63YyHN/bNTuGw7kRqyJFnSuWxmyIh/pM2/YDZUWz3xNmszGdWFRbkJfN8CdIZmveQw6GpLSUVLQRrJK4lDKctbGg7wYEx1+19hHOMnnlGwmIwnHlOuXShVt1NxiBMUE8/vGG+KFTfCWem0rcJy8a99w+Oue2MLtjFJOb4RJ+AIeK5n6tR3BqXhEIefqeuZr9dQWgqr/ShO9pgjN48c6RN5wAmGhQmYemVR/yforMs2C1GNpovLQVvXXXEeicVzsobpwlrHBLh997+DpNlpKQlDlPpFEcDImnmVv/GPn4OU8mn0mpzp48G0AnQ0KLgtfN6Hsah38w4/KdvyvS0B2ElEzGycS0qyLd9D/fA8bSsmycu5maVLs/xegZjuTgPFz2AJ3yAMgIgAGHs2tk3GZhHKrztXvAZclzyn0JoO7cWVnMDCBeUeLLBaPWZNXZeXerKZt7UFVPw9+J63eq5YSVL/ZhJEmdHq6TKmNogmiV6FSQ9A5iOK3ZWmumajeQYIbZp/tRdqpKcbAQHrg0N05PqYOlJLoox/yHF14lOSi+u391+JT2SOn7tp+a2Ycl/7umGI+orvB/7oWJSamHU7M7B0cNxUQ4YXDXNrW/dOfQv8pPcS8EeCrjtLRhAhvv9loGSq1lfFzriraZQHR3cHyIZk/aXy7D6h83UqwLvIdkGKpvQX1fGZPAVu8arVePz3SI2qF0vhF9BSuALfJJlTxHcw+bqk6DgAAeWrmfof5+YVu4VgijKPOoGR3Xo3wHkQL7JEVvzMfiKfrRNcIwwfzGBsxBt9srHQfH+zqZ/HhdnM0v9eelgM53OuEDPnluU5l2iJE62tPjiWDRugGAdjz336pLKOKz/fYgPsnr4hhGoSUF4KK6nMtLUqyoJ6ENFRDV+H3YoKjaOeyFcN1WqE+SUIKjw4hafZDIdV79Lk7c8JYfj+2PqujyuVkP88WjKkUvfMEGQEp1bcpTXT28swRJAt0v0m4zos+NFxa2UW7R+vqywEgLCcr/gFQaIaJEoHhqXrLHlZsWIftXC1Sra2NYO+1NZM7GpfOMrSdOAiVLWPNFT0Q52sQFJpQShceBOZOe3pqsbQn7KKyy2oS1bxr9I4hvQ5YdqVNdFZ0o/WYEj1CgHFbzxYrwzJ4UM2dz+3SyU5RLj3yDCTTHNoSjOtXhRvErWVcYtReuJULjSfDvPlYSf/rH97+JnsSOw7zgy7xTjMGCEAAAMC7v6lsaP9OkRvxgmHUnLUh+YUKcXwC8q5ihXPM43tigJjcm+G5KgM3gANFcmUmg4S7qNp+b9HUSXWfcmy+07KMvnUCjE2IhIFeovzop14NwEKBZ50TCtDE1dhTiptr/+sYuW2nbxAPYyO4mT00IPHVH1Kn7TEpSqAJaSPAhvbGKxbNnAIbVM9ChJq/SRSATzhKYhSuTL51BcQjOzfYUuV/osWfdddgkkwFTihePj4m6lV7UM3N5oEYDhfcMqiv683ILcuwg3/1A+ts9q06PfAzu8R8ek68JmzKAwjNjOVPAq0s64c+weppt3e1SuElJp01d8Qa12mwUtg/ezwsBFriR49+CRPA0ZtyqcnoYcgXCmqs0wVAr32rdx5o0CYva9f4S/SlMutpDkZAmTrN1Q5K5TJBRwcXYPDeHWQSxkwYhh/8GKZnvAMKG27N0XLJahwAS8KZ0qsRBAAVIDcgQAAARpBmiRsQr/+OEAAAiq/mgDFcV66dd5vOSl5fGvF+Tnp/VD4wucSZzVinnGXmRSwX1PHpJXx3m3x33PIhJq14Xb4LsAo/YtXVxkW9Rmd6eLcNcTmhUGlMf/+ch4CR4yaBJEvnkYEueJvXy55T1DWQMwE9dLReVN+Vf1U5ntHMRC6QopQS6tEpWYlOHLYDKGGLT5LjxvW99YFVbL51bs2H+KDRW/bqPpeJOaQphpNpfURu2HjL13C85LdPnHqpciVKRyVQzSzzl0WSs6ZffZyk4PtH5o6eYA0N7IxQ97W80ZViCr1BalGbOh5R29oGUDMPYC9go8aJD/QfFFXofqO0nT6P1anutw6uWUWhiRqaEJcw2Kr8t+i2/vgUPAAAABCQZ5CeI//AFY92nMIJsIg4k76/EEvP064Dmb3Owk3VPY9ADTWRAko54gEliYdTbUA+Dl26+3XjoDKRUay3SK/GIOxAAAAKQGeYXRG/wAAAwAAyNAU9+OXdXEngBxBupBA4YUWZDNzKlTpGl3QFEk0AAAAPAGeY2pG/wBr20ygADiJvvgAqWeKorr9zRc5qXE1v0JsLtuhodyJPIHjdQH/NWyRAj4vdAqmDJhjERmVMQAAAORBmmhJqEFomUwIV//+OEAAAaVoGAFKbxDa/dtxTELJa1mVYjvFFLpfb4pvb7xBrstezRrwC8w5WpWAJqJBhMHw8TaYsYjgXmTW1eqJ7RbOfyJwkGX4pzd1UvaO6Rle2O0pgI28Rp1GsCgqWf6Xzrbrn/nqXO21uU1wc+t1w/pI+8iypNN62dKTXsRBVwLcdWzbuSrDkqIlcKtYr4923x0cJgysCzvJpuacvfcb1jPP08bu6+sGrSrw+jGf0y5/KynKEtbHhCc4BIm13S/1fQuF//+bODy0XV3mK2fEPYRlDUUfn60AAABWQZ6GRREsfwBWPdpgACuZDL0D8c3soxeM3/YCuP8UpmfNYSjo2pw1tZyzRdFMvJNAEZ8HvBncFo0QJWLlp6Eia4OXthktwaAV2u94b6Dte2F9XaJqIjEAAABGAZ6ldEb/AAADAADNw1CUmFAZ/sjAAt7En2KVjp+BCQZ7jvuHSdY6EzFFJd7ynN6UfhwnwtyTH/gBFFVsJtphXOC1QvPoYQAAAD0BnqdqRv8Aa9tMoAA6DXiv87eUB0/Ru+tP8woAbYSPz5aPkku3veC/k3niVHRJRXdD03KWP6/bGHXydT5kAAABcEGarEmoQWyZTAhX//44QBFA4oBZWEo8RxfBnyk7E9rNtXiprMSexO/XngS9A0OZYE8EvIfz58V3tBa8YudYdMFz4LFsXkaXDKJdW+NUB3CMlTxX6AgaLP9MgN9gmrf3mhPcShqEeRRzZClevVys7rgA7t2+gqhnu00D1CvwD1rgOcKYNG9w/H0bYhQmI190uN2QH9V7M2XpgZFJtMcSPMr8ffr+kQbzXijQoVUiyj7OY96rpLd9bvnrlMCsAJGfb9HSVy/tScAwlWwYOmjahSZM8zkHd23nJZBGqoPT99XIlShwFxGc/bvsg+4CXNR+CePGNQ6bpuMsgoeZ6XO0fm+F3fDrDdRvF8U7bocJB76EV5PJytjwxuvs9gPdchrH/z6/7xo5sTvb758xFRUlxfW1rHtBVAGPnvCclKYoCLCCrWrkVEq02cFlM2mAtqi8EXumY8GGiQ28gHhBJB38UQ8wf25s/wVCGFyuf0xGJLrpAAAAckGeykUVLH8CK+wJidZ6CB7tmmYoSY+A8Gvj9Bo8PDK1yxlVrRVqU6bomcOAALYl0Z5HPVqgDGuUg8Up3V884XgeJdiRWoR38OI/t3XhALCKaw0V5z1ZTbN6DIoT/mrrqkdw+hWTGqyBKVwhSMWEeiKUQQAAAGMBnul0Rv8AAAMAS4eRNu2qxYi5DxK3WmSM06FKG5vwsgA3Ku9Gv6Wc5iXzbxjqALSDPE04+7j7XPoHo0dWgfEOJh8wTJr5tKjts+kjAYzhiP/Zouf19ynO2547E0tofxD73yoAAABqAZ7rakb/Ar+EN6219VhZ+zThgUhN7McS63jPYj7WT+FgBwtmTuFoAG6KoArsSSbjvo4gd5HhoeSWHFrev9NEFqQgMaztuTkcYDK0y9yc/+hrV8sjvuap0LEApmYHdB/JdBR1Angwsx4yoAAAAZFBmu5JqEFsmUwUTCv//jhAEMGDvAErazCqKeUdBxfRNF90uEIiLV+vKKKQAUzPYSoXVvY2VxWxQdgMgedm87ZZJsNm0FMeLly093PwQfVqQtuhBkbI/5AS/qMbEOKhctCxZp9XucxCtPh0Gx4/cHP0hcuh38vKjeZGa94RrA9NbRXIwDLv7VbDcDYQjfThmVtkrTcqJHNHZeTbLQta15u1aeulSeLRzes6/pYiX2u4djCODqdqzRAuCuUBYdapK6xvailT8w3fDdGOyVTAsWGE/yd3OATyRkeNaxTtwQ71SGj2fAlkB2K2q5RYFUCTqqmSx3GbL+2uHDxqEWy+VOkpBETCblIBpBBAfJi9JZid51phHTZSD0MKAvjoOH8uorYfU8MrLHNdofd6qa8RoaaOkP2X9RfG+3UI7Csxi20PQRW+ZoEXkNTzenxKxGZolwRmJgYtXCLCjUQnbInYBfkeU9wf92bW5yeZykM9d5eo24g/Y7z22Xlflj0hHnkia0l2vu9WcmBDOILpAZDRba6aywAAAGkBnw1qRv8Hnsu6P+Wuya9DFGUKcMjm4Iudzzhd0jRcoLg+SZWxNywneCS2RjFaWffxyaC3oooa03sfiWAOQhdADaUfBHLy8rcfX7uZyBL7fM4xs2jOkgotDpVYQ2MqmqvvYZa6BoqtD/UAAAE5QZsSSeEKUmUwIV/+OEAAAFG7nKBMJiIvy/yLZ6Aak4WL2vJszp724oBuRHCarjKSC8lHPgCjcT33fCBEZrdeIIy6V/Ct+rPnXttOrJ0Cv1nsTv9eFetKlhpKUKxFGfFRygyE5yeWF0pA1eOW3ZcNR7THYGiVBv3bpWtRYzLCFwSpdWR3+yVgvosgmk+aHrLk+t4h768eK9yI6NIV+jBuWuCvA4JcL7SMdoeTEiK31gY9XONYfbFcD2m5yqilSq40WgbD8nVu7Az7ncItBWCtvlpBsNOZsZNXzSuiz1nF8DKWWJIZVRm0gv+YLwqg3Sc3Xi/zb9t/pV4Zs+Ytqk3VZm4RaNlhNg9dvFZ1kPcvwrFpri5BMpnA3g67BglPzrBVf7vmoJcrsxec9nkMZvj+CkXhZly+VEbgnQAAAIdBnzBFNEx/BjYYfXHe2sBgrUClManXBRNSgAADOIKetf94j2DXkhttGmTq7gTDMP1K/H0AIqevEApfZSDDpAgOh1uh1tj7bFo+4vtMo74TsQr0XwWG743H/OH1a4KHMmFyzf/UAdstSKso3pSVKXou0ZJG8UGgVU7XTtB7YuIdiULKqpeogsAAAAA9AZ9PdEb/B5htcRGBgxNsdMYDkTWQZUdM+G3adIu4cJpDr9orsANqwYHLal4WzpwEe9LBMH/zmtz1OP0z4AAAAFUBn1FqRv8Hnsu6GyF7TXmopPQS1karK5/1i8SmYh2wjUUALSJFOn663t2AmQHzvNMO7YWc2emaKFX96VAt/RXfWx1zgMLPqRQulNk5AjD7G+BPLNWBAAABL0GbVkmoQWiZTAhX//44QAAAAwANMD7NzImqjWgAVyXrsto2dFvePQqXJ3gP/pHpw6yvxJmf9tH62Y1n2XRg9xxW8DwXy1aPP+X303a9YY1Tis37H/BDmW+k+X4kFH5E8t3Z2nEvTebYxPGSxTImpmMv7SsgUUYx8kTo5jHgTI22LrEtiKMPT0OLiLIvqMr89jdhwQrQI9bOgrMGPOsal+Qu52NzZPBcpfLtC9gpE2bp2FjXuWS4nQD4U6EB6RbSbVXDG+DD+WsFDrB2XBiB4F96JtxgX3WeZ4d0s4/x3Zk2aUzDtKwE+3oaeYRyFbAdtPViuhGHXdfW1u7mbSCsiJPpPHGlHpjRZpfwkhCueu1TJVPddYBX+sfrvkkoRGv7ZMXqS3KqaWYsecNOxO7yjAAAAG5Bn3RFESx/Aix5xUdZ+VvAQ4ra1sEZ4Hs62hBodEwne91xJ5LHHcwAGifNuA/UJwmHJF+C9YaLVRTe2l6v7fPdaYxVI29g5InwR4XmDodOY/Lm3mvyxvAaP0Em0G7OmXe4ooB04s/8vh8g7y9oYQAAAEkBn5N0Rv8CveFV8gxwpAAA/lIZMJIh8KgFMsCtACuPHytB1ET8ygluUQotgpwN3hhQJuDOpkxpNCJEfPK3pQiiO2yCpwJUPgpZAAAANwGflWpG/wK/hDettfSkAAD+iuMQDca9aJtZ2VY39/icK86EXxdYwAU8x5+6Ii5A+h0W28j9ZWAAAAE1QZuZSahBbJlMCFf//jhAAAADAA152hOIeABYUXibsZ33WKDTosUnpV1zjwwRCecyba9cFkA6SvAVPKoLjRn1wLBPR6LK+twh4XpD7n/ofEF5zdpq1egSHrP47tVMMf/Qs5G6N1q7A4+hCe/GcLI+qKE16FqXqph/6LVL+8GQEyFYzAd2UCRg8BmBtYSc5Qg+kOcBpRQEWOpFJALELOhnEPXGgfTW03iZeae8rBsXExGeXSLMGhECd0jJ1V1XP6a0rPLQ9nFZGQPSWreA0CepFB5f/4CODaXLNb+6RSU6Oj7URVcrJeUn7vHoaAWjICkrt/+xlPoZ4qF//ue2/uZcLv8BC4p1EtSc3jwanUGDd52AvwG2qzRUeUXji0ODUg+H32JpG1PRV6s2Q2BcpjCBly5fKhfhAAAAVkGft0UVLH8CLHnFR1n5W8BDitrWuELR8uZcKLgdZo6Pz7KGvMSACNfGfkGq9zV+E2cx8rzLiespvK7K1uGEr1p/SB73liFVy0zDpuxevYzpL4b368qHAAAAaAGf2GpG/wK/hDettfSkAAD+gM0zGKpsbhQV7rgs3f+AC407j2C4Po1FIJlLvaYPXJ+esywbPujLky6CUWo2HBB6SEAfbRtyT9IkleIj36QJixWE7wiZW1zQEnTaMyxwU6qZlbgw/YKAAAAA20Gb3UmoQWyZTAhX//44QAAAAwANgECn0K1GDMABx7TmPWPuQaOChryJvjpz76FzXR+w8CRA0LP03f+00Q8PeeHYI7CyNTK8Qq7CAhCFu46tKo9ywMncC9hBFXKFzroJKrbKeuDY031x8AW9e/PeRcJFpJ8rvhI2L2n6XwT4P6LRNviosN3dhDSlIp/7//sTooRKgy2q6fpnrm2VE3pi5Uq3se/sZMJSri9CDGZDwbWDgc+bwVVNncdPFvU088ZKTmwnYI/Za7JNnnjxxsO7c7gz75wj1k7PkdhV8QAAAGtBn/tFFSx/Aix5xUdZ+VvAQ4ra4dzCwnhiKL26Hi/6vwQBfdYkzbFWU/4G4awOd3iyIvZ1nLbzg4kPWeN4DsFaCsMYH/xdpPbbc2oDPQ+i/V+1TCj1p+IvTqt5cvjkQcVDWzSEy0BqxBXvKwAAAEQBnhp0Rv8CveFV8gxwpAAA/UL2oL4iYnd1iR/agAWtUThymohtEau95enDf5Zxq8MJfPmfbeHEFtQhcYs+spbyXLmnsQAAADUBnhxqRv8Cv4Q3rbX0pAACs3CV4UoazRjOk8rQq+CDw6GANwwfhJRs5T7ADStY6KwjW7j9nQAAAPVBmgFJqEFsmUwIV//+OEAAAAMAI540FiMQFCFHAtUMY48frCyFVlgI9ucoEMcARquYC45X36xf8E4LK2RbsyGBqst62jDRSkJSBZ0/hyHIzc6Jo7ghC89UYAkQT3GpkD1MPwTmGoS5UTqqxnHKOotQV4wRqYXaBGEQGcwe1rzqQDvahjfXKTtzHnCeCuX6+698sE0ZferDMaJCJyZum2k9dHWK/chUWJC3XsHO9nY0U6z54wfYIMf30qQpXWwMCSl2ZvxHUjfdw7Iv50QQC032TUIEvij1jhbB49Tjlu9wsftKz26J0TQ/0iSVCvrkqGWx72ELsAAAAHdBnj9FFSx/Aix5xUdZ+VvAQ4ra4b/r5GgNgKkL8AADlCoMFqdCe7FKp5omfr3OcHjj6tqNoqEuxaek6DCp1EOwWcV7lW6c1QFWKpBnQBiuk+jU9HihISezvmRxGxP47d3v5CtiS491gheXt5K/n/7DXuRqEnJueQAAAD0Bnl50Rv8CveFV8gxwpAACsb6pXLbse0eY/l+6QR7FLVSs1kNPfzpaHYKFn8CAEGDd5W5yz/yLXSYOKEB/AAAASwGeQGpG/wK/hDettfSkAAKxDhQzkBPPby2glC3/DwQAmsBp6OAkARuS0HdnxUT1z8UupzvObYhZHyRRgBHBazoL6sm6sc3BqIbLoAAAAOJBmkJJqEFsmUwIX//+jLAAAAMACUHL07gDkrZ70W1gnO4pLn+6VHvZTPPZUrbOThDBXizxuZ8e2NqFzyjHzNZIp0sWXvAqy8esTe5Vl1eQdYGrMEn1FpjLRVsouOb3b5/7MJEryCZzXmF98Aa/70kWBRDZDitQjcl2BGtOjkveL+QSp3Rx4uRBIljQje81kzkjMnj8D4ri/e4WW1fsYhGLYwz8eLHF0/3ZDg1x0jTlP4RPHTVQKjbpiSZ2Fd26/1ynBN75GYHFrslo69w70HyjuqqvfpU8uGITbC0f7a6wvjp9AAAAm0GaZknhClJlMCFf/jhAAAADACOfV5Ec310UAOV/d2EVSwkzFOZ5QK5Nh36NaCkTXvJ5Ty4ZlbI+SROaTNQgcvAOw/bXBKUfRWM7INxkEaqHxUre5h7CLmuJduD/qHXW4GsHXFbJ5/tDNN0UJlM8TuF/+AThzYlAEAP7Zhi5M990KA/CN5+AJFO4BmQlgVTOK9OPRiJrHwSk8f8gAAAAKEGehEU0TH8CLDpCfg1nmuAhxGrw7abw997AxOU2OnbjrQ3lOf0VxOEAAAA5AZ6jdEb/Ar3hVfIMcKQAArPxfKii3ruxO2j1TABbucb1iHkQzqizADBzi6lttOmLY84V55lnA1F1AAAAGAGepWpG/wK/hDettfSkAAKxmagg6CTAOwAAAIpBmqpJqEFomUwIV//+OEAAAAMAI6qNIHOGPFhQ8Al+kf9nYkhyLe+1vNs8UPUFQzMt/FYKIf/CNIEWPfsg3JmGHG1M6ce1pMdX2a+EWZn5FSpdw3NJo1zrTzrgAFLUePq4yD+rYUGHScSDUb7kbply5UnwihITe0B3wtY4sAfMwKCXm7CkVDFlchEAAAA+QZ7IRREsfwIsecVHWflbwEOK2uHkdZXAooHyI9owAAhDaArDOyQLPhaUVDZ1o5q5/0Mn+BE4bLu0Ij1NteMAAAAhAZ7ndEb/Ar3hVfIMcKQAArDS/wQqfQNi33oxKLSD4kDHAAAAQgGe6WpG/wK/hDettfSkAAKzcr4wiHHTbLyrnQAlpLXG56EXt9a5P2QMPepPKpDOGeku0fcyaAPejNLaslIBMGu9cQAAAOJBmu5JqEFsmUwIV//+OEAAAAMAI90TaCeFKMBJW5wEh17uiGX1Zm8XpD0xcOmeq6W9uy0uRplZIORLDlXbF0pS6RRX4CDbxDFiid6we6WbeVFVGSPEVJW0Vn6DZE9E/nVD2gX9zhUiMQ37NY4ow/EUmOr4MazVHGcT1u3uxcyJC9x0p7LZXSLDnZKW2qy02KZsExWXcSw88ewxGmyr2YI2JR+tEDqIcmEhmbL8gXTGJVaHL7gBZPn9fooa0WrUJtboiY6htUss/6yi9GpBBf7NfCIm9FM2DT3mmAgn2CE72Oa3AAAAaUGfDEUVLH8CLHnFR1n5W8BDitrh2xUNAMBwlz5Imq4fFmADNxZAzNZ2EB0Du36W9hjV9Xs2iPOKwhRWITwLgCQyK8HXpNksyjXIgYFVWsxVqR7h3JvZztVs7dnVMRdxRcOJczNOuYU8gAAAAEUBnyt0Rv8CveFV8gxwpAACs4becC1VA/CrrW9U0ZlKEAEOK1OPBwIJvY5Oh/P2cDb6/l07CT1dwg3erVktAvij2fdiD4EAAABkAZ8takb/Ar+EN6219KQAAqrd48pN8qcHXAS0APb8kc/QXe8eXWpACKJm5IHxfJqw0grYynl0P3EYOt/i94xltmkmGxyVP8G4L863bpmJ1dP56CJtLUcoyeRX+FYnJTXNFHNuwQAAANdBmzJJqEFsmUwIV//+OEAAAAMADXngWjx2I+rvW78AchTj9lWfLvYOU9pBlaVGavUpbzuaAVgaaN5xkOAYeMfDlZ8RQBPwgUWmbPc0bD0vC2glb00b5QsZexliZ2Z7d3N2exWQlSp8CsdEIVjb5U7WOqpjROmi7qm8oMfoK0LIeD4Tgaf+poVjf2W1JFe8jd7N+yHsvJdxdnGwr27S44lq9v7iTIQkjmy5ZxbOUdGAzUmyTiiaZnXl9BIq7kWaa6ioxLvUBy1/eJUFti5miyxc71FC5DtEgQAAAF5Bn1BFFSx/Aix5xUdZ+VvAQ4ra4aOHGQOcWMXUy/iZlP6NaeYq0ifZBdICt6kL35b2D+KMAftILhuXogPE8UNfCqcWYontKfc8ljUj8P95r6b7PJEnc5VieS9C/W7gAAAAMAGfb3RG/wK94VXyDHCkAAKxOBkj7f8O5HRllDVMrKHqR1KP/pIARWQ7PlM/Tv7gzgAAAEABn3FqRv8Cv4Q3rbX0pAACs3CU6B1AhXC2ERoBPoYfLsABQpxSh3ubmAA34fvyEV1gXd/Z/CC4e2SY93tJdszBAAABFEGbdUmoQWyZTAhX//44QAAAAwANzPMp9qjEcgUZXwA02dgbvffcAItGAfS7l1gEJ2mTIRZM4QQr/D1k4o1kUpP6sSR726KY0ElgXETlnVl7EwOvz3MxlTV6rJxMfh/qFc7vc04hHU93g5W11RmaqM+SF8gXUpTAG9/IHsmp1Qoa02hm9j/pehOk29RadFyXsKneRA8bYnuUmkUB+An3X0u8qhTr3o9wHdNcw7E89TJ+PKBIVzO84TaiVZ0ej/wCgb4NdK7tNlnOmB3qQXhDZ/QDoSMTlzxrssLX9xxMnUdC9fkJ0yEvFb8nWI58Qp4Rtv/qVv19JvlRey7lUI6v0uf6VDjGV6x35sXuWBVtAdM72DLD1AAAAGRBn5NFFSx/Aix5xUdZ+VvAQ4ra4eG9rQtINf44dtnNaS6j5e8IALXB7hSYelwHZ0vGdGs8yqwQNDsUVtCQ8AZ1PH4lRM7DXxmlJeAyNgOrxja5v8JtsMpXBdEwfI8r195wDcHVAAAAUQGftGpG/wK/hDettfSkAAKzcNowmJtkiychmygAXTDTxLbX+n087RleXQ1XjicK01XCEaLJgNR/0sF5RwjkZesfm+GB1yLROtHgAjm+nwyagQAAAQJBm7lJqEFsmUwIV//+OEAAAAMADSnlaaItgCfJDJ24BBEeEdpb/oyGFJCduhHztvLlrBvJaqO8SUug2KsKpMxuDkpjtby1cyI9ELq3rFVkGLCGU2rp6c6xBuXQ0aiTd+Pk+tMI/Z5GGPFyf2bkl4zNLmDYFOVozDk5SI9fLNgVUL7ITC+Zpzk1R4k76xAtRU6l0KP/ME7yosWXcEaNHpl53w99R7nmxGOwPD8BcVoi0gH/HMeJnuVvkYTZcQhnyhEbOxZAa8pXS/+MFbP4Fxkg5ILig7xWWPcIDANo0pXXJuHzm923RVMvRXawuGVXw1dcIK4JjM2fYAr053k93qAnvVAAAABpQZ/XRRUsfwIsecVHWflbwEOK2uHhyKxbfD08CDvx5tCRsAIxYka2VlKRRKQ5xq7uGSDzVkLj8L5a4unN0Fbb8D6f4GnFd7qS6/B96gEguKy6lQnHH+UUzZzbyURBszbXtVMGlXbDMwN5AAAAPQGf9nRG/wK94VXyDHCkAAKxO9KBN/14AW12pBiBBl2iIM6vwdWpCmP6wPvxuNz4A+P2XVVs4m0RAw5Vyr0AAABEAZ/4akb/Ar+EN6219KQAArNyoedJh6NqmZ/8SWnqIgaAAryZIL/+dChgUM0RGhjrrGRmXPBp33yZgJJIC+OwjFMb/WAAAAEuQZv9SahBbJlMCFf//jhAAAADAA0+/9I0ZFtqzUCPJ0qjngANC/zY5L+65PWLAb5LUSoAqR+oU75Wn6kX0f5kLBKAl4DZtJRA67oATQvKW7ByOulQvbF05F3MuFjO+hxd0XUhovw028s6UIFMvpZDKAKB5Mg1tg5GEgBF3bwtHPkAEw5Y10Tzsvuehdy1cshGh1tNdzxGpI2Up7KC1W176Q8ZMJToV5S7L2HE2JoFrTG/Ovp0ZGBfGphwB+KbJKhq3BcfyirYsM94Dmub730WgXGEHZhG31Q/6poERZ6zodBz4F/jveZVfXjjEYKhnm9X4wl7csD2tqv+U/PbB8Gx0GurrgiZiuFQfj3ekR8rtJpbynbEnick4vcVmW/AVVAXEcLMPpZavI/QMhkZKRMAAAB0QZ4bRRUsfwIsecVHWflbwEOK2uHhyJJvnWlktRLP5MnfANUDZ5NXPDW//mQAWnXK/md7EbI2cCIIhri6uv0O8/in7i0p1tdLRZh80xY8EA0yYfqGka3Qtixs+IIpcJTR4SEJxjwVQ1ga9l/ica4T6zgvA/AAAABfAZ46dEb/Ar3hVfIMcKQAArEi8YYmf5/dgH5040A45TfV+utpbdkDbmyfWAAhfD+fyAT7bi0OwLaIHBiFNZv0IazoQ9tq1hLlWi27EIItPZsHc6sqXVSTHXwuzbLej8EAAABXAZ48akb/Ar+EN6219KQAArNpyakFsDhWqIc5yKDJfKE5YQKqQWEVaVVABvzLgVpxYZrX/288mXKF1QBB6lOi22qWgZz97BuOk8zq1p6mlBBmkXfWd3GBAAAA5kGaP0moQWyZTBRML//+jLAAAAMAA1FWbBIWAsgfyhq4AnPJLenYx7LvdzO72AnNiycH61uKkeCxh13v6lCBnjuKIsXGQ2BWSjvlmXJYSmKLlD/2iafSpr713n/TsCK04c1dAtkX8WQngaashq19QnmlRLk0GzA2EjwAG82315/HEC2vACNDOc2FdGgXVQyJegmMyvEn9Nx/xFpHgd9VeLaRhf/dZxskg9TVLu0nq67TPO9ZFJdqG0sWTZarWeNz3LGZxupwmB/sBBiKME8NrQi6J8X9yfV7K1JoW/dhkgencgaHjX9IAAAAQQGeXmpG/wLAUUnmb3JsZsvJahvXzO/yf93E+iJD5WowelaAoAM7BJKg7cpjvj3ZJHSuI4fAkbxalKmlTeuRQRGCAAABGkGaQ0nhClJlMCF//oywAAADAAFL5lxzKzTIsGX4bMVJfAA4OKWu3juI+d9LJ0cXrZf2uWrbK6rIP8PrCbd8yTuz1n8rEykfdd/UCCCdrKAWzbQyuxMSuQ0aKmhNlCuQWApUhE0khlm/hhwXR9apmrdUL8wZ05K/zZx3WDtgcJ/2AMXuIcXPJl00xE4lws0az7gFBj3xJ1t6eBoUuQG64yV7VMbKGBESlyFAL1Z2qUA8AUbpUm92ymAS/qRnmJ1iuEl8RnQYpdlYm85r7zMClszw6teAnIHKN+LyKnd4dXHoAcqhvXklPwXd80FmVUVOLiQic6q6V8wcRVFoulpl1Z8xeAffIwMGUxeV59OMkSK1uVktFo5hXKRTwQAAAHhBnmFFNEx/Aiw6Qn4NZ5rgIcRq8PB8sk/U9escjDins9Q9ZVyPN6Zs0VfVcAN0l0ZF3ZvCt09Jlowk+zwuIBMnGuC7BrvSESmi+I3TAv6JIxsC8CajE6xzpxQRYHsTKoxpmbsesdvGxONBj8pnzUkKXG7ye1LMQl0AAABaAZ6AdEb/Ar3hVfIMcKQAArEi+QLxlAl+PZC9meu23iRdEPBEsruSTwynkKHR6cq7cgJOBG9FdZRjN0kIsSSbPmGL/F4I9L9JttDYZvTruRJgx50LVqP6p0bBAAAAYAGegmpG/wK/hDettfSkAAKzacgkM/aWhOC3Of8oLqr7WsNk63WAFrTLWNMBWanDqhBuOZWNFX/m2cWAYF9N9V2bYJhYymEo/dI4HWXXcmAEGxpBz7iExAzDyIZ9AmSMxAAAAQJBmodJqEFomUwIX//+jLAAAAMAAUFOhiYnF3AG3lBtLPgYsFPyim+EmocN1fJy97Kth3bXLIU/YyidtSrqR+FRpDgeQGB/87CxPsRJf/F749ojqPven1hcwNq/EdLumwfFHKabxexewsOaDhH3h3tRbRDt0U5x7EtUAAKMf2qX6Lefrybay/72UEJzCk7+eA87pZH119W3Nmfkz++jnUczwfDSNRLLFj+f/ueOYGeGme/PMeKVWaipqG9Kxj0nGL8V/PqPUa3lxvjl/YWGbCyu27KukAYTFyTHyKGsK2InQBulirxOWA7pv/1Ro2zxG+aCaBy7j3sk3n6VfC+In0VAnoEAAABWQZ6lRREsfwIsecVHWflbwEOK2uHhoOFeaioeFSwAn2yaDLIHhCy3x8P/oFyIEI7x7DGvRC1w+fmoAcVksa0ZrhT+7pgkAiG04g8LmGmhaLQcFglE94EAAAA8AZ7EdEb/Ar3hVfIMcKQAArEi+PzE7Q1Kcjd9fcEStmR9gBqp7sf6SpUQD4/TR/gA2Gfv87pI27xw0pVBAAAASwGexmpG/wK/hDettfSkAAKzacgkM/bUSe5LCjGsOzNdJ3OOAFt1zyRPBnivLzmelbCse6xFuSqmkSi1GNFsP8AiYo4lrNfONl4iwQAAAMBBmstJqEFsmUwIX//+jLAAAAMAAUFOfEaTNkr7wgDkTxQtQTqNkbCeJTagGBXHlewJVu+s2uB37HLuB/N0P/+CmukaKP1hsep4WnHK0b75kF/wmCbITPWDN8AxyjV8lVebu9GcRRb6URd/pMYiugUgYfo/s1F4slywHD0MsXfd+Y1dPTzUgryw3j0B/G0jeovvoC53m8ps/rfUEBU2+7vbvHk+CZOMeyldXKosDpLBBCoNMHnxw5DqKa6zKwp7P8QAAAA5QZ7pRRUsfwIsecVHWflbwEOK2uHhoOFgi8RhAKBieSwNOFZfZ30RsRSBJAELuL8r+o1NDRbSFBTAAAAAQgGfCHRG/wK94VXyDHCkAAKxIvjzFGtABWyl1MQza9F143mTfEgmUhAOySWeKJ8ORYrwI0MAZN4gshelNiUc6g19qwAAACcBnwpqRv8Cv4Q3rbX0pAACs2nIFlkIJbX6sAHHTNh+5Ie5FjayrR0AAACXQZsPSahBbJlMCF///oywAAADAAFBW7xABBiJttspt6JK0M+AdY3czHnC4CQ0AJ8IShz/mn5TGFzavIJCWj61TqWre+zfzgMxp+e2FtCCaiY6spvrcWvEexEs6+FNaWB7uRsSq9sZlEhVGwDWmtGCrfSf2lc3Cq7ff8cmmz0CUN2RIeIE170zH6cUj3KnmWH2snL87QUHYAAAADZBny1FFSx/Aix5xUdZ+VvAQ4ra4eGg4V+qn8PlkChWKNADj078N6DJ+X7ZFV2pHspUhUtS6WkAAAAnAZ9MdEb/Ar3hVfIMcKQAArEi+NNN/8nZVkMntLkp0gLwX88Gebw5AAAASgGfTmpG/wK/hDettfSkAAKzacghUbqKiRIkABGJLd10HGQg6BagSrpyM/Q/G2l7lGfS9PPFjk8/lGuO32EDAPMd/uPCHfAZknGVAAAA10GbU0moQWyZTAhf//6MsAAAAwABQKZp58sAN1z6wnwor/BqiWOAALDLKNgdryNLjxdQWpG97VGEf3ovQTZxuerQB8MF7syGqtlzTnE9vCMte3uTN/1JgQ68aj3i3TuFgAsH7XEMUZQiVfNIp77WGQVv/KFfg6WrRfSXgRppsEUdJbct++R8TDeyYE1sgkkskA5pPuV1jsW9H/vsq1fC3BtQ9EvqIy3Ls2iU7ecFNz4M5j1DIB0+NQXjFWrjUt64Qom3K1zekdpgvh7ylFSV70Xi0aq7nQy0AAAAYEGfcUUVLH8CLHnFR1n5W8BDitrh4aDhXN3MhpyPiA8xCPZfbX0AEsmEhqxgv1vv5M6hatBSbwXGZWaFKAG0AkO/zJzinDhXsBjpQHaRZ6SJbqIFig5FfuNJeBhtExOS3AAAAFcBn5B0Rv8CveFV8gxwpAACsSL48fHTx9nRI1jf20PJTSAC4X8GP8wD/Y1owfv+xe5rFQOhCpLXeA73nUI48+RJ2IZ7Zn3ha1+tUa6fywMjzV2j4OyYW28AAAA7AZ+Sakb/Ar+EN6219KQAArNpyCIauH1mlWIPYgpCsvPOFcwdisKSUJ7sLEmAj+RQA2AgnGwyFXt989MAAAD5QZuXSahBbJlMCFf//jhAAAADAAT/k+qBnwOuRDv56HSL0AJanioEBbz8w/InwzJ0LThoxRipGSz/XTK7MwwepXUieWJiBpDzR2PKP3dXPwmteO9ucUDSmpKioRFNZXU+80Kl3EaQ0Q70I6h7hOA9xETZzZyIIZihcbq3R3SmBBDMRGV6Cy1q4FJUekr1Hq9mY66eM8OA1mvmeB/Zsc3S2i3Jo468cP3SgK1bAmCnOG4FeBmtublKL/cT/uehNE6hxb5z3uZZzCsApve5X14z2UR1DvNPsXaTB+LS78ppYdqk8mrCp/+0HtUAecZNLaDaqg7tw/OX1V8QAAAAdEGftUUVLH8CLHnFR1n5W8BDitrh4aDhXmodqaCes6VizpYZmbdtvQ1qCvg8ADiYP2LLkyRyJHxmTEht5WgJx5/mspXKYf+LUph2F8jn9shKcXiGHvq2bnflTIYOXA33W0GL2uN+lWj88jPP/2jtJiz8bEn7AAAASgGf1HRG/wK94VXyDHCkAAKxIvj8xO0KefwY/XG+T1XOCggA4YhqnCQqJeSQBKwoHNcSYf8PVOP5h2f62/rAzpVBEyB9ns1vdv63AAAAaQGf1mpG/wK/hDettfSkAAKzacgl4lJqe7u9zFjaIKRngAin8dh7GrhAY2yzxuFXp8uTPVJv8nOO0hQ73GVBqwi0j3BPEwqMI0jO8wSbUenrxrFoUkUvaZNFOQOuJwqiVpuJUQ1gkPNN+QAAARpBm9tJqEFsmUwIV//+OEAAAAMABPyV+SiTQGoQAiWej1J/S4YNm55gQYLJOA+PKeEeHbbzoz8pofif+lmereJZaQRiaAP33x815t2B/wVEUDUNcL5K+/OpGJi1OxZcxniONw6E/AzXTho5zytjh9U6f/zMMbcdJJqGJ478MkCEWMKKACAqLPhRZtGxQAyLLVVPtQOGQ4i/q6g7cspymW3B05mvjDzjLH3SzvuE/NQMoYwb0H4NN0P1AJxn1OZ8ZSHiR4YtPwvJJ3Tg+RhkHEZuBqNGS+MWycbd2yqdKAC8JiVmqsOicqMIWIo+zD654mUmfjNnONK8lVXfHRnMdAWxvnF5WDNyulls1oLtah3B9LSIELd/TP/52h0AAABvQZ/5RRUsfwIsecVHWflbwEOK2uHhoN1KThaL5aZuscB1fdiNQdctOIXmGUANVxwkp6/jLBEr0I9pZhvO4+veb2zmFxX4OHSLcmZLv1uQTwXc8dD9fVanyX1/QlGLc8db3POQfFxaR7FK9KaHKi3wAAAATgGeGHRG/wK94VXyDHCkAAKxIvyR9i/raIh7JhSxkInYHgAa5hqvTtx+5wlczFkh3WYQIXtG+aM+E9vVfKyprUs8whT8fk/azhmHclcJbQAAAFsBnhpqRv8Cv4Q3rbX0pAACs2nGmydPKk46B/lS2s7raDYSqw+Kw7yQ+oAN9nwSE24p28bcxIiqZhNFM7aBts3oakLk0DScaCuBAzf2m8cC1pq+VvkRJ/CXJ6CAAAAA/UGaH0moQWyZTAhX//44QAAAAwANKePlBrpAFbn2IVhzK4Cf4sN4WRDx439yvgtYy9ClgHcXJdLsK2IUT8XLR7JCqvYQyO65Icg0vsy6TErbK2a0NTg1UUOlXrdXYQpw4CHGHBecPtHS+iws6baIkjwg6J7Q484eK64TrPPnQ0W1NzYhZ1lk0ipiBdqggE0R2DYjM4vReH5Xfr7qeDgqsl5FL1R/zW7w8mONXdOW1X5y2ufTm7m8HmzCKGuWiGZPwYDv7KomJY9HUpMVS87LjDGXltpWApXZ+5dceIDlYvR5R2B8g7E9by59ZX808N73IhHLyrW67TpmlW4uT3EAAACLQZ49RRUsfwIsecVHWflbwEOK2uHhoN2jRMi9AEtW23Emt43QWCJ0KTUafD+1CJnXqd/SXKkgq5iOtt3tQ4c6To01zbn8+r3DELo7jveR64RGYJMboNtOaepVRsUHXNKAo7OfWieRyDF2wcBGV8IYedD7KEeCEsp9ZENk7IEmdORyO1n8xE50GgAzKwAAAEgBnlx0Rv8CveFV8gxwpAACsSLyDHXye0kwrgAWFJc1Qbipbgk7eAnoRotgyf+g2/rhgv46rbJsUeWhOl6pynyYD1CQKxM6yJgAAABbAZ5eakb/Ar+EN6219KQAArNpxtuf+riQoBD6CFHACUtaIuL48pScv0SeqFOXY//iP/zidydkfFxjCI93Ck8v+/IQAUEZbIm+DdLcy7Z1+FWh/9b3AG9wMbe5JwAAATVBmkJJqEFsmUwIX//+jLAAAAMAA2YCMlBgMr3OACz+oS/hOdch1bxJLRfoXr1FEBQdZVMiBhDbUsqknogRQ9L/Rmf0N77GpWSOb4Lo4cVyUutfeKXF+/zWkuue1+1ZrBE7U/pYlq4eTJYRVainh5J5yenTt05W1xoCuy5z6WW5b0spBYFmIhKx1w7h09oeqZtxVgMXGgRAIjQsjraoMKAw/zfXHQyLnrfBXvTlTobslEKBytnsnS6BP7KlwAQ+dfRcdLI7G/qzT8aOeOeFpSVE+pBuftDyhrv2p8h43QZBIkeKMl/J/uQUK/JvHJ0x7vTp4bwuqv9KqFXC1kW58aLWwZiifma/21ok2B7pLJRiBY8vGKZl4K8pp2PzMzsFjY3NEWb5gZ6j4OQUkynWKS7mf9NH1UEAAAB4QZ5gRRUsfwIsecVHWflbwEOK2uHhoNxbWCnEV8mttfxBDWgCFNrz7KB/Pt+N4w6ljar7CXTdwKXOF3aG4ngIHm/VWPOWcMu1+nZhb75AhhdpgvszPHIR4U+wXWRvtDosmQyi0krCLhJhlp9XamrwlpN2k2ji1n7IAAAASAGegWpG/wK/hDettfSkAAKzacXxOoJAwBqTArs3/YAHBGTRliBTMvSzdJyhrKhGVXy0OOIjuoKjiBG/Rf/5qC49SHxe9vBrQQAAARVBmoZJqEFsmUwIX//+jLAAAAMAA3wDleidUYUBACMieCZeGEAWt3T5XROyHysmgKBqwjpPVfgAEFOpW7qoVc2ZE5dVcoQzl3Q85sHafu4Hxl57bVBewzV7bW4Plls7tjCPDfZ4oloHtD7hBwy9h9ITlMYYvbhNMEV3R8JhiO9ZibhmjpdGEknMTnSUWEweSjJULfrMQI01P8uSR0nh156pQFETk3n4fFoQyHZ2qQLFf3slbz0l9YC2jgsu1a+AqBoE9IdK9nC2rp9aQ7TNUdqYkU65qvS0aTrY6qWjeOEFeVSebzVHBxiAW6IY+/vmVKavX03y5GwdWtLvGsbQxbStSeZSkqtacNRMx4PwK3mz+88Jj06AAAAAakGepEUVLH8CLHnFR1n5W8BDitrh4aDVZoWkrqqNRT4nLb8mnePiAASwLRh4pGHgDr7gZ/VvwnBjb90ivGtTjfFFdMK9eBUku4OcF8Z2m9B1I86FhByCUW8C/HOy5q/t7H6L1A9RIND9++EAAABPAZ7DdEb/Ar3hVfIMcKQAArEi7Mr2X6lyjBmovVgJ7v0SKrABGkIFm1Ni5HgTxLBRCpOt+RIpkDLFycFV22k7J8K2jtv5Li/mahWrxbaZOQAAAG4BnsVqRv8Cv4Q3rbX0pAACs2m/DcgrBuRAnn39tHvbDSor9zIAWtXm+jD9YpzRoQqoyCL4ng5kH0k76sPP+jWZ9Z6hFvkmdYbCd67u4rSF2EhhPSEyq4NZCa1shOyx81nyZ3GLf3UXeaX9xhTuSQAAAQxBmspJqEFsmUwIX//+jLAAAAMAA3gJIVQiLt406+mADjIe3zWXPMfh3LDpz4kalZOH8Tj+9HRJ9ThNmwj5jw3ytH8Viyq24Dsza3KgfjAP0gnjKXoXXbgAgaBOk5bD6C3N5yA5zmYcWE1EHqCTNhw+la1+jzc+cBcgO30eSy6/H3Cgt7TrK47fyQkSQG39il3iiZPJanZr8EhhZQEu7cHSmz84Se2N2ZdqGClMblaiockBvfWE2nwFhkee0FSLs4Szq5VDy5vK4S2SKq7tn8eCpru6cRZBKSG2Tp2jY1VHhBfqguJeMTxItlBbca9TK64rW3T3tlKU28VH6reBBV2oS28ddOfWnfmZ47L5AAAAUEGe6EUVLH8CLHnFR1n5W8BDitrh4ZDyeiWCdxHio+GdRcvajNoZvfIkFyVxI4WZFEK6AFyruljip5DiBnKexu1HTvWv12duGifxHz/b+yLYAAAAWQGfB3RG/wK94VXyDHCkAAKfO/yJIH9tQRIqNZ8B6wABtktwtrCFIzOZotHUZlkBuJmB1bHlG7cI/p0r5hy9OZG7wiJ0QY3gfq82wpB5orpKspTy7vhnmkvYAAAANAGfCWpG/wK/hDettfSkAAD+gV6zqaQz9Q+Tygz0T8O0R+YABwq+nSOX+ueybql6J8sBmt0AAADwQZsOSahBbJlMCF///oywAAADAAlAHo4ziAF/SXtAJEdTTwSF+prq0I8wV6gjdOH0UKaIsYxp2DRL0vP9zNiHBQoE3E/QOqxzemdap2bSY94MBUbsgclyVamt4RGBeygwTu88cm0yOrWD4Z1M78f4XF1kMuL4uoHFvoaKGYd3EBiqYfwQqbNTeWQxi8gzRzeSn4PIPjfAdVlp2JiTNX/7YuV+pVisVL+NotgxEslz386InUJvrHo6O0umSQ9gobNwcnUMVUYDLG/6jLXjgYzUjytOS7MLcx/WGoArb6yFekrshzq+lrbsyDFSpLKLjghIAAAAbEGfLEUVLH8CLHnFR1n5W8BDitrhzuG1XSI896HQ2RwAPcF8kdOBv1vm1Y2nacYreS4956bXikFeLuv62Zzhr+0tF9qpfmW6HNOEKuoWauAoxYUXDZw12UlGn5v0lMmsILZ4dursV9SydgqS4AAAADwBn0t0Rv8CveFV8gxwpAACrudKmoQH6e7S/xkVX7gwVppyCLyRHLXUBFVyzQANfKuSRQP/1/dKWFiYSdMAAABUAZ9Nakb/Ar+EN6219KQAArMwkr4qomABEcvbLw/0h32Hf3vbxXWwH0R9HvWkRICe14L2eCMs05YlA4qM+k6415fqiH7Mtm5/w5529j55t7livQ25AAAAeUGbUkmoQWyZTAhf//6MsAAAAwAJQF4OPVzM7v6/ABWwvI+BuUlEGtOSpDe9S/hiNduh37MGAMiw/Yun5dN2k+DyF82OQJnWIH0OKyqEkb18M2h3E8HeoeNNArpkYsAma/5ezLtlUzHItuFBzbQqF9kegkr/T18jS/cAAAAtQZ9wRRUsfwIsecVHWflbwEOK2uJcmw/larNYxbr3m2lsy8KuHxY6bF9bB0PAAAAAOQGfj3RG/wK94VXyDHCkAAKz8Xyoot67XqJtxdABbxXr+0/ofxTu228KE6jjOIQ/abfZ4jHYeUHguAAAAB4Bn5FqRv8Cv4Q3rbX0pAACxWWliac1ketYy+JvseEAAACfQZuWSahBbJlMCFf//jhAAAADACOqjSBzhjxYUPAM215MOe+4Vm245io5GLY5+pqPxSNmsFEP7Qm4RixrBq4nMuWNwi0GtS5tggApeBfr640otwBIMiPCyeprzQY/CP/fNBSnvKNnV8MQjLAezosObDYcw+yUNx7WA40te+PfHc9kbzFouzc6tcy9MZ5qnj1QFmpE8p7X8QHh+4W681wgAAAAREGftEUVLH8CLHnFR1n5W8BDitriYaV7raNGhQ69ZlTNlkdDn5V+fYwnL3AAfYhziJTBk8cSlasN64sZDj15b5hafz6bAAAAKAGf03RG/wK94VXyDHCkAALDKBr+LVwVl0AHYKZnwKBgz3SgubU7OtkAAABeAZ/Vakb/Ar+EN6219KQAAsVprzJOSjrkQAjHWFgNXQPCfFesFDASigUDQpJHQl91Xn4CyyE2djhOMlT9QOmvsvjN4T54kEBIeQcqzlr6dG9UFgpBuapP8RKn6J7pwAAAAPBBm9pJqEFsmUwIV//+OEAAAAMAI90TZeP2F0xAtTsh4TPIQ1HZRVyWNUiyyqk+jBHbzm80fk9+wx014mK2L0TbrSOLTXIatQbqxK4iMhkJrHr2wNUrAVmb5Iks4QhYDahwA3Mvy1ELZqMOD2J3CeovRYtd/fyUrL+Vo151FjlS/RKYAByl7uVJIcVjVWmLByqhsys3yn1PgB2F1FWXLFYJX2kElopYUPgiuT2iSZXZRYVEwWjeL/odCkMu1tbBULEkjWPKpBUZLEXet+neWcJrolU1Fdm4pgj1lFsGb19IWEj8DWJylEI5PZ7wrUvb5EEAAABtQZ/4RRUsfwIsecVHWflbwEOK2uJhYSz0rRZb7wUFLWaOl0i9AAq6LzcaRoua1oxB4YT+qQM/F+RfVUsod15DQrmptk8INzYpGChPZMvuvli5dtFL1DThCTcuFjUJCx21DJppwWkVyBIFt92hzQAAAGgBnhd0Rv8CveFV8gxwpAACwzI5qiSvLk9JoLRhXXCVYAG1rw+j/bZRzq7eajzaQm0bhX55x68bLCXuzbvj0Aw/S7iug1bdzUgc8BPxL6YmmFFGFow/peapRlCOCb40+FCbzlt/s59TgAAAAFEBnhlqRv8Cv4Q3rbX0pAACxWKD5J1PbMf6IAVt4bggmYHg5xFgGDJYh3dDrnrsJbW3Qd3s7FZkuDe6C+6NbTeK6q3uHq8ek8/mivx0Or9brKEAAADIQZoeSahBbJlMCFf//jhAAAADAA2AQMUqY7SO3KCgBYGq2lbVmNZ1GFJsda+udD+iD7tKkxcwo8ssz5iOHwM9MtRiwt/QgLiIQp7XqaZyG8BYJozb7w/ixcgEampgjZlDOgc2HzWOT6+4TOgbhb21CNvOX/mGHBnlnrPWdv9J2ufAGHlo4SWBAkgexmGosdFGxjQWEvNOnLDlayIjoXoW6gbojhWB/cHIHJlWOqgUYmVT2odq4ayzuk/eDq+3Vv1YB8eG1L0xVIAAAABOQZ48RRUsfwIsecVHWflbwEOK2uJiAHf3OmXnFo887S624AsoYJX01XuiIytvP4eLHzlUv+J28PlwAV04wHlIYJAQw8vOL6vyyzUVWz/BAAAALwGeW3RG/wK94VXyDHCkAALDUB5eIC0EJ5eu9wuCqtP/yzyzuQKYvGlv1BF6iqiBAAAAWgGeXWpG/wK/hDettfSkAALFfzS5E1oMiUCuCoWX30i7aogA3XOlv0R0TpV1Dlp1ZNU/V+xoIDZqW2C1E1MElc2RC4bwNlQKhehedP+vIuZsJ0HR4e29nIShuAAAARFBmkFJqEFsmUwIV//+OEAAAAMADY/6uABc6pWdn9mxNxIGKS5m7nHkJPNNDDNBRj9MOEdEkjpixnhBll6G3Gv5YYacaARf2N0q1sUW1mdAmdV1CZqzISqrTuN8bPjE0SpMmNgAXNdPrQT8fOZO50X1qOtX5fcd7t0QDxDbqIYAq8fCMiUSvQ8xJ/INrOTvYG8AMDjFy2wwTDfopC74PIFXYmN7yXs+EUM47qMkIuXWr/WqXnsiRvXRHq1L8E/NL0MVeNNwUVvUPGst29a7fQFEa2p9m3kazUwhCs7G6FBVMmf/KG1R6NHEi7ePQ2phw/JCH0yTH71EbgKsmM+acDwR2fXjTXUyzN96kUeWvPfjPz4AAABmQZ5/RRUsfwIsecVHWflbwEOK2uJiAz7hXP6RtL/r/I1W0a8Inv9fQAbclvG/mjAoek1iRchhwFB+vpcFGlG8WywlAi+8P2Q7U4PA0+sJ6aMg2smmMAHlR+bPY7/Iawj2wn2D7Z7vAAAAVAGegGpG/wK/hDettfSkAALFfzS5KJUvBrJlCxOW1QALphp4slgXtmt7RpY/WYToGepn9hny2FW4AT+Iz5c8K2sEZeJeyEdZ8ba8wgw6O7CDY2fBogAAARpBmoVJqEFsmUwIV//+OEAAAAMADSnkZqItgCf5Kf41pnU4GzHg/+ZNzzRVemaji6Y2Elf4kxNaaZ1rrnaovMOF916dkSZVrFiUotFlB8hfpWs5RA1wNRt7iMyAe2dbEmjvrHdWx3OSNalVC1zxn+Q4chAWfPujoTLAyiKlQbqxqiVrIV3jWE+b1Xn9DTgFOuMiPFG8AZGu0LIDS3XkTKqPwbpLmN7HJzJG7MHRViNVSsK+84wlPy5xMYhwqmCal0h+k+zJcd5uXzwwZi5dz7v9CVsJohg1JZOoiujy7f/a2vnHIJc4CFr7Hlo68eR/Ma2Ddmzv0u0LQlQnv84cUIpX8r3xj3628GWjKWAtJ8z9CUlYKJCjImRUEjcAAAB2QZ6jRRUsfwIsecVHWflbwEOK2uJiAz7Xfj5lb4n6/uih1oAG6du8cO5Fe+MKT5hjPQ8MWKAOnrtxzLf6nZuJzt50AJX3IHim8ycCx4Xn8C5GHe9aNhXh/5J6jlmY5uH/Frx4mH/Irqbpj1gtqX+u5CDzCAp3zAAAAEYBnsJ0Rv8CveFV8gxwpAACw1AeNv8TBAABbTNH2nhmqib3zYBW0mrDqfbriD3nbTLDbGJhKXkXmXepHIC6D88wl5FEt5ArAAAATwGexGpG/wK/hDettfSkAALFfzSiYGFjVvr/NWsY9PgAI2ViEVYtNAuf51iOIK5NQvdNfKKLuxyPe4AbZrx7ycqwyeM2eCTSivBcv1JZ15kAAAEDQZrJSahBbJlMCFf//jhAAAADAA0+/9LOPz+adLh9AoAh1+XLfFke/Zjh5NlJc/5i3Gd2P+Hd9Bu0YsYzEhdMEpMMTMam2w0WqtNVG1lh8G90ovlsyrvdmgfCvOjGtZCybfhsKojMfdXX5K/NzerhyjmrQCzwzbFBiEf6cSlZCfh2qf0VeoItxw/Ya8Y3qJSq+E9Ay4VUocIBT3Fqq2pscU25yvydrvsI85YYSi85t96tMv8o9VtXLdjBoRO7hjoQyQ+e7PFqphZ8O+psgHFLpBmAWDlKFs2Rb6DpICnwdgZP5lORifr3xV+PuNRmruw7xsp8DASRW1w6eMXIeqovODWcYQAAAHRBnudFFSx/Aix5xUdZ+VvAQ4ra4mIDPs8PvKb1F7Tdwm9BltGC2lhfvT/aYAbgRSf1VH4BM9+CofLRx6Ln8SCa3i5npQJcYzd8uVyCEsBcKzcjnARsD/R4AgiJ2C4r8PAVJac+UcGaPJv7p/kzA8EeQglPHwAAAF8BnwZ0Rv8CveFV8gxwpAACw1AeNv8QLpGo4fU6uekglhRmOS5FfLAA2FxX5VWrHdU2bhXROT2pYWH8ApdJCN+TLzXYh32wKxg27hxtXaiboK1Ci2H6mcsoZ2M5nZfPgAAAAFQBnwhqRv8Cv4Q3rbX0pAACxX80uGyvP3D58mcBzw45zxVCQ9xLb08AG/MuQZv2eAmzpEt8GJIsELNq7k+qHx9g8A+X/oJspBYlIRdqvooAGlX9KmcAAAEyQZsNSahBbJlMCFf//jhAAAADAAT/k7nXlAFRneDGuLGhsACMbP+M3d9xeQX5x/Z8ZvRk+0fznX32iwRC0tGECW93ilV/lUmV4vP31HIjigvR60GuT4MrFZPNV63+aqKgiu/DPI7gKEi1agGF10keFuuam3NWeeRjG3cPKc+dosTfAGcAel0wufNmwM6nvpwo56WRiTqsIo10ony15+bQvk1LQ7NskCcSzcgfLCazPe3z5IdZU+4VE3XmE6Chs8AI5UnXqS5wFyiPBpxS6w49ohlc4BOUZ7ETYgPbMnbOtF8Fh3oLXBGb7edr9lUzWfQ77Bqe5tuGyiP1q18J/8LbPgK2Ng7KEkb87lmaMQRGARezjcyRt5rJ0swxHwTYIexvhfCJwrI4DkaqjUMAl1ZK5+RBAAAAd0GfK0UVLH8CLHnFR1n5W8BDitriYgM+zw+6D9zn53lZzBKT94TPWgB6YQgBx5LECqMTF0M9RjOyaBTqbXWTrvitNGNr+UoqZtCttfcnDvFeXlDnN1AIIrVPu6Tyd4Z4ciljN7g5jmTlk8VEIgp5iFhyeyGXvdWqAAAASwGfSnRG/wK94VXyDHCkAALDUB4zGUDQ/8RcDDvERUVFMcXikACngAmDgrRdOyg1ETeF2OUj4wktQ31p1c2Gy7ztbvgw8CvHzlrVJgAAAGEBn0xqRv8Cv4Q3rbX0pAACxX80plP19rfa/tslQ1V82FEfFBi2ADir5FV/OcJ+EBrTorc2cJ6WQfuUbVLUfLPzyM6fYpUdymELTii9m9t5Gfc1XHfzH/g76PHNaWpCjgmhAAAA9EGbT0moQWyZTBRML//+jLAAAAMAAUvmVJVLVTzrsbm7GACxT9jkFUbaR3lI4Fd7PrVN9QN5YyK9nYXk+LB4AXuWrUqcuoHGRhEQN2HPdwdr6Fo1fgB+DJpfDF/IvWrPV7IqMJG8+LwbJTrr00gjst9ejKiuM0AYJaNAf1GD15M8B/tqa/L+XQw2GwjAosqSRrDEde1Qp5muDckmhPaopVmk3HzRqOgfF084TY2E6vSpz18eRifEuDkpkZ9jtVwLGUeFNfLZHd8Fm5oomARa84zzD+P/cs4PIZTmdZrIQteyK9AiAEFiV2JKwDfNjuGEX/nB8LEAAABjAZ9uakb/AsBRSeZvcmxmy85tKTJBeqpqMZ7ZYy9Sv53sZRfrXQ7zgAGq1qOgiryGIMQqHMHmbLVH3t7zeCvnum0CdgdA79d7fKpFF+VIHbeze3GuKuxdlHnveIkXtJqLSe8RAAABCkGbc0nhClJlMCFf/jhAAAADAATTyLp2xAJr3VvKIdFDYWaVJI/mj/NBveSRcpBJff7d2JQTLdr4MlRNiVCoKS/oY4btCh19cU5S2X78oRF+qpD8DyG5NpFWhFSMHglVBq58jCw8XqsWaeMpzH/RFRf+lc7Fnj27rQ88LuR31Y8N7W7LAiUzCl9BW9SXLQcJI12pqqkW5387wcwAfCBBF9fdFlZSsAj0y6WiqzdgdXil5EsOm5rgXgJYB69mLEWHna1WFAsQRFtB2RtxWMMpzyyzLH1uwUS1xHmgo658wURHaZd7kSVZAmf5kn6ZpX0HnyUtiW1Ayl7TifdFKFpK0iJvcb3lM3wH9EnQAAAAZEGfkUU0TH8CLDpCfg1nmuAhxGrxMKQ6gYvD3V3G+owr8jX1s9m1Ndr8YA9XenElczyom/84AEY8ZmrrIvM7tpwOrmnz9DBsLryjYt79cA6H6GACYwUJ9ZDipTxulrHpXps0nYAAAAA+AZ+wdEb/Ar3hVfIMcKQAAsNQHjNNSKsCEQOSmOYABX5/hlgiG0waqgSsAkpklzTTnkhvIz1v83CAXd6KXisAAABMAZ+yakb/Ar+EN6219KQAAsV/NKZk5agoQN+DvTPahFxCgAtuua+N+Dy2exRQe2BmOZBZ5+maz+LRBh6sDLHQZ1rnoBvCgCCMU5zggAAAALFBm7dJqEFomUwIV//+OEAAAAMABNPrgJlm7Qxl1JokAUyhKq9bI+bH/437mmd7y32jeZMD6T9C6LT6teYrqXifiRGvpkl4eDMkkUB1ZcwCM4am3OE9BktjSAXg1JODTH974pycZSouMD8TvOrl0S8I8mLELuuOh9j30jYn2RMoCAKrBSnO/+IIf3FsPj67c2kaCDmqg/02TaqjfVlNxgh4TBtzS+SBFmZsniagUHP5aXoAAAA2QZ/VRREsfwIsecVHWflbwEOK2uJiAz7SiNA1ev4A2ADXyYQl5x9APyIi+li1EXCMof8iqqxBAAAAQQGf9HRG/wK94VXyDHCkAALDUB4zqBl6dABVNxqpd2iW7kXyb79mCjd/flVOcWjJElhRT8fHLxWH+4GF0h8w8wegAAAAKwGf9mpG/wK/hDettfSkAALFfzSmj2RKRd+7cR9MAOOmahgxLMJ3VLOkSS8AAACBQZv7SahBbJlMCFf//jhAAAADAATTT7YAaTvzuqdbAs1xNWAVZaQqZ/a4G1rSpK6tJqUNLLhj/sIP1bhnXt1ReUW5SeG6vQcidJuNb0J886bV8FvIKdWezMsrQ9IXx8J+DDyLVxXErjb6RpqHJaNgstWE83vAITCYyFLfgt8XLqkxAAAANEGeGUUVLH8CLHnFR1n5W8BDitriYgM+0ojHw+kd5WtLzvdgBuuhWRXgaZj62xk62uoRJuAAAAAkAZ44dEb/Ar3hVfIMcKQAAsNQHjOm1AQeoIR6T8z2Eecyi3nBAAAANQGeOmpG/wK/hDettfSkAALFfzSmj7DgtOGACMSWydxABzKflwVpgugOfgAnWVnauAJlt3OsAAAAyEGaP0moQWyZTAhX//44QAAAAwAE0sRkR0wzk61uAA3uGvVzHE2i+y6a/Wb12QJWFh8bWvO1607ECIODFLVvWm37X+00ufnQsx0sVxObbGdAe3hFaLiLjvPMggUIKaTSzcKLwI9B3qepLF5MByvSZMxqxHq1/ApZiBshSYegu6ZoEM+ex2uhItPDl97pbciCgA99DlGwXsySpzHGbt1QJqRKdsd3DUx9uR4B6pBbkoSFPEtUVD0A2yZa/6Qb8EWls+IxxSE//filAAAAYEGeXUUVLH8CLHnFR1n5W8BDitriYgM+0oinuWkNOZt51d0rv5SCN5cGZWh3+O0CBTC7MFs1OhPZBYE1t+Ymy7+OgBF65QAWmm3QMHX375/jCBIVj5Xlb8i4p2grw6IugQAAADsBnnx0Rv8CveFV8gxwpAACw1AeM00Qx3oU5Fhp3omAWCbu3gAuF/AY2+221ggRGs9s2AohMkKrJsHiMAAAAEABnn5qRv8Cv4Q3rbX0pAACxX80pmUJuzTvDglAkgZMv/wSdTax57v6FuJXqiADgDlQ4AorFzQ9Uk31fp1qLV7AAAAA90GaY0moQWyZTAhX//44QAAAAwAE0sSTB2qD8lKe6AAJaVkCQE+Hcuph0f5Swx4VGtM0t0NuG2VV9y85HArOD3TR/HCF2Pt5+ts5ycOaFR4Om6Fn3l4Hp+vQ+3uQ1LdfX+KjFcVtRTf7U/3rxKaa47e2HHZ77epaxWHhCxWRAnycBMsJ/7qmLU6LimDER4r2kJ8ckWulPR+7VDAn87KjZHu1uHkbd9JwvCc5g+ufFSrMmV2CeYjcoMSSaFCyDOc5lPA/HWMBqiOXvuipsVHrb09jJjs4pcNcuLt/bttXw7z9SpOtDPS1keiNLefRLYHtcVd52bRv8nkAAAB7QZ6BRRUsfwIsecVHWflbwEOK2uJiAz7Sjn55l+AwXz1bOVgyUPuCrugdc73AAlg3AjtDIRzM0+WXwaa2mEUWB+WdwZ2QP1nMtjmWewgkYB6b94HBCy/MBJN9lRZqJ3ceJYsZl6s75YWvuYTkDVhwWyVCaXi70V7c9RcwAAAATQGeoHRG/wK94VXyDHCkAALDUB4zqDInFrP0QB8+HLpnDbAAcOYOQjncUPMknX0YhDQHOZh6b/hr35gS2LDD/lIbAlhAcmXI3VkpNfspAAAAXwGeompG/wK/hDettfSkAALFfzSmfVMMKJKo9nCJP8cVJUoART+O0eQrCAxtDbd0xaQLeZmg1sPk6Gj8n9Ijf1JqUSi0pARfi5FZoRLH1qTtDZPAcwWIiijrqfPQ15HYAAAA0kGapkmoQWyZTAhX//44QAAAAwAE/JODvzeCuQAb6sU08Z0IPR4I4zEivw8Om08DfLXI37hYuGHY3Zpch7t+uAJoxWORxmDrrCjU8J0xRtjz5/IKQckp08BOHw2z1ZkfLx88x3k92P+VzTDalM380rIBa+SWEnoARVmutn6qpVIvG3SZ34YdcGXij9mEsX9peiyvYvzqV7lhTh56Mk2dB8t7kF5U05eeCzXNJqI3FuI2SutieUl/sfPefU9bEPpTpVxOiXw024vsU0frhphPPJJYwQAAAGJBnsRFFSx/Aix5xUdZ+VvAQ4ra4mIDPtKROdwMdYLMV5Uuv3aYDLlBWFUgAC2Qa8P79H+LIu+K82+N2odAZ5ff5DiUnoNX5e+jJmWZ6tigL0ZUrsYlrU3/QUcqV/WIYF9IVQAAAFUBnuVqRv8Cv4Q3rbX0pAACxX80plQNFogs9XL8rmjIQhFX0AK7rHUuScUAtnLLocSKucjSihc6L9pPNepzMAottLC0/015D9wjOw/So0xlwM6jd82BAAAA2EGa6EmoQWyZTBRMK//+OEAAAAMADSzzXbUBsAwZuD6CGO9/GIq0/vI1guJac1dmb67vLeW8HDudmO0Xwhgr7YdqXY694x/eBoWss3zcDRf+iq+FWoiqKJrjHGKq/SXK3HhSugt5miVINbTpk62aawl8CDsZsgi8QmtBLtvb3dJdf1wxO3MHPpLrZfVmkciJT+WU6bcbSbK5GkTpFC6theVRA+xkjJF6g32PRZpR3nqF1IcGDUoPOxukZVak6OYD2whIM9B082vA9wvXpYxS/4On9POfPmV/SQAAAD8BnwdqRv8CwFFJ5m9ybGbLzm0pMkF6ospGAnFp05nHifn2dY0cmHvmQAWjDGTJM2Ce5ZBgFnetkG3RImCIW80AAADqQZsKSeEKUmUwUsL//oywAAADAANmJumScgBBbtqFg5scBzmdbhWH5WBy1qHymdH3vq/daQkwJ/lvUxt+9NONWc9peGgmBPOgZkTVFJ0jy/vdOG3IHaAn/fK2vAVlbami9DQotHNfBTUnUFK3FeapVGj2iBtDZHrczfRHKdZeqrTF77PbwMXXqQqCo8xK6FSrPVZ/v1gPvHKFx+mXKkZ7+ohlRxqzsS8TnQA3ApjAQGr23eDsAXzz3hnV+KTtUvEFkCg8lAkHTiQp/rVi/ezFLG+QHSuWeFX4I1NvlZGNyjdGA3K+j3oD/oBsAAAAUAGfKWpG/wBr20ygAJqaSWul6bLqXo0qjU20t0uQ7zQJ+9uABsLivyqp2d12bxQFODwUI/fiqJBOlnX0HeyT8SFWST5ovwwhkUuqq/VMccr5AAABHkGbLknhDomUwIX//oywAAADAANkDOuXHVKi4vEAEqDHKj8InQqJobIBUpxseYkSOo5ftRhPja51Po/HfmLSXP920VivBRjoCLpuGX+OpjlefvOUHjrbKdOMANRgmxqeeUjfI26B4xIX0d3oz9nyZC+MEm+g/aT2GaGrhztlnZFaQ6xc0Ig1qi7AmOjR4Lp+Z2GPZl1mCbOg7+S4fjPO8FwreFvTx5k+CxabR0a9Qek1I4TRkjFLZ9xWLWY/iyhKDDkAU6bz945WddjJj0eWuR2xZOpsb7ECfKFaaK1Z9Hlx7ugOHdT1MyR1Ypv7AqAJMGmpCi6Oc28APik4yS/DNZrktEJtNkGNvegk+BMGn9kW6L5o86tLFjkbuu/7WvgAAABmQZ9MRRU8fwBWPdpgAHGGd5YP7glrZ9TisABF9rFDncqdVdf2xnDIxe7JFUXp1B9Mg7i59Sd2VoWw2IJ5OG2u8eZePExHDzluqFuWpoWpBlt6N+rIyBH6Ct/UOH7GpiekKOi6TYW6AAAAWQGfa3RG/wAAAwACKlo5+L8am8YAEe4oCGMRMA7v8SzIhVQDF5VkxmX/Ash2SEi/Xkv92WfXr2xK72+GBdBInNxB7ZorUUjZj6kDve1v6JfK20+Gq9pCvM/BAAAAPgGfbWpG/wBr20ygAJpTfTKQkFaITbdR3Xt0fotpLEHRE6oavbEoQAMqkpfdVv3v47CjDQGIuNTb+SF9wzWBAAABIkGbckmoQWiZTAhX//44QAAAAwANh9fsjImdF8gAsPOAVs+IyxuuGPRh8DfE3euucMYlQzSj6beUuLqmTGB0utko7H7vE+TZ8zmQTeX1FKoEq5wpbvlaSgnw6vBQu2xTnCEaN/vy/ncoVIz5c7WQI5OIrcaJPCg18Bd/T4qX/jomRWE00SUZO3uFF+Q43UY+uUaqEpLvO/ruRmySLLMFFiwHPrTIxGgGWLAatQDNaIWKcGmoWQvP04P69HA6F/ocrsy6FMF+ddqJ35LDGWS/FoqUQlq7fEhHqZO6w7rIeL+IsW+lS6cnFoVnfq05//TfLqT1B+z1t/8zdCt0RUwJaaKKL2IcgjeuJRp/g+fPlHobyAomZgDKMHdyHv59aFLmJZGXAAAAakGfkEURLH8AVj3aYAB01zBM0LRRqKfDCC3h2CCC9BiFcAAFzPNiXfquX7scgoWeBs4ya/DD9MLiho5r0ZYL3MU3WLM2RRjCpbC2hhJ71k/tvn8+aD9VEWZOCfWXHqSeqikLfVEXNTdEtUAAAABQAZ+vdEb/AAADAAI7+b3SNKrYxrczTEAGoYjZqvx+OcKzNzRREw4dmYa4fKMWDG5XlY6QzFVEL+6vU5z9F6uAkj0H6EXuhgQXmv6jRvPiLRAAAABaAZ+xakb/AGvbTKAAniPYLh5Sh/qZ9tsHYZu5a3RA/EsAG4TMB0u8ZuP+VLHy+aR61dmG3VQ2WEFnhYoOn/1vWN2fs2cKWg3JUAcC7NLwYZllledwWd2AgIzpAAABBkGbtkmoQWyZTAhX//44QAAAAwANdyR9kvzT4tVdQACMSDJqKSvlePUiGYRBNIXzo8wiKwQMufIVrA8Hw9iwV2J9SmZVDXHrQm6ldY6lZsFzCzMB8s3+wBIWklPmL5/V/SINorRtn9wZSFUgg86gMM/CKZykYxCq9AXpTRE86btaq4JMkgujk15zObBJqpM3QRZ3UexIyQc5UkueltKNlD+BbxOpJPjLq0Sb+pVKbSLPeR9ZVNBreZn2GDO9HPDE7KG4XHSa8oWT+gKFm8iPZHgTtPyjZ1lI9mMQJv3HpFKDon/X8vJbyz23dw/4k3WLwmll/H+mzXAuzim5i+rAVN/fq/CODeAAAABbQZ/URRUsfwBWPdpgAHTXMNhSIZcOfK3qWyuAAVeei/h/eBFy6ZP63ZnsQsMfGoiCje6XI8kx/Fihci5A4lMhFZ15bWUaeLdxU7mKz6+CdQQzcKsZhE/hgi44TAAAAFoBn/N0Rv8AAAMAAjwGvxN79S7mbWiRE+uI6ugBriysITr2WwdsddB04GWf3SBOdZ1rCk3GWtGyiPaeGq1vt6CTADzR3/u0Kl9hRHSJYGnTgksXHSXb3dph+QkAAABLAZ/1akb/AGvbTKAAn1sx98bmI9B+xK/YADipictMQd7HL/zdrcAMgYLvj08UR9/1Zs4kR8d5cZkR/lLEKyZ7hT0fnnAnGsLMPiW5AAAA20Gb+kmoQWyZTAhX//44QAAAAwAjnjQV9RAJnB0+w3vNam+qtxKgAMea+ojiIOpTr3NtCFyHMvJn4PbhMkwWuujFpkgxqn4D5S+bS88fSj0Jh9KdN4+L/zE/aBt1nROxlmPSZC6bx7yjIB/41Xy9dcShcQvJL17KyHS9W80gH9zrpZ3PHLKM+xSvbXbBB2lIh2da1CYP+9gB71IgHjdkS0YQLyUG+jMrLhKtcbnqtmYPzuegxBWzFsjCP4sZyhF+UXiqgoN6X3NOOZI8c/jwzbGjhfeeQjM/HDFPUwAAAGFBnhhFFSx/AFY92mABOFkTyTL0nyx3F9MAC0pobsR7Bft7cO0oWlPtH9wSVgv//Y1CWaodoiSXWoeB9MY/DYM1Lk9wVDPYVZscRZ401DfD55X/04LEK0OKX+mIGLc8VjXBAAAAMwGeN3RG/wAAAwAF9h8KOdmhbueOAyi2NSSAhTj1DEKxXLN4GoAad5UfphyABUTU5/yKEAAAAD8BnjlqRv8Aa9tMoAGmLsOvmx4cACKjmAHslsUrsF8T5A+Sr5IDUTFpooIA91fhyIi6v4fIFlH4vU4N9qE3WYEAAAB1QZo+SahBbJlMCFf//jhAAAADACOfV5EcxBtpSneWUyD7iNRUEB/9dHFqk10AWI9QPP+oFHVrEL6HA4Ddb3cYB4/J106w7c/PqVg2hwbiNaPhlkoYaEOeWTF9ppvWwTikZQ7XtQ8tKZn9an2m8OdThLu7Djd4AAAAJUGeXEUVLH8AVj3aYAE4xcS9bSTrDN7YUP06LfJD/3UdTcG4RYEAAAA0AZ57dEb/AAADAAYehyF0TXDzlx0TbVAAEoTB4snn1Moj8wVnoBu8RXNUpdXy4Y1BVD0g4QAAABUBnn1qRv8Aa9tMoAGwjOIDDeXvVoQAAAB4QZpiSahBbJlMCE///fEAAAMAAFh/YkJBzdwQ1QAI7NJG35n0DTl47qbrDCquNeWX6E/OPOYSzien21iX80AUWXjWYzwqhHUIfHYObP6lwcD5MBVBiey2r1M39Fajoc5Ezwe6kaqjXsA0CHmxlAQAvVxC/NMcSrPgAAAAPkGegEUVLH8AVj3aYAFCKRUETmyrqu+dkk6OEPVJC/ZHL6wcvheg8ACbwKwFUKHg6DVUGi169yqXlpOs7+fxAAAAHgGev3RG/wAAAwAGHoiTG4DXwGvzOhtyMPJlD/SIQAAAAGUBnqFqRv8Aa9tMoAGwjKaupTGOMgAOOpIdw5QPJm7wMFmxWMIR5vlKGGQJe+FJzOoPlJrfY2znz9gFdTL5MVtFC+TzG8wvjaAYKYqg9ZrwKzil9/E/LIA/+YFn5p0fyIAMJ96egQAAAKhBmqZJqEFsmUwIR//94QAAAwAAi3RNsJrKw4/gQYoYKg0AJaja9rzr6G5w1wL8a3Mplsq92nFRkaGzuNYwlFci9GcEA6jbIudWUxOuAqwaM6QzxewBprk3ASuOK63yXXV4+NXLdIJZlEqisLnclKV8iUR2EGD/QVyVzc/y26Hzr6fWtwv3PN5EY5mrHoGrMdMiAoa35pRkQZQ8ywLhintv50L7RhECHSAAAABnQZ7ERRUsfwBWPdpgAUIqgvq7j2W3rnJh43OBoW3wSABqed8PhvqR8ELDFo1T4qco+zVsydH1/WYgI6qm0j31tIczdTfXk2rdJge4L7irxFfWZ6XSUL/Fjr9QV1cl/9Z4GKkL0HSUwQAAAFsBnuN0Rv8AAAMABh6GPYvxH8FKgARLm1UFkpVGzUWdsWosNKimK5+p3JJPYneMfdvsh5eJLjIxejqwa3d5BfPsMZMjlFBh3f+onLnPybbtUboRKnSDvxLkefOhAAAATQGe5WpG/wBr20ygAbCGSeqesrmQ2z5taAiEP05i7tQBUJpPOi/r/k2TsdMlupBlVLc+zNunmO83Wm1SLzh6T84aBAOykB9VVl2fibuhAAAAQ0Ga6EmoQWyZTBRMb/pYAAADAAGT2IYLc7ixyuXGPdtI5TDkEsJGzKdTBmXmex7/+5QBDDxYTXn6pk8HHq8wBaoQ1XcAAAA6AZ8Hakb/AGvbTKABogtuDTox/R/fmDAvpjokdAGNfiVAgC3ac8Vi8tthaUGNdcOI4Gln5pQMUpU7wAAADG5tb292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAAAaLAABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAALmXRyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAAaLAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAACWAAAAZAAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAAAQAAGiwAAAQAAAEAAAAACxFtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAADwAAAGSAFXEAAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAAq8bWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAAKfHN0YmwAAACwc3RzZAAAAAAAAAABAAAAoGF2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAACWAGQAEgAAABIAAAAAAAAAAEUTGF2YzYxLjMuMTAwIGxpYngyNjQAAAAAAAAAAAAAAAAY//8AAAA2YXZjQwFkAB7/4QAZZ2QAHqzZQJgz5eEAAAMAAQAAAwA8DxYtlgEABmjr48siwP34+AAAAAAUYnRydAAAAAAAAImUAACJlAAAABhzdHRzAAAAAAAAAAEAAADJAAACAAAAABRzdHNzAAAAAAAAAAEAAAABAAAGMGN0dHMAAAAAAAAAxAAAAAEAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAIAAAAAAIAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAIAAAAAAIAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAHHN0c2MAAAAAAAAAAQAAAAEAAADJAAAAAQAAAzhzdHN6AAAAAAAAAAAAAADJAAAQrgAAAR4AAABGAAAALQAAAEAAAADoAAAAWgAAAEoAAABBAAABdAAAAHYAAABnAAAAbgAAAZUAAABtAAABPQAAAIsAAABBAAAAWQAAATMAAAByAAAATQAAADsAAAE5AAAAWgAAAGwAAADfAAAAbwAAAEgAAAA5AAAA+QAAAHsAAABBAAAATwAAAOYAAACfAAAALAAAAD0AAAAcAAAAjgAAAEIAAAAlAAAARgAAAOYAAABtAAAASQAAAGgAAADbAAAAYgAAADQAAABEAAABGAAAAGgAAABVAAABBgAAAG0AAABBAAAASAAAATIAAAB4AAAAYwAAAFsAAADqAAAARQAAAR4AAAB8AAAAXgAAAGQAAAEGAAAAWgAAAEAAAABPAAAAxAAAAD0AAABGAAAAKwAAAJsAAAA6AAAAKwAAAE4AAADbAAAAZAAAAFsAAAA/AAAA/QAAAHgAAABOAAAAbQAAAR4AAABzAAAAUgAAAF8AAAEBAAAAjwAAAEwAAABfAAABOQAAAHwAAABMAAABGQAAAG4AAABTAAAAcgAAARAAAABUAAAAXQAAADgAAAD0AAAAcAAAAEAAAABYAAAAfQAAADEAAAA9AAAAIgAAAKMAAABIAAAALAAAAGIAAAD0AAAAcQAAAGwAAABVAAAAzAAAAFIAAAAzAAAAXgAAARUAAABqAAAAWAAAAR4AAAB6AAAASgAAAFMAAAEHAAAAeAAAAGMAAABYAAABNgAAAHsAAABPAAAAZQAAAPgAAABnAAABDgAAAGgAAABCAAAAUAAAALUAAAA6AAAARQAAAC8AAACFAAAAOAAAACgAAAA5AAAAzAAAAGQAAAA/AAAARAAAAPsAAAB/AAAAUQAAAGMAAADWAAAAZgAAAFkAAADcAAAAQwAAAO4AAABUAAABIgAAAGoAAABdAAAAQgAAASYAAABuAAAAVAAAAF4AAAEKAAAAXwAAAF4AAABPAAAA3wAAAGUAAAA3AAAAQwAAAHkAAAApAAAAOAAAABkAAAB8AAAAQgAAACIAAABpAAAArAAAAGsAAABfAAAAUQAAAEcAAAA+AAAAFHN0Y28AAAAAAAAAAQAAADAAAABhdWR0YQAAAFltZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAACxpbHN0AAAAJKl0b28AAAAcZGF0YQAAAAEAAAAATGF2ZjYxLjEuMTAw\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "slZCdTaNlaWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de construcción de una red neuronal y entrenamiento con SARSA\n",
        "\n",
        "########################## Creación del Ambiente ##########################\n",
        "env = gym.make('MountainCar-v0') #crea el ambiente utilizando gym.\n",
        "np.random.seed(613) #semilla aleatoria\n",
        "env.seed(613)\n",
        "nb_actions = env.action_space.n #obtiene la cantidad de acciones (3)\n",
        "\n",
        "###################### Creación de la Red Neuronal ########################\n",
        "# Modelo secuencial, una capa después de otra\n",
        "model = Sequential()\n",
        "\n",
        "#Capa de Entrada\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "#Capas \"Escondidas\". Aquí se pueden añadir varias capas con distinta cantidad de neuronas\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(16,activation='relu'))\n",
        "\n",
        "#Capa de Salida\n",
        "model.add(Dense(nb_actions,activation='relu'))\n",
        "\n",
        "print(model.summary()) #muestra las neuronas y parámetros de la red\n",
        "\n",
        "###################### Configuración del Modelo ##########################\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1) #límite de la memoria\n",
        "policy = BoltzmannQPolicy() #política inicial\n",
        "\n",
        "#se crea el agente de SARSA\n",
        "sarsa_solucion_1 = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=100, policy=policy)\n",
        "sarsa_solucion_1.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "\n",
        "model.save_weights('model_sarsa_solucion_1_valoresiniciales.h5') #guarda los valores iniciales\n",
        "\n",
        "############################ Entrenamiento ###############################\n",
        "#Se entrena el agente por 10,000 pasos de tiempo y se guardan los resultados\n",
        "\n",
        "#carga los pesos iniciales para entrenar desde 0 (buena practica)\n",
        "sarsa_solucion_1.load_weights('model_sarsa_solucion_1_valoresiniciales.h5')\n",
        "\n",
        "#Comienza a entrenar\n",
        "#Guarda el historial y muestra progreso en pantalla\n",
        "historial_solucion_sarsa_ejemplo_1 = sarsa_solucion_1.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
        "\n",
        "#guarda los pesos finales (buena practica)\n",
        "sarsa_solucion_1.save_weights('model_sarsa_solucion_1_valoresfinales.h5f', overwrite=True)"
      ],
      "metadata": {
        "id": "6aYCb-uylajm",
        "outputId": "1a2e0979-1c7b-44ef-a8ec-3c62503ce3d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_8 (Flatten)         (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 32)                96        \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 675 (2.64 KB)\n",
            "Trainable params: 675 (2.64 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   200/100000: episode: 1, duration: 2.790s, episode steps: 200, steps per second:  72, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500272, mae: 0.336641, mean_q: 0.013720\n",
            "   400/100000: episode: 2, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   600/100000: episode: 3, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   800/100000: episode: 4, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1000/100000: episode: 5, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1200/100000: episode: 6, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1400/100000: episode: 7, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1600/100000: episode: 8, duration: 1.567s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1800/100000: episode: 9, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2000/100000: episode: 10, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2200/100000: episode: 11, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2400/100000: episode: 12, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2600/100000: episode: 13, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2800/100000: episode: 14, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3000/100000: episode: 15, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3200/100000: episode: 16, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3400/100000: episode: 17, duration: 1.750s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3600/100000: episode: 18, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3800/100000: episode: 19, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4000/100000: episode: 20, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4200/100000: episode: 21, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4400/100000: episode: 22, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4600/100000: episode: 23, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4800/100000: episode: 24, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5000/100000: episode: 25, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5200/100000: episode: 26, duration: 1.804s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5400/100000: episode: 27, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5600/100000: episode: 28, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5800/100000: episode: 29, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6000/100000: episode: 30, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6200/100000: episode: 31, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6400/100000: episode: 32, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6600/100000: episode: 33, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6800/100000: episode: 34, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7000/100000: episode: 35, duration: 1.808s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7200/100000: episode: 36, duration: 1.624s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7400/100000: episode: 37, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7600/100000: episode: 38, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7800/100000: episode: 39, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8000/100000: episode: 40, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8200/100000: episode: 41, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8400/100000: episode: 42, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8600/100000: episode: 43, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8800/100000: episode: 44, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9000/100000: episode: 45, duration: 1.529s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9200/100000: episode: 46, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9400/100000: episode: 47, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9600/100000: episode: 48, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9800/100000: episode: 49, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10000/100000: episode: 50, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10200/100000: episode: 51, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10400/100000: episode: 52, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10600/100000: episode: 53, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10800/100000: episode: 54, duration: 1.542s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11000/100000: episode: 55, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11200/100000: episode: 56, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11400/100000: episode: 57, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11600/100000: episode: 58, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11800/100000: episode: 59, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12000/100000: episode: 60, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12200/100000: episode: 61, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12400/100000: episode: 62, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12600/100000: episode: 63, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12800/100000: episode: 64, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13000/100000: episode: 65, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13200/100000: episode: 66, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13400/100000: episode: 67, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13600/100000: episode: 68, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13800/100000: episode: 69, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14000/100000: episode: 70, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14200/100000: episode: 71, duration: 1.759s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14400/100000: episode: 72, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14600/100000: episode: 73, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14800/100000: episode: 74, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15000/100000: episode: 75, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15200/100000: episode: 76, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15400/100000: episode: 77, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15600/100000: episode: 78, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15800/100000: episode: 79, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16000/100000: episode: 80, duration: 1.714s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16200/100000: episode: 81, duration: 1.635s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16400/100000: episode: 82, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16600/100000: episode: 83, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16800/100000: episode: 84, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17000/100000: episode: 85, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17200/100000: episode: 86, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17400/100000: episode: 87, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17600/100000: episode: 88, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17800/100000: episode: 89, duration: 1.626s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18000/100000: episode: 90, duration: 1.669s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18200/100000: episode: 91, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18400/100000: episode: 92, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18600/100000: episode: 93, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18800/100000: episode: 94, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19000/100000: episode: 95, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19200/100000: episode: 96, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19400/100000: episode: 97, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19600/100000: episode: 98, duration: 1.498s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19800/100000: episode: 99, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20000/100000: episode: 100, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20200/100000: episode: 101, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20400/100000: episode: 102, duration: 1.423s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20600/100000: episode: 103, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20800/100000: episode: 104, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21000/100000: episode: 105, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21200/100000: episode: 106, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21400/100000: episode: 107, duration: 1.662s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21600/100000: episode: 108, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21800/100000: episode: 109, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22000/100000: episode: 110, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22200/100000: episode: 111, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22400/100000: episode: 112, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22600/100000: episode: 113, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22800/100000: episode: 114, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23000/100000: episode: 115, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23200/100000: episode: 116, duration: 1.774s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23400/100000: episode: 117, duration: 1.604s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23600/100000: episode: 118, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23800/100000: episode: 119, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24000/100000: episode: 120, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24200/100000: episode: 121, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24400/100000: episode: 122, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24600/100000: episode: 123, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24800/100000: episode: 124, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25000/100000: episode: 125, duration: 1.763s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25200/100000: episode: 126, duration: 1.650s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25400/100000: episode: 127, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25600/100000: episode: 128, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25800/100000: episode: 129, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26000/100000: episode: 130, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26200/100000: episode: 131, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26400/100000: episode: 132, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26600/100000: episode: 133, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26800/100000: episode: 134, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27000/100000: episode: 135, duration: 1.517s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27200/100000: episode: 136, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27400/100000: episode: 137, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27600/100000: episode: 138, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27800/100000: episode: 139, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28000/100000: episode: 140, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28200/100000: episode: 141, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28400/100000: episode: 142, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28600/100000: episode: 143, duration: 1.964s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28800/100000: episode: 144, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29000/100000: episode: 145, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29200/100000: episode: 146, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29400/100000: episode: 147, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29600/100000: episode: 148, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29800/100000: episode: 149, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30000/100000: episode: 150, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30200/100000: episode: 151, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30400/100000: episode: 152, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30600/100000: episode: 153, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30800/100000: episode: 154, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31000/100000: episode: 155, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31200/100000: episode: 156, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31400/100000: episode: 157, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31600/100000: episode: 158, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31800/100000: episode: 159, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32000/100000: episode: 160, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32200/100000: episode: 161, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32400/100000: episode: 162, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32600/100000: episode: 163, duration: 1.286s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32800/100000: episode: 164, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33000/100000: episode: 165, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33200/100000: episode: 166, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33400/100000: episode: 167, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33600/100000: episode: 168, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33800/100000: episode: 169, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34000/100000: episode: 170, duration: 1.942s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34200/100000: episode: 171, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34400/100000: episode: 172, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34600/100000: episode: 173, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34800/100000: episode: 174, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35000/100000: episode: 175, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35200/100000: episode: 176, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35400/100000: episode: 177, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35600/100000: episode: 178, duration: 1.493s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35800/100000: episode: 179, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36000/100000: episode: 180, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36200/100000: episode: 181, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36400/100000: episode: 182, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36600/100000: episode: 183, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36800/100000: episode: 184, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37000/100000: episode: 185, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37200/100000: episode: 186, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37400/100000: episode: 187, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37600/100000: episode: 188, duration: 1.946s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37800/100000: episode: 189, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38000/100000: episode: 190, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38200/100000: episode: 191, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38400/100000: episode: 192, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38600/100000: episode: 193, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38800/100000: episode: 194, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39000/100000: episode: 195, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39200/100000: episode: 196, duration: 1.481s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39400/100000: episode: 197, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39600/100000: episode: 198, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39800/100000: episode: 199, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40000/100000: episode: 200, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40200/100000: episode: 201, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40400/100000: episode: 202, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40600/100000: episode: 203, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40800/100000: episode: 204, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41000/100000: episode: 205, duration: 1.556s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41200/100000: episode: 206, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41400/100000: episode: 207, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41600/100000: episode: 208, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41800/100000: episode: 209, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42000/100000: episode: 210, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42200/100000: episode: 211, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42400/100000: episode: 212, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42600/100000: episode: 213, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42800/100000: episode: 214, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43000/100000: episode: 215, duration: 1.756s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43200/100000: episode: 216, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43400/100000: episode: 217, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43600/100000: episode: 218, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43800/100000: episode: 219, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44000/100000: episode: 220, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44200/100000: episode: 221, duration: 1.365s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44400/100000: episode: 222, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44600/100000: episode: 223, duration: 1.680s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44800/100000: episode: 224, duration: 1.732s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45000/100000: episode: 225, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45200/100000: episode: 226, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45400/100000: episode: 227, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45600/100000: episode: 228, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45800/100000: episode: 229, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46000/100000: episode: 230, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46200/100000: episode: 231, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46400/100000: episode: 232, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46600/100000: episode: 233, duration: 1.614s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46800/100000: episode: 234, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47000/100000: episode: 235, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47200/100000: episode: 236, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47400/100000: episode: 237, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47600/100000: episode: 238, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47800/100000: episode: 239, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48000/100000: episode: 240, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48200/100000: episode: 241, duration: 2.018s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48400/100000: episode: 242, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48600/100000: episode: 243, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48800/100000: episode: 244, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49000/100000: episode: 245, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49200/100000: episode: 246, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49400/100000: episode: 247, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49600/100000: episode: 248, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49800/100000: episode: 249, duration: 1.636s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50000/100000: episode: 250, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50200/100000: episode: 251, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50400/100000: episode: 252, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50600/100000: episode: 253, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50800/100000: episode: 254, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51000/100000: episode: 255, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51200/100000: episode: 256, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51400/100000: episode: 257, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51600/100000: episode: 258, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51800/100000: episode: 259, duration: 1.610s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52000/100000: episode: 260, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52200/100000: episode: 261, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52400/100000: episode: 262, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52600/100000: episode: 263, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52800/100000: episode: 264, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53000/100000: episode: 265, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53200/100000: episode: 266, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53400/100000: episode: 267, duration: 1.896s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53600/100000: episode: 268, duration: 1.508s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53800/100000: episode: 269, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54000/100000: episode: 270, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54200/100000: episode: 271, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54400/100000: episode: 272, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54600/100000: episode: 273, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54800/100000: episode: 274, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55000/100000: episode: 275, duration: 1.465s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55200/100000: episode: 276, duration: 1.975s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55400/100000: episode: 277, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55600/100000: episode: 278, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55800/100000: episode: 279, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56000/100000: episode: 280, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56200/100000: episode: 281, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56400/100000: episode: 282, duration: 1.365s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56600/100000: episode: 283, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56800/100000: episode: 284, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57000/100000: episode: 285, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57200/100000: episode: 286, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57400/100000: episode: 287, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57600/100000: episode: 288, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57800/100000: episode: 289, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58000/100000: episode: 290, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58200/100000: episode: 291, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58400/100000: episode: 292, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58600/100000: episode: 293, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58800/100000: episode: 294, duration: 1.622s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59000/100000: episode: 295, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59200/100000: episode: 296, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59400/100000: episode: 297, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59600/100000: episode: 298, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59800/100000: episode: 299, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60000/100000: episode: 300, duration: 1.347s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60200/100000: episode: 301, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60400/100000: episode: 302, duration: 1.926s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60600/100000: episode: 303, duration: 1.590s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60800/100000: episode: 304, duration: 1.441s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61000/100000: episode: 305, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61200/100000: episode: 306, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61400/100000: episode: 307, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61600/100000: episode: 308, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61800/100000: episode: 309, duration: 1.388s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62000/100000: episode: 310, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62200/100000: episode: 311, duration: 1.562s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62400/100000: episode: 312, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62600/100000: episode: 313, duration: 1.510s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62800/100000: episode: 314, duration: 1.448s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63000/100000: episode: 315, duration: 1.807s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63200/100000: episode: 316, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63400/100000: episode: 317, duration: 1.545s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63600/100000: episode: 318, duration: 2.055s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63800/100000: episode: 319, duration: 1.365s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64000/100000: episode: 320, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64200/100000: episode: 321, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64400/100000: episode: 322, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64600/100000: episode: 323, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64800/100000: episode: 324, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65000/100000: episode: 325, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65200/100000: episode: 326, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65400/100000: episode: 327, duration: 1.729s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65600/100000: episode: 328, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65800/100000: episode: 329, duration: 1.494s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66000/100000: episode: 330, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66200/100000: episode: 331, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66400/100000: episode: 332, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66600/100000: episode: 333, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66800/100000: episode: 334, duration: 1.519s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67000/100000: episode: 335, duration: 1.999s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67200/100000: episode: 336, duration: 1.523s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67400/100000: episode: 337, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67600/100000: episode: 338, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67800/100000: episode: 339, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68000/100000: episode: 340, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68200/100000: episode: 341, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68400/100000: episode: 342, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68600/100000: episode: 343, duration: 1.996s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68800/100000: episode: 344, duration: 1.580s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69000/100000: episode: 345, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69200/100000: episode: 346, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69400/100000: episode: 347, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69600/100000: episode: 348, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69800/100000: episode: 349, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70000/100000: episode: 350, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70200/100000: episode: 351, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70400/100000: episode: 352, duration: 2.020s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70600/100000: episode: 353, duration: 1.398s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70800/100000: episode: 354, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71000/100000: episode: 355, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71200/100000: episode: 356, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71400/100000: episode: 357, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71600/100000: episode: 358, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71800/100000: episode: 359, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72000/100000: episode: 360, duration: 1.775s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72200/100000: episode: 361, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72400/100000: episode: 362, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72600/100000: episode: 363, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72800/100000: episode: 364, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73000/100000: episode: 365, duration: 1.477s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73200/100000: episode: 366, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73400/100000: episode: 367, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73600/100000: episode: 368, duration: 1.507s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73800/100000: episode: 369, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74000/100000: episode: 370, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74200/100000: episode: 371, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74400/100000: episode: 372, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74600/100000: episode: 373, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74800/100000: episode: 374, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75000/100000: episode: 375, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75200/100000: episode: 376, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75400/100000: episode: 377, duration: 1.470s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75600/100000: episode: 378, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75800/100000: episode: 379, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76000/100000: episode: 380, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76200/100000: episode: 381, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76400/100000: episode: 382, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76600/100000: episode: 383, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76800/100000: episode: 384, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77000/100000: episode: 385, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77200/100000: episode: 386, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77400/100000: episode: 387, duration: 1.952s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77600/100000: episode: 388, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77800/100000: episode: 389, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78000/100000: episode: 390, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78200/100000: episode: 391, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78400/100000: episode: 392, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78600/100000: episode: 393, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78800/100000: episode: 394, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79000/100000: episode: 395, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79200/100000: episode: 396, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79400/100000: episode: 397, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79600/100000: episode: 398, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79800/100000: episode: 399, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80000/100000: episode: 400, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80200/100000: episode: 401, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80400/100000: episode: 402, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80600/100000: episode: 403, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80800/100000: episode: 404, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81000/100000: episode: 405, duration: 1.715s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81200/100000: episode: 406, duration: 1.538s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81400/100000: episode: 407, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81600/100000: episode: 408, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81800/100000: episode: 409, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82000/100000: episode: 410, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82200/100000: episode: 411, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82400/100000: episode: 412, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82600/100000: episode: 413, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82800/100000: episode: 414, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83000/100000: episode: 415, duration: 1.692s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83200/100000: episode: 416, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83400/100000: episode: 417, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83600/100000: episode: 418, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83800/100000: episode: 419, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84000/100000: episode: 420, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84200/100000: episode: 421, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84400/100000: episode: 422, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84600/100000: episode: 423, duration: 1.574s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84800/100000: episode: 424, duration: 1.790s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85000/100000: episode: 425, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85200/100000: episode: 426, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85400/100000: episode: 427, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85600/100000: episode: 428, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85800/100000: episode: 429, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86000/100000: episode: 430, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86200/100000: episode: 431, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86400/100000: episode: 432, duration: 1.465s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86600/100000: episode: 433, duration: 1.921s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86800/100000: episode: 434, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87000/100000: episode: 435, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87200/100000: episode: 436, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87400/100000: episode: 437, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87600/100000: episode: 438, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87800/100000: episode: 439, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88000/100000: episode: 440, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88200/100000: episode: 441, duration: 1.481s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88400/100000: episode: 442, duration: 1.921s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88600/100000: episode: 443, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88800/100000: episode: 444, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89000/100000: episode: 445, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89200/100000: episode: 446, duration: 1.394s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89400/100000: episode: 447, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89600/100000: episode: 448, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89800/100000: episode: 449, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90000/100000: episode: 450, duration: 1.539s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90200/100000: episode: 451, duration: 1.785s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90400/100000: episode: 452, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90600/100000: episode: 453, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90800/100000: episode: 454, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91000/100000: episode: 455, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91200/100000: episode: 456, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91400/100000: episode: 457, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91600/100000: episode: 458, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91800/100000: episode: 459, duration: 1.493s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92000/100000: episode: 460, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92200/100000: episode: 461, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92400/100000: episode: 462, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92600/100000: episode: 463, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92800/100000: episode: 464, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93000/100000: episode: 465, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93200/100000: episode: 466, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93400/100000: episode: 467, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93600/100000: episode: 468, duration: 1.702s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93800/100000: episode: 469, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94000/100000: episode: 470, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94200/100000: episode: 471, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94400/100000: episode: 472, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94600/100000: episode: 473, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94800/100000: episode: 474, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95000/100000: episode: 475, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95200/100000: episode: 476, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95400/100000: episode: 477, duration: 1.781s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95600/100000: episode: 478, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95800/100000: episode: 479, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96000/100000: episode: 480, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96200/100000: episode: 481, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96400/100000: episode: 482, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96600/100000: episode: 483, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96800/100000: episode: 484, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97000/100000: episode: 485, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97200/100000: episode: 486, duration: 2.167s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97400/100000: episode: 487, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97600/100000: episode: 488, duration: 1.510s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97800/100000: episode: 489, duration: 1.509s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98000/100000: episode: 490, duration: 1.459s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98200/100000: episode: 491, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98400/100000: episode: 492, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98600/100000: episode: 493, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98800/100000: episode: 494, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99000/100000: episode: 495, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99200/100000: episode: 496, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99400/100000: episode: 497, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99600/100000: episode: 498, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99800/100000: episode: 499, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100000/100000: episode: 500, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "done, took 703.994 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de construcción de una red neuronal y entrenamiento con SARSA\n",
        "\n",
        "########################## Creación del Ambiente ##########################\n",
        "env = gym.make('MountainCar-v0') #crea el ambiente utilizando gym.\n",
        "np.random.seed(613) #semilla aleatoria\n",
        "env.seed(613)\n",
        "nb_actions = env.action_space.n #obtiene la cantidad de acciones (3)\n",
        "\n",
        "###################### Creación de la Red Neuronal ########################\n",
        "# Modelo secuencial, una capa después de otra\n",
        "model = Sequential()\n",
        "\n",
        "#Capa de Entrada\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "#Capas \"Escondidas\". Aquí se pueden añadir varias capas con distinta cantidad de neuronas\n",
        "model.add(Dense(16,activation='relu'))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "\n",
        "#Capa de Salida\n",
        "model.add(Dense(nb_actions,activation='relu'))\n",
        "\n",
        "print(model.summary()) #muestra las neuronas y parámetros de la red\n",
        "\n",
        "###################### Configuración del Modelo ##########################\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1) #límite de la memoria\n",
        "policy = BoltzmannQPolicy() #política inicial\n",
        "\n",
        "#se crea el agente de SARSA\n",
        "sarsa_solucion_2 = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=100, policy=policy)\n",
        "sarsa_solucion_2.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "\n",
        "model.save_weights('model_sarsa_solucion_2_valoresiniciales.h5') #guarda los valores iniciales\n",
        "\n",
        "############################ Entrenamiento ###############################\n",
        "#Se entrena el agente por 10,000 pasos de tiempo y se guardan los resultados\n",
        "\n",
        "#carga los pesos iniciales para entrenar desde 0 (buena practica)\n",
        "sarsa_solucion_2.load_weights('model_sarsa_solucion_2_valoresiniciales.h5')\n",
        "\n",
        "#Comienza a entrenar\n",
        "#Guarda el historial y muestra progreso en pantalla\n",
        "historial_solucion_sarsa_ejemplo_2 = sarsa_solucion_2.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
        "\n",
        "#guarda los pesos finales (buena practica)\n",
        "sarsa_solucion_2.save_weights('model_sarsa_solucion_2_valoresfinales.h5f', overwrite=True)"
      ],
      "metadata": {
        "id": "Vpv7jA87s0dz",
        "outputId": "4e956d2b-735f-485d-b384-922836259bae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_7 (Flatten)         (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 16)                48        \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 3)                 27        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 211 (844.00 Byte)\n",
            "Trainable params: 211 (844.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100000 steps ...\n",
            "   200/100000: episode: 1, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   400/100000: episode: 2, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   600/100000: episode: 3, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   800/100000: episode: 4, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1000/100000: episode: 5, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1200/100000: episode: 6, duration: 1.569s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1400/100000: episode: 7, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1600/100000: episode: 8, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1800/100000: episode: 9, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2000/100000: episode: 10, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2200/100000: episode: 11, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2400/100000: episode: 12, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2600/100000: episode: 13, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2800/100000: episode: 14, duration: 1.753s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3000/100000: episode: 15, duration: 1.606s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3200/100000: episode: 16, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3400/100000: episode: 17, duration: 1.613s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3600/100000: episode: 18, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3800/100000: episode: 19, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4000/100000: episode: 20, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4200/100000: episode: 21, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4400/100000: episode: 22, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4600/100000: episode: 23, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4800/100000: episode: 24, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5000/100000: episode: 25, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5200/100000: episode: 26, duration: 1.677s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5400/100000: episode: 27, duration: 1.654s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5600/100000: episode: 28, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5800/100000: episode: 29, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6000/100000: episode: 30, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6200/100000: episode: 31, duration: 1.759s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6400/100000: episode: 32, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6600/100000: episode: 33, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6800/100000: episode: 34, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7000/100000: episode: 35, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7200/100000: episode: 36, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7400/100000: episode: 37, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7600/100000: episode: 38, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7800/100000: episode: 39, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8000/100000: episode: 40, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8200/100000: episode: 41, duration: 1.665s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8400/100000: episode: 42, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8600/100000: episode: 43, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8800/100000: episode: 44, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9000/100000: episode: 45, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9200/100000: episode: 46, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9400/100000: episode: 47, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9600/100000: episode: 48, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9800/100000: episode: 49, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10000/100000: episode: 50, duration: 1.645s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10200/100000: episode: 51, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10400/100000: episode: 52, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10600/100000: episode: 53, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10800/100000: episode: 54, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11000/100000: episode: 55, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11200/100000: episode: 56, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11400/100000: episode: 57, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11600/100000: episode: 58, duration: 1.698s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11800/100000: episode: 59, duration: 1.649s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12000/100000: episode: 60, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12200/100000: episode: 61, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12400/100000: episode: 62, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12600/100000: episode: 63, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12800/100000: episode: 64, duration: 1.638s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13000/100000: episode: 65, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13200/100000: episode: 66, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13400/100000: episode: 67, duration: 1.921s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13600/100000: episode: 68, duration: 1.463s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13800/100000: episode: 69, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14000/100000: episode: 70, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14200/100000: episode: 71, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14400/100000: episode: 72, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14600/100000: episode: 73, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14800/100000: episode: 74, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15000/100000: episode: 75, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15200/100000: episode: 76, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15400/100000: episode: 77, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15600/100000: episode: 78, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15800/100000: episode: 79, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16000/100000: episode: 80, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16200/100000: episode: 81, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16400/100000: episode: 82, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16600/100000: episode: 83, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16800/100000: episode: 84, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17000/100000: episode: 85, duration: 1.722s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17200/100000: episode: 86, duration: 1.502s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17400/100000: episode: 87, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17600/100000: episode: 88, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17800/100000: episode: 89, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18000/100000: episode: 90, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18200/100000: episode: 91, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18400/100000: episode: 92, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18600/100000: episode: 93, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18800/100000: episode: 94, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19000/100000: episode: 95, duration: 1.643s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19200/100000: episode: 96, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19400/100000: episode: 97, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19600/100000: episode: 98, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19800/100000: episode: 99, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20000/100000: episode: 100, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20200/100000: episode: 101, duration: 1.191s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20400/100000: episode: 102, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20600/100000: episode: 103, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20800/100000: episode: 104, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21000/100000: episode: 105, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21200/100000: episode: 106, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21400/100000: episode: 107, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21600/100000: episode: 108, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21800/100000: episode: 109, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22000/100000: episode: 110, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22200/100000: episode: 111, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22400/100000: episode: 112, duration: 1.505s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22600/100000: episode: 113, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22800/100000: episode: 114, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23000/100000: episode: 115, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23200/100000: episode: 116, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23400/100000: episode: 117, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23600/100000: episode: 118, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23800/100000: episode: 119, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24000/100000: episode: 120, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24200/100000: episode: 121, duration: 1.636s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24400/100000: episode: 122, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24600/100000: episode: 123, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24800/100000: episode: 124, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25000/100000: episode: 125, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25200/100000: episode: 126, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25400/100000: episode: 127, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25600/100000: episode: 128, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25800/100000: episode: 129, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26000/100000: episode: 130, duration: 1.567s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26200/100000: episode: 131, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26400/100000: episode: 132, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26600/100000: episode: 133, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26800/100000: episode: 134, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27000/100000: episode: 135, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27200/100000: episode: 136, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27400/100000: episode: 137, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27600/100000: episode: 138, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27800/100000: episode: 139, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28000/100000: episode: 140, duration: 1.733s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28200/100000: episode: 141, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28400/100000: episode: 142, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28600/100000: episode: 143, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28800/100000: episode: 144, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29000/100000: episode: 145, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29200/100000: episode: 146, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29400/100000: episode: 147, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29600/100000: episode: 148, duration: 1.610s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29800/100000: episode: 149, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30000/100000: episode: 150, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30200/100000: episode: 151, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30400/100000: episode: 152, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30600/100000: episode: 153, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30800/100000: episode: 154, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31000/100000: episode: 155, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31200/100000: episode: 156, duration: 1.193s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31400/100000: episode: 157, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31600/100000: episode: 158, duration: 2.496s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31800/100000: episode: 159, duration: 1.587s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32000/100000: episode: 160, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32200/100000: episode: 161, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32400/100000: episode: 162, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32600/100000: episode: 163, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32800/100000: episode: 164, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33000/100000: episode: 165, duration: 1.692s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33200/100000: episode: 166, duration: 2.023s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33400/100000: episode: 167, duration: 1.533s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33600/100000: episode: 168, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33800/100000: episode: 169, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34000/100000: episode: 170, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34200/100000: episode: 171, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34400/100000: episode: 172, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34600/100000: episode: 173, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34800/100000: episode: 174, duration: 1.698s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35000/100000: episode: 175, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35200/100000: episode: 176, duration: 1.197s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35400/100000: episode: 177, duration: 1.210s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35600/100000: episode: 178, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35800/100000: episode: 179, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36000/100000: episode: 180, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36200/100000: episode: 181, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36400/100000: episode: 182, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36600/100000: episode: 183, duration: 1.505s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36800/100000: episode: 184, duration: 1.745s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37000/100000: episode: 185, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37200/100000: episode: 186, duration: 1.203s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37400/100000: episode: 187, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37600/100000: episode: 188, duration: 1.202s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37800/100000: episode: 189, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38000/100000: episode: 190, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38200/100000: episode: 191, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38400/100000: episode: 192, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38600/100000: episode: 193, duration: 1.757s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38800/100000: episode: 194, duration: 1.423s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39000/100000: episode: 195, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39200/100000: episode: 196, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39400/100000: episode: 197, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39600/100000: episode: 198, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39800/100000: episode: 199, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40000/100000: episode: 200, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40200/100000: episode: 201, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40400/100000: episode: 202, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40600/100000: episode: 203, duration: 1.672s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40800/100000: episode: 204, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41000/100000: episode: 205, duration: 1.201s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41200/100000: episode: 206, duration: 1.214s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41400/100000: episode: 207, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41600/100000: episode: 208, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41800/100000: episode: 209, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42000/100000: episode: 210, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42200/100000: episode: 211, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42400/100000: episode: 212, duration: 1.812s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42600/100000: episode: 213, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42800/100000: episode: 214, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43000/100000: episode: 215, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43200/100000: episode: 216, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43400/100000: episode: 217, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43600/100000: episode: 218, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43800/100000: episode: 219, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44000/100000: episode: 220, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44200/100000: episode: 221, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44400/100000: episode: 222, duration: 1.536s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44600/100000: episode: 223, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44800/100000: episode: 224, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45000/100000: episode: 225, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45200/100000: episode: 226, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45400/100000: episode: 227, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45600/100000: episode: 228, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45800/100000: episode: 229, duration: 1.210s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46000/100000: episode: 230, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46200/100000: episode: 231, duration: 1.761s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46400/100000: episode: 232, duration: 1.211s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46600/100000: episode: 233, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46800/100000: episode: 234, duration: 1.199s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47000/100000: episode: 235, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47200/100000: episode: 236, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47400/100000: episode: 237, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47600/100000: episode: 238, duration: 1.191s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47800/100000: episode: 239, duration: 1.196s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48000/100000: episode: 240, duration: 1.682s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48200/100000: episode: 241, duration: 1.510s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48400/100000: episode: 242, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48600/100000: episode: 243, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48800/100000: episode: 244, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49000/100000: episode: 245, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49200/100000: episode: 246, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49400/100000: episode: 247, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49600/100000: episode: 248, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49800/100000: episode: 249, duration: 1.579s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50000/100000: episode: 250, duration: 1.722s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50200/100000: episode: 251, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50400/100000: episode: 252, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50600/100000: episode: 253, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50800/100000: episode: 254, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51000/100000: episode: 255, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51200/100000: episode: 256, duration: 1.200s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51400/100000: episode: 257, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51600/100000: episode: 258, duration: 1.278s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51800/100000: episode: 259, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52000/100000: episode: 260, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52200/100000: episode: 261, duration: 1.194s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52400/100000: episode: 262, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52600/100000: episode: 263, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52800/100000: episode: 264, duration: 1.196s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53000/100000: episode: 265, duration: 1.195s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53200/100000: episode: 266, duration: 1.201s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53400/100000: episode: 267, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53600/100000: episode: 268, duration: 1.475s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53800/100000: episode: 269, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54000/100000: episode: 270, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54200/100000: episode: 271, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54400/100000: episode: 272, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54600/100000: episode: 273, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54800/100000: episode: 274, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55000/100000: episode: 275, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55200/100000: episode: 276, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55400/100000: episode: 277, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55600/100000: episode: 278, duration: 1.816s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55800/100000: episode: 279, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56000/100000: episode: 280, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56200/100000: episode: 281, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56400/100000: episode: 282, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56600/100000: episode: 283, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56800/100000: episode: 284, duration: 1.216s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57000/100000: episode: 285, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57200/100000: episode: 286, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57400/100000: episode: 287, duration: 1.694s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57600/100000: episode: 288, duration: 1.559s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57800/100000: episode: 289, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58000/100000: episode: 290, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58200/100000: episode: 291, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58400/100000: episode: 292, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58600/100000: episode: 293, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58800/100000: episode: 294, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59000/100000: episode: 295, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59200/100000: episode: 296, duration: 1.548s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59400/100000: episode: 297, duration: 1.649s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59600/100000: episode: 298, duration: 1.196s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59800/100000: episode: 299, duration: 1.201s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60000/100000: episode: 300, duration: 1.199s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60200/100000: episode: 301, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60400/100000: episode: 302, duration: 1.209s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60600/100000: episode: 303, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60800/100000: episode: 304, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61000/100000: episode: 305, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61200/100000: episode: 306, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61400/100000: episode: 307, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61600/100000: episode: 308, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61800/100000: episode: 309, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62000/100000: episode: 310, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62200/100000: episode: 311, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62400/100000: episode: 312, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62600/100000: episode: 313, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62800/100000: episode: 314, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63000/100000: episode: 315, duration: 1.695s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63200/100000: episode: 316, duration: 1.646s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63400/100000: episode: 317, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63600/100000: episode: 318, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63800/100000: episode: 319, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64000/100000: episode: 320, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64200/100000: episode: 321, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64400/100000: episode: 322, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64600/100000: episode: 323, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64800/100000: episode: 324, duration: 1.554s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65000/100000: episode: 325, duration: 1.633s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65200/100000: episode: 326, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65400/100000: episode: 327, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65600/100000: episode: 328, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65800/100000: episode: 329, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66000/100000: episode: 330, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66200/100000: episode: 331, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66400/100000: episode: 332, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66600/100000: episode: 333, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66800/100000: episode: 334, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67000/100000: episode: 335, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67200/100000: episode: 336, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67400/100000: episode: 337, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67600/100000: episode: 338, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67800/100000: episode: 339, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68000/100000: episode: 340, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68200/100000: episode: 341, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68400/100000: episode: 342, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68600/100000: episode: 343, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68800/100000: episode: 344, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69000/100000: episode: 345, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69200/100000: episode: 346, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69400/100000: episode: 347, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69600/100000: episode: 348, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69800/100000: episode: 349, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70000/100000: episode: 350, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70200/100000: episode: 351, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70400/100000: episode: 352, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70600/100000: episode: 353, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70800/100000: episode: 354, duration: 1.203s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71000/100000: episode: 355, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71200/100000: episode: 356, duration: 1.214s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71400/100000: episode: 357, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71600/100000: episode: 358, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71800/100000: episode: 359, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72000/100000: episode: 360, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72200/100000: episode: 361, duration: 1.449s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72400/100000: episode: 362, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72600/100000: episode: 363, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72800/100000: episode: 364, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73000/100000: episode: 365, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73200/100000: episode: 366, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73400/100000: episode: 367, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73600/100000: episode: 368, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73800/100000: episode: 369, duration: 1.211s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74000/100000: episode: 370, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74200/100000: episode: 371, duration: 1.805s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74400/100000: episode: 372, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74600/100000: episode: 373, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74800/100000: episode: 374, duration: 1.197s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75000/100000: episode: 375, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75200/100000: episode: 376, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75400/100000: episode: 377, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75600/100000: episode: 378, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75800/100000: episode: 379, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76000/100000: episode: 380, duration: 1.581s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76200/100000: episode: 381, duration: 1.699s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76400/100000: episode: 382, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76600/100000: episode: 383, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76800/100000: episode: 384, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77000/100000: episode: 385, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77200/100000: episode: 386, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77400/100000: episode: 387, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77600/100000: episode: 388, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77800/100000: episode: 389, duration: 1.506s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78000/100000: episode: 390, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78200/100000: episode: 391, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78400/100000: episode: 392, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78600/100000: episode: 393, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78800/100000: episode: 394, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79000/100000: episode: 395, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79200/100000: episode: 396, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79400/100000: episode: 397, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79600/100000: episode: 398, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79800/100000: episode: 399, duration: 1.754s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80000/100000: episode: 400, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80200/100000: episode: 401, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80400/100000: episode: 402, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80600/100000: episode: 403, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80800/100000: episode: 404, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81000/100000: episode: 405, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81200/100000: episode: 406, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81400/100000: episode: 407, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81600/100000: episode: 408, duration: 1.672s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81800/100000: episode: 409, duration: 1.558s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82000/100000: episode: 410, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82200/100000: episode: 411, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82400/100000: episode: 412, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82600/100000: episode: 413, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82800/100000: episode: 414, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83000/100000: episode: 415, duration: 1.210s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83200/100000: episode: 416, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83400/100000: episode: 417, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83600/100000: episode: 418, duration: 1.727s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83800/100000: episode: 419, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84000/100000: episode: 420, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84200/100000: episode: 421, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84400/100000: episode: 422, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84600/100000: episode: 423, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84800/100000: episode: 424, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85000/100000: episode: 425, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85200/100000: episode: 426, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85400/100000: episode: 427, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85600/100000: episode: 428, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85800/100000: episode: 429, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86000/100000: episode: 430, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86200/100000: episode: 431, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86400/100000: episode: 432, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86600/100000: episode: 433, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86800/100000: episode: 434, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87000/100000: episode: 435, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87200/100000: episode: 436, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87400/100000: episode: 437, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87600/100000: episode: 438, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87800/100000: episode: 439, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88000/100000: episode: 440, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88200/100000: episode: 441, duration: 1.193s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88400/100000: episode: 442, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88600/100000: episode: 443, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88800/100000: episode: 444, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89000/100000: episode: 445, duration: 1.572s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89200/100000: episode: 446, duration: 1.613s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89400/100000: episode: 447, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89600/100000: episode: 448, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89800/100000: episode: 449, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90000/100000: episode: 450, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90200/100000: episode: 451, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90400/100000: episode: 452, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90600/100000: episode: 453, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90800/100000: episode: 454, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91000/100000: episode: 455, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91200/100000: episode: 456, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91400/100000: episode: 457, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91600/100000: episode: 458, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91800/100000: episode: 459, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92000/100000: episode: 460, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92200/100000: episode: 461, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92400/100000: episode: 462, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92600/100000: episode: 463, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92800/100000: episode: 464, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93000/100000: episode: 465, duration: 1.488s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93200/100000: episode: 466, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93400/100000: episode: 467, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93600/100000: episode: 468, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93800/100000: episode: 469, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94000/100000: episode: 470, duration: 1.195s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94200/100000: episode: 471, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94400/100000: episode: 472, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94600/100000: episode: 473, duration: 1.607s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94800/100000: episode: 474, duration: 1.682s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95000/100000: episode: 475, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95200/100000: episode: 476, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95400/100000: episode: 477, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95600/100000: episode: 478, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95800/100000: episode: 479, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96000/100000: episode: 480, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96200/100000: episode: 481, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96400/100000: episode: 482, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96600/100000: episode: 483, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96800/100000: episode: 484, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97000/100000: episode: 485, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97200/100000: episode: 486, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97400/100000: episode: 487, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97600/100000: episode: 488, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97800/100000: episode: 489, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98000/100000: episode: 490, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98200/100000: episode: 491, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98400/100000: episode: 492, duration: 1.696s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98600/100000: episode: 493, duration: 1.525s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98800/100000: episode: 494, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99000/100000: episode: 495, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99200/100000: episode: 496, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99400/100000: episode: 497, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99600/100000: episode: 498, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99800/100000: episode: 499, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100000/100000: episode: 500, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "done, took 675.410 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de construcción de una red neuronal y entrenamiento con SARSA\n",
        "\n",
        "########################## Creación del Ambiente ##########################\n",
        "env = gym.make('MountainCar-v0') #crea el ambiente utilizando gym.\n",
        "np.random.seed(613) #semilla aleatoria\n",
        "env.seed(613)\n",
        "nb_actions = env.action_space.n #obtiene la cantidad de acciones (3)\n",
        "\n",
        "###################### Creación de la Red Neuronal ########################\n",
        "# Modelo secuencial, una capa después de otra\n",
        "model = Sequential()\n",
        "\n",
        "#Capa de Entrada\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "#Capas \"Escondidas\". Aquí se pueden añadir varias capas con distinta cantidad de neuronas\n",
        "model.add(Dense(16,activation='relu'))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "\n",
        "#Capa de Salida\n",
        "model.add(Dense(nb_actions,activation='relu'))\n",
        "\n",
        "print(model.summary()) #muestra las neuronas y parámetros de la red\n",
        "\n",
        "###################### Configuración del Modelo ##########################\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1) #límite de la memoria\n",
        "policy = BoltzmannQPolicy() #política inicial\n",
        "\n",
        "#se crea el agente de SARSA\n",
        "sarsa_solucion_3 = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=100, policy=policy)\n",
        "sarsa_solucion_3.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "\n",
        "model.save_weights('model_sarsa_solucion_3_valoresiniciales.h5') #guarda los valores iniciales\n",
        "\n",
        "############################ Entrenamiento ###############################\n",
        "#Se entrena el agente por 10,000 pasos de tiempo y se guardan los resultados\n",
        "\n",
        "#carga los pesos iniciales para entrenar desde 0 (buena practica)\n",
        "sarsa_solucion_3.load_weights('model_sarsa_solucion_3_valoresiniciales.h5')\n",
        "\n",
        "#Comienza a entrenar\n",
        "#Guarda el historial y muestra progreso en pantalla\n",
        "historial_solucion_sarsa_ejemplo_3 = sarsa_solucion_3.fit(env, nb_steps=600000, visualize=False, verbose=2)\n",
        "\n",
        "#guarda los pesos finales (buena practica)\n",
        "sarsa_solucion_3.save_weights('model_sarsa_solucion_3_valoresfinales.h5f', overwrite=True)"
      ],
      "metadata": {
        "id": "ACZlZaWI6JOx",
        "outputId": "9851c486-e68f-49dd-8df6-bcb4f7fb637f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_9 (Flatten)         (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 16)                48        \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 3)                 27        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 211 (844.00 Byte)\n",
            "Trainable params: 211 (844.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 600000 steps ...\n",
            "    200/600000: episode: 1, duration: 2.422s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500237, mae: 0.333496, mean_q: 0.000693\n",
            "    400/600000: episode: 2, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "    600/600000: episode: 3, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "    800/600000: episode: 4, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1000/600000: episode: 5, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1200/600000: episode: 6, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1400/600000: episode: 7, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1600/600000: episode: 8, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1800/600000: episode: 9, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2000/600000: episode: 10, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2200/600000: episode: 11, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2400/600000: episode: 12, duration: 3.069s, episode steps: 200, steps per second:  65, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2600/600000: episode: 13, duration: 2.305s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2800/600000: episode: 14, duration: 2.773s, episode steps: 200, steps per second:  72, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3000/600000: episode: 15, duration: 2.315s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3200/600000: episode: 16, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3400/600000: episode: 17, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3600/600000: episode: 18, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3800/600000: episode: 19, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4000/600000: episode: 20, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4200/600000: episode: 21, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4400/600000: episode: 22, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4600/600000: episode: 23, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4800/600000: episode: 24, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5000/600000: episode: 25, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5200/600000: episode: 26, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5400/600000: episode: 27, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5600/600000: episode: 28, duration: 1.476s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5800/600000: episode: 29, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6000/600000: episode: 30, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6200/600000: episode: 31, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6400/600000: episode: 32, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6600/600000: episode: 33, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6800/600000: episode: 34, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7000/600000: episode: 35, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7200/600000: episode: 36, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7400/600000: episode: 37, duration: 1.529s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7600/600000: episode: 38, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7800/600000: episode: 39, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8000/600000: episode: 40, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8200/600000: episode: 41, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8400/600000: episode: 42, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8600/600000: episode: 43, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8800/600000: episode: 44, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9000/600000: episode: 45, duration: 1.737s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9200/600000: episode: 46, duration: 1.527s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9400/600000: episode: 47, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9600/600000: episode: 48, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9800/600000: episode: 49, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10000/600000: episode: 50, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10200/600000: episode: 51, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10400/600000: episode: 52, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10600/600000: episode: 53, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10800/600000: episode: 54, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11000/600000: episode: 55, duration: 1.638s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11200/600000: episode: 56, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11400/600000: episode: 57, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11600/600000: episode: 58, duration: 1.208s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11800/600000: episode: 59, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12000/600000: episode: 60, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12200/600000: episode: 61, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12400/600000: episode: 62, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12600/600000: episode: 63, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12800/600000: episode: 64, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13000/600000: episode: 65, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13200/600000: episode: 66, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13400/600000: episode: 67, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13600/600000: episode: 68, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13800/600000: episode: 69, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14000/600000: episode: 70, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14200/600000: episode: 71, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14400/600000: episode: 72, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14600/600000: episode: 73, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14800/600000: episode: 74, duration: 1.465s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15000/600000: episode: 75, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15200/600000: episode: 76, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15400/600000: episode: 77, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15600/600000: episode: 78, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15800/600000: episode: 79, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16000/600000: episode: 80, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16200/600000: episode: 81, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16400/600000: episode: 82, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16600/600000: episode: 83, duration: 1.763s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16800/600000: episode: 84, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17000/600000: episode: 85, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17200/600000: episode: 86, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17400/600000: episode: 87, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17600/600000: episode: 88, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17800/600000: episode: 89, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18000/600000: episode: 90, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18200/600000: episode: 91, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18400/600000: episode: 92, duration: 1.717s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18600/600000: episode: 93, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18800/600000: episode: 94, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19000/600000: episode: 95, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19200/600000: episode: 96, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19400/600000: episode: 97, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19600/600000: episode: 98, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19800/600000: episode: 99, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20000/600000: episode: 100, duration: 1.480s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20200/600000: episode: 101, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20400/600000: episode: 102, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20600/600000: episode: 103, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20800/600000: episode: 104, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21000/600000: episode: 105, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21200/600000: episode: 106, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21400/600000: episode: 107, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21600/600000: episode: 108, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21800/600000: episode: 109, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22000/600000: episode: 110, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22200/600000: episode: 111, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22400/600000: episode: 112, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22600/600000: episode: 113, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22800/600000: episode: 114, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23000/600000: episode: 115, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23200/600000: episode: 116, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23400/600000: episode: 117, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23600/600000: episode: 118, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23800/600000: episode: 119, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24000/600000: episode: 120, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24200/600000: episode: 121, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24400/600000: episode: 122, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24600/600000: episode: 123, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24800/600000: episode: 124, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25000/600000: episode: 125, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25200/600000: episode: 126, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25400/600000: episode: 127, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25600/600000: episode: 128, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25800/600000: episode: 129, duration: 1.607s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26000/600000: episode: 130, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26200/600000: episode: 131, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26400/600000: episode: 132, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26600/600000: episode: 133, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26800/600000: episode: 134, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27000/600000: episode: 135, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27200/600000: episode: 136, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27400/600000: episode: 137, duration: 1.559s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27600/600000: episode: 138, duration: 1.738s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27800/600000: episode: 139, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28000/600000: episode: 140, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28200/600000: episode: 141, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28400/600000: episode: 142, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28600/600000: episode: 143, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28800/600000: episode: 144, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29000/600000: episode: 145, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29200/600000: episode: 146, duration: 1.384s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29400/600000: episode: 147, duration: 1.793s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29600/600000: episode: 148, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29800/600000: episode: 149, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30000/600000: episode: 150, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30200/600000: episode: 151, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30400/600000: episode: 152, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30600/600000: episode: 153, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30800/600000: episode: 154, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31000/600000: episode: 155, duration: 1.432s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31200/600000: episode: 156, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31400/600000: episode: 157, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31600/600000: episode: 158, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31800/600000: episode: 159, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32000/600000: episode: 160, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32200/600000: episode: 161, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32400/600000: episode: 162, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32600/600000: episode: 163, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32800/600000: episode: 164, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33000/600000: episode: 165, duration: 1.802s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33200/600000: episode: 166, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33400/600000: episode: 167, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33600/600000: episode: 168, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33800/600000: episode: 169, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34000/600000: episode: 170, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34200/600000: episode: 171, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34400/600000: episode: 172, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34600/600000: episode: 173, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34800/600000: episode: 174, duration: 1.797s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35000/600000: episode: 175, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35200/600000: episode: 176, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35400/600000: episode: 177, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35600/600000: episode: 178, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35800/600000: episode: 179, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36000/600000: episode: 180, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36200/600000: episode: 181, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36400/600000: episode: 182, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36600/600000: episode: 183, duration: 1.766s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36800/600000: episode: 184, duration: 1.568s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37000/600000: episode: 185, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37200/600000: episode: 186, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37400/600000: episode: 187, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37600/600000: episode: 188, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37800/600000: episode: 189, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38000/600000: episode: 190, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38200/600000: episode: 191, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38400/600000: episode: 192, duration: 1.649s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38600/600000: episode: 193, duration: 1.673s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38800/600000: episode: 194, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39000/600000: episode: 195, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39200/600000: episode: 196, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39400/600000: episode: 197, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39600/600000: episode: 198, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39800/600000: episode: 199, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40000/600000: episode: 200, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40200/600000: episode: 201, duration: 1.631s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40400/600000: episode: 202, duration: 1.683s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40600/600000: episode: 203, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40800/600000: episode: 204, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41000/600000: episode: 205, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41200/600000: episode: 206, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41400/600000: episode: 207, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41600/600000: episode: 208, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41800/600000: episode: 209, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42000/600000: episode: 210, duration: 1.483s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42200/600000: episode: 211, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42400/600000: episode: 212, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42600/600000: episode: 213, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42800/600000: episode: 214, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43000/600000: episode: 215, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43200/600000: episode: 216, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43400/600000: episode: 217, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43600/600000: episode: 218, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43800/600000: episode: 219, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44000/600000: episode: 220, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44200/600000: episode: 221, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44400/600000: episode: 222, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44600/600000: episode: 223, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44800/600000: episode: 224, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45000/600000: episode: 225, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45200/600000: episode: 226, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45400/600000: episode: 227, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45600/600000: episode: 228, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45800/600000: episode: 229, duration: 1.750s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46000/600000: episode: 230, duration: 1.560s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46200/600000: episode: 231, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46400/600000: episode: 232, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46600/600000: episode: 233, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46800/600000: episode: 234, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47000/600000: episode: 235, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47200/600000: episode: 236, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47400/600000: episode: 237, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47600/600000: episode: 238, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47800/600000: episode: 239, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48000/600000: episode: 240, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48200/600000: episode: 241, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48400/600000: episode: 242, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48600/600000: episode: 243, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48800/600000: episode: 244, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49000/600000: episode: 245, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49200/600000: episode: 246, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49400/600000: episode: 247, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49600/600000: episode: 248, duration: 1.782s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49800/600000: episode: 249, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50000/600000: episode: 250, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50200/600000: episode: 251, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50400/600000: episode: 252, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50600/600000: episode: 253, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50800/600000: episode: 254, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51000/600000: episode: 255, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51200/600000: episode: 256, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51400/600000: episode: 257, duration: 1.705s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51600/600000: episode: 258, duration: 1.555s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51800/600000: episode: 259, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52000/600000: episode: 260, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52200/600000: episode: 261, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52400/600000: episode: 262, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52600/600000: episode: 263, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52800/600000: episode: 264, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53000/600000: episode: 265, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53200/600000: episode: 266, duration: 1.590s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53400/600000: episode: 267, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53600/600000: episode: 268, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53800/600000: episode: 269, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54000/600000: episode: 270, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54200/600000: episode: 271, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54400/600000: episode: 272, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54600/600000: episode: 273, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54800/600000: episode: 274, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55000/600000: episode: 275, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55200/600000: episode: 276, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55400/600000: episode: 277, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55600/600000: episode: 278, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55800/600000: episode: 279, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56000/600000: episode: 280, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56200/600000: episode: 281, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56400/600000: episode: 282, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56600/600000: episode: 283, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56800/600000: episode: 284, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57000/600000: episode: 285, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57200/600000: episode: 286, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57400/600000: episode: 287, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57600/600000: episode: 288, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57800/600000: episode: 289, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58000/600000: episode: 290, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58200/600000: episode: 291, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58400/600000: episode: 292, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58600/600000: episode: 293, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58800/600000: episode: 294, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59000/600000: episode: 295, duration: 1.519s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59200/600000: episode: 296, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59400/600000: episode: 297, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59600/600000: episode: 298, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59800/600000: episode: 299, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60000/600000: episode: 300, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60200/600000: episode: 301, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60400/600000: episode: 302, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60600/600000: episode: 303, duration: 1.613s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60800/600000: episode: 304, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61000/600000: episode: 305, duration: 1.972s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61200/600000: episode: 306, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61400/600000: episode: 307, duration: 1.480s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61600/600000: episode: 308, duration: 1.442s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61800/600000: episode: 309, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62000/600000: episode: 310, duration: 1.626s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62200/600000: episode: 311, duration: 1.936s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62400/600000: episode: 312, duration: 1.329s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62600/600000: episode: 313, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62800/600000: episode: 314, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63000/600000: episode: 315, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63200/600000: episode: 316, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63400/600000: episode: 317, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63600/600000: episode: 318, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63800/600000: episode: 319, duration: 1.579s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64000/600000: episode: 320, duration: 1.736s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64200/600000: episode: 321, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64400/600000: episode: 322, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64600/600000: episode: 323, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64800/600000: episode: 324, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65000/600000: episode: 325, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65200/600000: episode: 326, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65400/600000: episode: 327, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65600/600000: episode: 328, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65800/600000: episode: 329, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66000/600000: episode: 330, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66200/600000: episode: 331, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66400/600000: episode: 332, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66600/600000: episode: 333, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66800/600000: episode: 334, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67000/600000: episode: 335, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67200/600000: episode: 336, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67400/600000: episode: 337, duration: 1.538s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67600/600000: episode: 338, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67800/600000: episode: 339, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68000/600000: episode: 340, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68200/600000: episode: 341, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68400/600000: episode: 342, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68600/600000: episode: 343, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68800/600000: episode: 344, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69000/600000: episode: 345, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69200/600000: episode: 346, duration: 1.365s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69400/600000: episode: 347, duration: 1.810s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69600/600000: episode: 348, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69800/600000: episode: 349, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70000/600000: episode: 350, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70200/600000: episode: 351, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70400/600000: episode: 352, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70600/600000: episode: 353, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70800/600000: episode: 354, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71000/600000: episode: 355, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71200/600000: episode: 356, duration: 1.728s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71400/600000: episode: 357, duration: 1.553s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71600/600000: episode: 358, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71800/600000: episode: 359, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72000/600000: episode: 360, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72200/600000: episode: 361, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72400/600000: episode: 362, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72600/600000: episode: 363, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72800/600000: episode: 364, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73000/600000: episode: 365, duration: 1.584s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73200/600000: episode: 366, duration: 1.668s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73400/600000: episode: 367, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73600/600000: episode: 368, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73800/600000: episode: 369, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74000/600000: episode: 370, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74200/600000: episode: 371, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74400/600000: episode: 372, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74600/600000: episode: 373, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74800/600000: episode: 374, duration: 1.444s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75000/600000: episode: 375, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75200/600000: episode: 376, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75400/600000: episode: 377, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75600/600000: episode: 378, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75800/600000: episode: 379, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76000/600000: episode: 380, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76200/600000: episode: 381, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76400/600000: episode: 382, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76600/600000: episode: 383, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76800/600000: episode: 384, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77000/600000: episode: 385, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77200/600000: episode: 386, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77400/600000: episode: 387, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77600/600000: episode: 388, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77800/600000: episode: 389, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78000/600000: episode: 390, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78200/600000: episode: 391, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78400/600000: episode: 392, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78600/600000: episode: 393, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78800/600000: episode: 394, duration: 1.482s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79000/600000: episode: 395, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79200/600000: episode: 396, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79400/600000: episode: 397, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79600/600000: episode: 398, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79800/600000: episode: 399, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80000/600000: episode: 400, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80200/600000: episode: 401, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80400/600000: episode: 402, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80600/600000: episode: 403, duration: 1.672s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80800/600000: episode: 404, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81000/600000: episode: 405, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81200/600000: episode: 406, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81400/600000: episode: 407, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81600/600000: episode: 408, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81800/600000: episode: 409, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82000/600000: episode: 410, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82200/600000: episode: 411, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82400/600000: episode: 412, duration: 1.839s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82600/600000: episode: 413, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82800/600000: episode: 414, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83000/600000: episode: 415, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83200/600000: episode: 416, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83400/600000: episode: 417, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83600/600000: episode: 418, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83800/600000: episode: 419, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84000/600000: episode: 420, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84200/600000: episode: 421, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84400/600000: episode: 422, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84600/600000: episode: 423, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84800/600000: episode: 424, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85000/600000: episode: 425, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85200/600000: episode: 426, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85400/600000: episode: 427, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85600/600000: episode: 428, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85800/600000: episode: 429, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86000/600000: episode: 430, duration: 1.677s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86200/600000: episode: 431, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86400/600000: episode: 432, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86600/600000: episode: 433, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86800/600000: episode: 434, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87000/600000: episode: 435, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87200/600000: episode: 436, duration: 1.238s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87400/600000: episode: 437, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87600/600000: episode: 438, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87800/600000: episode: 439, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88000/600000: episode: 440, duration: 1.730s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88200/600000: episode: 441, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88400/600000: episode: 442, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88600/600000: episode: 443, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88800/600000: episode: 444, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89000/600000: episode: 445, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89200/600000: episode: 446, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89400/600000: episode: 447, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89600/600000: episode: 448, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89800/600000: episode: 449, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90000/600000: episode: 450, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90200/600000: episode: 451, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90400/600000: episode: 452, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90600/600000: episode: 453, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90800/600000: episode: 454, duration: 1.207s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91000/600000: episode: 455, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91200/600000: episode: 456, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91400/600000: episode: 457, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91600/600000: episode: 458, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91800/600000: episode: 459, duration: 1.584s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92000/600000: episode: 460, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92200/600000: episode: 461, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92400/600000: episode: 462, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92600/600000: episode: 463, duration: 1.206s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92800/600000: episode: 464, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93000/600000: episode: 465, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93200/600000: episode: 466, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93400/600000: episode: 467, duration: 1.547s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93600/600000: episode: 468, duration: 1.712s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93800/600000: episode: 469, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94000/600000: episode: 470, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94200/600000: episode: 471, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94400/600000: episode: 472, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94600/600000: episode: 473, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94800/600000: episode: 474, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95000/600000: episode: 475, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95200/600000: episode: 476, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95400/600000: episode: 477, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95600/600000: episode: 478, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95800/600000: episode: 479, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96000/600000: episode: 480, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96200/600000: episode: 481, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96400/600000: episode: 482, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96600/600000: episode: 483, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96800/600000: episode: 484, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97000/600000: episode: 485, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97200/600000: episode: 486, duration: 1.732s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97400/600000: episode: 487, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97600/600000: episode: 488, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97800/600000: episode: 489, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98000/600000: episode: 490, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98200/600000: episode: 491, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98400/600000: episode: 492, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98600/600000: episode: 493, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98800/600000: episode: 494, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99000/600000: episode: 495, duration: 1.639s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99200/600000: episode: 496, duration: 1.648s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99400/600000: episode: 497, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99600/600000: episode: 498, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99800/600000: episode: 499, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100000/600000: episode: 500, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100200/600000: episode: 501, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100400/600000: episode: 502, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100600/600000: episode: 503, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100800/600000: episode: 504, duration: 1.413s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101000/600000: episode: 505, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101200/600000: episode: 506, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101400/600000: episode: 507, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101600/600000: episode: 508, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101800/600000: episode: 509, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102000/600000: episode: 510, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102200/600000: episode: 511, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102400/600000: episode: 512, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102600/600000: episode: 513, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102800/600000: episode: 514, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103000/600000: episode: 515, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103200/600000: episode: 516, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103400/600000: episode: 517, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103600/600000: episode: 518, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103800/600000: episode: 519, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104000/600000: episode: 520, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104200/600000: episode: 521, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104400/600000: episode: 522, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104600/600000: episode: 523, duration: 1.617s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104800/600000: episode: 524, duration: 1.623s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105000/600000: episode: 525, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105200/600000: episode: 526, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105400/600000: episode: 527, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105600/600000: episode: 528, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105800/600000: episode: 529, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106000/600000: episode: 530, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106200/600000: episode: 531, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106400/600000: episode: 532, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106600/600000: episode: 533, duration: 1.764s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106800/600000: episode: 534, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107000/600000: episode: 535, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107200/600000: episode: 536, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107400/600000: episode: 537, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107600/600000: episode: 538, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107800/600000: episode: 539, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108000/600000: episode: 540, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108200/600000: episode: 541, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108400/600000: episode: 542, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108600/600000: episode: 543, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108800/600000: episode: 544, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109000/600000: episode: 545, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109200/600000: episode: 546, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109400/600000: episode: 547, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109600/600000: episode: 548, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109800/600000: episode: 549, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110000/600000: episode: 550, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110200/600000: episode: 551, duration: 1.723s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110400/600000: episode: 552, duration: 1.612s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110600/600000: episode: 553, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110800/600000: episode: 554, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111000/600000: episode: 555, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111200/600000: episode: 556, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111400/600000: episode: 557, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111600/600000: episode: 558, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111800/600000: episode: 559, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112000/600000: episode: 560, duration: 1.592s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112200/600000: episode: 561, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112400/600000: episode: 562, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112600/600000: episode: 563, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112800/600000: episode: 564, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113000/600000: episode: 565, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113200/600000: episode: 566, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113400/600000: episode: 567, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113600/600000: episode: 568, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113800/600000: episode: 569, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114000/600000: episode: 570, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114200/600000: episode: 571, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114400/600000: episode: 572, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114600/600000: episode: 573, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114800/600000: episode: 574, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115000/600000: episode: 575, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115200/600000: episode: 576, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115400/600000: episode: 577, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115600/600000: episode: 578, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115800/600000: episode: 579, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116000/600000: episode: 580, duration: 1.448s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116200/600000: episode: 581, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116400/600000: episode: 582, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116600/600000: episode: 583, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116800/600000: episode: 584, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117000/600000: episode: 585, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117200/600000: episode: 586, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117400/600000: episode: 587, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117600/600000: episode: 588, duration: 1.569s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117800/600000: episode: 589, duration: 1.633s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118000/600000: episode: 590, duration: 1.190s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118200/600000: episode: 591, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118400/600000: episode: 592, duration: 1.195s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118600/600000: episode: 593, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118800/600000: episode: 594, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119000/600000: episode: 595, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119200/600000: episode: 596, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119400/600000: episode: 597, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119600/600000: episode: 598, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119800/600000: episode: 599, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120000/600000: episode: 600, duration: 1.208s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120200/600000: episode: 601, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120400/600000: episode: 602, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120600/600000: episode: 603, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120800/600000: episode: 604, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121000/600000: episode: 605, duration: 1.210s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121200/600000: episode: 606, duration: 1.202s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121400/600000: episode: 607, duration: 1.641s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121600/600000: episode: 608, duration: 1.560s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121800/600000: episode: 609, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122000/600000: episode: 610, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122200/600000: episode: 611, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122400/600000: episode: 612, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122600/600000: episode: 613, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122800/600000: episode: 614, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123000/600000: episode: 615, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123200/600000: episode: 616, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123400/600000: episode: 617, duration: 1.843s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123600/600000: episode: 618, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123800/600000: episode: 619, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124000/600000: episode: 620, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124200/600000: episode: 621, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124400/600000: episode: 622, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124600/600000: episode: 623, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124800/600000: episode: 624, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125000/600000: episode: 625, duration: 1.486s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125200/600000: episode: 626, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125400/600000: episode: 627, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125600/600000: episode: 628, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125800/600000: episode: 629, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126000/600000: episode: 630, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126200/600000: episode: 631, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126400/600000: episode: 632, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126600/600000: episode: 633, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126800/600000: episode: 634, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127000/600000: episode: 635, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127200/600000: episode: 636, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127400/600000: episode: 637, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127600/600000: episode: 638, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127800/600000: episode: 639, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128000/600000: episode: 640, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128200/600000: episode: 641, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128400/600000: episode: 642, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128600/600000: episode: 643, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128800/600000: episode: 644, duration: 1.724s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129000/600000: episode: 645, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129200/600000: episode: 646, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129400/600000: episode: 647, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129600/600000: episode: 648, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129800/600000: episode: 649, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130000/600000: episode: 650, duration: 1.286s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130200/600000: episode: 651, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130400/600000: episode: 652, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130600/600000: episode: 653, duration: 1.612s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130800/600000: episode: 654, duration: 1.617s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131000/600000: episode: 655, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131200/600000: episode: 656, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131400/600000: episode: 657, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131600/600000: episode: 658, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131800/600000: episode: 659, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132000/600000: episode: 660, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132200/600000: episode: 661, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132400/600000: episode: 662, duration: 1.418s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132600/600000: episode: 663, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132800/600000: episode: 664, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133000/600000: episode: 665, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133200/600000: episode: 666, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133400/600000: episode: 667, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133600/600000: episode: 668, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133800/600000: episode: 669, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134000/600000: episode: 670, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134200/600000: episode: 671, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134400/600000: episode: 672, duration: 1.810s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134600/600000: episode: 673, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134800/600000: episode: 674, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135000/600000: episode: 675, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135200/600000: episode: 676, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135400/600000: episode: 677, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135600/600000: episode: 678, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135800/600000: episode: 679, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136000/600000: episode: 680, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136200/600000: episode: 681, duration: 1.709s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136400/600000: episode: 682, duration: 1.544s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136600/600000: episode: 683, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136800/600000: episode: 684, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137000/600000: episode: 685, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137200/600000: episode: 686, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137400/600000: episode: 687, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137600/600000: episode: 688, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137800/600000: episode: 689, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138000/600000: episode: 690, duration: 1.555s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138200/600000: episode: 691, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138400/600000: episode: 692, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138600/600000: episode: 693, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138800/600000: episode: 694, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139000/600000: episode: 695, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139200/600000: episode: 696, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139400/600000: episode: 697, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139600/600000: episode: 698, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139800/600000: episode: 699, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140000/600000: episode: 700, duration: 1.773s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140200/600000: episode: 701, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140400/600000: episode: 702, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140600/600000: episode: 703, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140800/600000: episode: 704, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141000/600000: episode: 705, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141200/600000: episode: 706, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141400/600000: episode: 707, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141600/600000: episode: 708, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141800/600000: episode: 709, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142000/600000: episode: 710, duration: 1.588s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142200/600000: episode: 711, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142400/600000: episode: 712, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142600/600000: episode: 713, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142800/600000: episode: 714, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143000/600000: episode: 715, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143200/600000: episode: 716, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143400/600000: episode: 717, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143600/600000: episode: 718, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143800/600000: episode: 719, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144000/600000: episode: 720, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144200/600000: episode: 721, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144400/600000: episode: 722, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144600/600000: episode: 723, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144800/600000: episode: 724, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145000/600000: episode: 725, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145200/600000: episode: 726, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145400/600000: episode: 727, duration: 1.648s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145600/600000: episode: 728, duration: 1.684s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145800/600000: episode: 729, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146000/600000: episode: 730, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146200/600000: episode: 731, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146400/600000: episode: 732, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146600/600000: episode: 733, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146800/600000: episode: 734, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147000/600000: episode: 735, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147200/600000: episode: 736, duration: 1.423s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147400/600000: episode: 737, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147600/600000: episode: 738, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147800/600000: episode: 739, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148000/600000: episode: 740, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148200/600000: episode: 741, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148400/600000: episode: 742, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148600/600000: episode: 743, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148800/600000: episode: 744, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149000/600000: episode: 745, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149200/600000: episode: 746, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149400/600000: episode: 747, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149600/600000: episode: 748, duration: 1.201s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149800/600000: episode: 749, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150000/600000: episode: 750, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150200/600000: episode: 751, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150400/600000: episode: 752, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150600/600000: episode: 753, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150800/600000: episode: 754, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151000/600000: episode: 755, duration: 1.705s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151200/600000: episode: 756, duration: 1.526s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151400/600000: episode: 757, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151600/600000: episode: 758, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151800/600000: episode: 759, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152000/600000: episode: 760, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152200/600000: episode: 761, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152400/600000: episode: 762, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152600/600000: episode: 763, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152800/600000: episode: 764, duration: 1.565s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153000/600000: episode: 765, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153200/600000: episode: 766, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153400/600000: episode: 767, duration: 1.214s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153600/600000: episode: 768, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153800/600000: episode: 769, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154000/600000: episode: 770, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154200/600000: episode: 771, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154400/600000: episode: 772, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154600/600000: episode: 773, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154800/600000: episode: 774, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155000/600000: episode: 775, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155200/600000: episode: 776, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155400/600000: episode: 777, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155600/600000: episode: 778, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155800/600000: episode: 779, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156000/600000: episode: 780, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156200/600000: episode: 781, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156400/600000: episode: 782, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156600/600000: episode: 783, duration: 1.769s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156800/600000: episode: 784, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157000/600000: episode: 785, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157200/600000: episode: 786, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157400/600000: episode: 787, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157600/600000: episode: 788, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157800/600000: episode: 789, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158000/600000: episode: 790, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158200/600000: episode: 791, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158400/600000: episode: 792, duration: 1.680s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158600/600000: episode: 793, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158800/600000: episode: 794, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159000/600000: episode: 795, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159200/600000: episode: 796, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159400/600000: episode: 797, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159600/600000: episode: 798, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159800/600000: episode: 799, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160000/600000: episode: 800, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160200/600000: episode: 801, duration: 1.561s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160400/600000: episode: 802, duration: 1.679s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160600/600000: episode: 803, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160800/600000: episode: 804, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161000/600000: episode: 805, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161200/600000: episode: 806, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161400/600000: episode: 807, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161600/600000: episode: 808, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161800/600000: episode: 809, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162000/600000: episode: 810, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162200/600000: episode: 811, duration: 1.790s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162400/600000: episode: 812, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162600/600000: episode: 813, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162800/600000: episode: 814, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163000/600000: episode: 815, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163200/600000: episode: 816, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163400/600000: episode: 817, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163600/600000: episode: 818, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163800/600000: episode: 819, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164000/600000: episode: 820, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164200/600000: episode: 821, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164400/600000: episode: 822, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164600/600000: episode: 823, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164800/600000: episode: 824, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165000/600000: episode: 825, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165200/600000: episode: 826, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165400/600000: episode: 827, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165600/600000: episode: 828, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165800/600000: episode: 829, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166000/600000: episode: 830, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166200/600000: episode: 831, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166400/600000: episode: 832, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166600/600000: episode: 833, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166800/600000: episode: 834, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167000/600000: episode: 835, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167200/600000: episode: 836, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167400/600000: episode: 837, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167600/600000: episode: 838, duration: 1.665s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167800/600000: episode: 839, duration: 1.627s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168000/600000: episode: 840, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168200/600000: episode: 841, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168400/600000: episode: 842, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168600/600000: episode: 843, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168800/600000: episode: 844, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169000/600000: episode: 845, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169200/600000: episode: 846, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169400/600000: episode: 847, duration: 1.556s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169600/600000: episode: 848, duration: 1.717s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169800/600000: episode: 849, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170000/600000: episode: 850, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170200/600000: episode: 851, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170400/600000: episode: 852, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170600/600000: episode: 853, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170800/600000: episode: 854, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171000/600000: episode: 855, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171200/600000: episode: 856, duration: 1.513s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171400/600000: episode: 857, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171600/600000: episode: 858, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171800/600000: episode: 859, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172000/600000: episode: 860, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172200/600000: episode: 861, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172400/600000: episode: 862, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172600/600000: episode: 863, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172800/600000: episode: 864, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173000/600000: episode: 865, duration: 1.453s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173200/600000: episode: 866, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173400/600000: episode: 867, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173600/600000: episode: 868, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173800/600000: episode: 869, duration: 1.206s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174000/600000: episode: 870, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174200/600000: episode: 871, duration: 1.211s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174400/600000: episode: 872, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174600/600000: episode: 873, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174800/600000: episode: 874, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175000/600000: episode: 875, duration: 1.725s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175200/600000: episode: 876, duration: 1.543s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175400/600000: episode: 877, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175600/600000: episode: 878, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175800/600000: episode: 879, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176000/600000: episode: 880, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176200/600000: episode: 881, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176400/600000: episode: 882, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176600/600000: episode: 883, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176800/600000: episode: 884, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177000/600000: episode: 885, duration: 1.637s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177200/600000: episode: 886, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177400/600000: episode: 887, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177600/600000: episode: 888, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177800/600000: episode: 889, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178000/600000: episode: 890, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178200/600000: episode: 891, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178400/600000: episode: 892, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178600/600000: episode: 893, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178800/600000: episode: 894, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179000/600000: episode: 895, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179200/600000: episode: 896, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179400/600000: episode: 897, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179600/600000: episode: 898, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179800/600000: episode: 899, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180000/600000: episode: 900, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180200/600000: episode: 901, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180400/600000: episode: 902, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180600/600000: episode: 903, duration: 1.798s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180800/600000: episode: 904, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181000/600000: episode: 905, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181200/600000: episode: 906, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181400/600000: episode: 907, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181600/600000: episode: 908, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181800/600000: episode: 909, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182000/600000: episode: 910, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182200/600000: episode: 911, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182400/600000: episode: 912, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182600/600000: episode: 913, duration: 1.572s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182800/600000: episode: 914, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183000/600000: episode: 915, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183200/600000: episode: 916, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183400/600000: episode: 917, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183600/600000: episode: 918, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183800/600000: episode: 919, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184000/600000: episode: 920, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184200/600000: episode: 921, duration: 1.585s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184400/600000: episode: 922, duration: 1.665s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184600/600000: episode: 923, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184800/600000: episode: 924, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185000/600000: episode: 925, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185200/600000: episode: 926, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185400/600000: episode: 927, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185600/600000: episode: 928, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185800/600000: episode: 929, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186000/600000: episode: 930, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186200/600000: episode: 931, duration: 1.768s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186400/600000: episode: 932, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186600/600000: episode: 933, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186800/600000: episode: 934, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187000/600000: episode: 935, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187200/600000: episode: 936, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187400/600000: episode: 937, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187600/600000: episode: 938, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187800/600000: episode: 939, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188000/600000: episode: 940, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188200/600000: episode: 941, duration: 1.531s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188400/600000: episode: 942, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188600/600000: episode: 943, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188800/600000: episode: 944, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189000/600000: episode: 945, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189200/600000: episode: 946, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189400/600000: episode: 947, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189600/600000: episode: 948, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189800/600000: episode: 949, duration: 1.520s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190000/600000: episode: 950, duration: 1.688s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190200/600000: episode: 951, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190400/600000: episode: 952, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190600/600000: episode: 953, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190800/600000: episode: 954, duration: 1.199s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191000/600000: episode: 955, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191200/600000: episode: 956, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191400/600000: episode: 957, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191600/600000: episode: 958, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191800/600000: episode: 959, duration: 1.890s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192000/600000: episode: 960, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192200/600000: episode: 961, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192400/600000: episode: 962, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192600/600000: episode: 963, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192800/600000: episode: 964, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193000/600000: episode: 965, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193200/600000: episode: 966, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193400/600000: episode: 967, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193600/600000: episode: 968, duration: 1.778s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193800/600000: episode: 969, duration: 1.454s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194000/600000: episode: 970, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194200/600000: episode: 971, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194400/600000: episode: 972, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194600/600000: episode: 973, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194800/600000: episode: 974, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195000/600000: episode: 975, duration: 1.204s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195200/600000: episode: 976, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195400/600000: episode: 977, duration: 1.607s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195600/600000: episode: 978, duration: 1.736s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195800/600000: episode: 979, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196000/600000: episode: 980, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196200/600000: episode: 981, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196400/600000: episode: 982, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196600/600000: episode: 983, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196800/600000: episode: 984, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197000/600000: episode: 985, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197200/600000: episode: 986, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197400/600000: episode: 987, duration: 1.721s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197600/600000: episode: 988, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197800/600000: episode: 989, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198000/600000: episode: 990, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198200/600000: episode: 991, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198400/600000: episode: 992, duration: 1.209s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198600/600000: episode: 993, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198800/600000: episode: 994, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199000/600000: episode: 995, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199200/600000: episode: 996, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199400/600000: episode: 997, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199600/600000: episode: 998, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199800/600000: episode: 999, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200000/600000: episode: 1000, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200200/600000: episode: 1001, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200400/600000: episode: 1002, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200600/600000: episode: 1003, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200800/600000: episode: 1004, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201000/600000: episode: 1005, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201200/600000: episode: 1006, duration: 1.580s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201400/600000: episode: 1007, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201600/600000: episode: 1008, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201800/600000: episode: 1009, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202000/600000: episode: 1010, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202200/600000: episode: 1011, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202400/600000: episode: 1012, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202600/600000: episode: 1013, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202800/600000: episode: 1014, duration: 1.555s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203000/600000: episode: 1015, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203200/600000: episode: 1016, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203400/600000: episode: 1017, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203600/600000: episode: 1018, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203800/600000: episode: 1019, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204000/600000: episode: 1020, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204200/600000: episode: 1021, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204400/600000: episode: 1022, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204600/600000: episode: 1023, duration: 1.438s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204800/600000: episode: 1024, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205000/600000: episode: 1025, duration: 1.202s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205200/600000: episode: 1026, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205400/600000: episode: 1027, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205600/600000: episode: 1028, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205800/600000: episode: 1029, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206000/600000: episode: 1030, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206200/600000: episode: 1031, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206400/600000: episode: 1032, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206600/600000: episode: 1033, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206800/600000: episode: 1034, duration: 1.476s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207000/600000: episode: 1035, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207200/600000: episode: 1036, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207400/600000: episode: 1037, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207600/600000: episode: 1038, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207800/600000: episode: 1039, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208000/600000: episode: 1040, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208200/600000: episode: 1041, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208400/600000: episode: 1042, duration: 1.571s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208600/600000: episode: 1043, duration: 1.630s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208800/600000: episode: 1044, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209000/600000: episode: 1045, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209200/600000: episode: 1046, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209400/600000: episode: 1047, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209600/600000: episode: 1048, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209800/600000: episode: 1049, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210000/600000: episode: 1050, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210200/600000: episode: 1051, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210400/600000: episode: 1052, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210600/600000: episode: 1053, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210800/600000: episode: 1054, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211000/600000: episode: 1055, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211200/600000: episode: 1056, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211400/600000: episode: 1057, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211600/600000: episode: 1058, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211800/600000: episode: 1059, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212000/600000: episode: 1060, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212200/600000: episode: 1061, duration: 1.727s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212400/600000: episode: 1062, duration: 1.469s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212600/600000: episode: 1063, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212800/600000: episode: 1064, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213000/600000: episode: 1065, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213200/600000: episode: 1066, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213400/600000: episode: 1067, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213600/600000: episode: 1068, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213800/600000: episode: 1069, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214000/600000: episode: 1070, duration: 1.580s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214200/600000: episode: 1071, duration: 1.635s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214400/600000: episode: 1072, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214600/600000: episode: 1073, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214800/600000: episode: 1074, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215000/600000: episode: 1075, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215200/600000: episode: 1076, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215400/600000: episode: 1077, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215600/600000: episode: 1078, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215800/600000: episode: 1079, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216000/600000: episode: 1080, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216200/600000: episode: 1081, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216400/600000: episode: 1082, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216600/600000: episode: 1083, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216800/600000: episode: 1084, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217000/600000: episode: 1085, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217200/600000: episode: 1086, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217400/600000: episode: 1087, duration: 1.210s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217600/600000: episode: 1088, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217800/600000: episode: 1089, duration: 1.761s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218000/600000: episode: 1090, duration: 1.501s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218200/600000: episode: 1091, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218400/600000: episode: 1092, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218600/600000: episode: 1093, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218800/600000: episode: 1094, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219000/600000: episode: 1095, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219200/600000: episode: 1096, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219400/600000: episode: 1097, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219600/600000: episode: 1098, duration: 1.708s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219800/600000: episode: 1099, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220000/600000: episode: 1100, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220200/600000: episode: 1101, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220400/600000: episode: 1102, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220600/600000: episode: 1103, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220800/600000: episode: 1104, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221000/600000: episode: 1105, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221200/600000: episode: 1106, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221400/600000: episode: 1107, duration: 1.510s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221600/600000: episode: 1108, duration: 1.764s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221800/600000: episode: 1109, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222000/600000: episode: 1110, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222200/600000: episode: 1111, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222400/600000: episode: 1112, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222600/600000: episode: 1113, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222800/600000: episode: 1114, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223000/600000: episode: 1115, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223200/600000: episode: 1116, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223400/600000: episode: 1117, duration: 1.781s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223600/600000: episode: 1118, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223800/600000: episode: 1119, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224000/600000: episode: 1120, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224200/600000: episode: 1121, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224400/600000: episode: 1122, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224600/600000: episode: 1123, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224800/600000: episode: 1124, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225000/600000: episode: 1125, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225200/600000: episode: 1126, duration: 1.727s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225400/600000: episode: 1127, duration: 1.518s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225600/600000: episode: 1128, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225800/600000: episode: 1129, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226000/600000: episode: 1130, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226200/600000: episode: 1131, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226400/600000: episode: 1132, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226600/600000: episode: 1133, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226800/600000: episode: 1134, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227000/600000: episode: 1135, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227200/600000: episode: 1136, duration: 1.744s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227400/600000: episode: 1137, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227600/600000: episode: 1138, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227800/600000: episode: 1139, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228000/600000: episode: 1140, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228200/600000: episode: 1141, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228400/600000: episode: 1142, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228600/600000: episode: 1143, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228800/600000: episode: 1144, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229000/600000: episode: 1145, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229200/600000: episode: 1146, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229400/600000: episode: 1147, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229600/600000: episode: 1148, duration: 1.216s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229800/600000: episode: 1149, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230000/600000: episode: 1150, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230200/600000: episode: 1151, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230400/600000: episode: 1152, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230600/600000: episode: 1153, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230800/600000: episode: 1154, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231000/600000: episode: 1155, duration: 1.549s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231200/600000: episode: 1156, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231400/600000: episode: 1157, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231600/600000: episode: 1158, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231800/600000: episode: 1159, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232000/600000: episode: 1160, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232200/600000: episode: 1161, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232400/600000: episode: 1162, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232600/600000: episode: 1163, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232800/600000: episode: 1164, duration: 1.726s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233000/600000: episode: 1165, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233200/600000: episode: 1166, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233400/600000: episode: 1167, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233600/600000: episode: 1168, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233800/600000: episode: 1169, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234000/600000: episode: 1170, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234200/600000: episode: 1171, duration: 1.209s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234400/600000: episode: 1172, duration: 1.416s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234600/600000: episode: 1173, duration: 1.804s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234800/600000: episode: 1174, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235000/600000: episode: 1175, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235200/600000: episode: 1176, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235400/600000: episode: 1177, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235600/600000: episode: 1178, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235800/600000: episode: 1179, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236000/600000: episode: 1180, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236200/600000: episode: 1181, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236400/600000: episode: 1182, duration: 1.824s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236600/600000: episode: 1183, duration: 1.472s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236800/600000: episode: 1184, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237000/600000: episode: 1185, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237200/600000: episode: 1186, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237400/600000: episode: 1187, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237600/600000: episode: 1188, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237800/600000: episode: 1189, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238000/600000: episode: 1190, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238200/600000: episode: 1191, duration: 1.654s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238400/600000: episode: 1192, duration: 1.630s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238600/600000: episode: 1193, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238800/600000: episode: 1194, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239000/600000: episode: 1195, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239200/600000: episode: 1196, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239400/600000: episode: 1197, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239600/600000: episode: 1198, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239800/600000: episode: 1199, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240000/600000: episode: 1200, duration: 1.563s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240200/600000: episode: 1201, duration: 1.820s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240400/600000: episode: 1202, duration: 1.640s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240600/600000: episode: 1203, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240800/600000: episode: 1204, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241000/600000: episode: 1205, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241200/600000: episode: 1206, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241400/600000: episode: 1207, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241600/600000: episode: 1208, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241800/600000: episode: 1209, duration: 1.746s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242000/600000: episode: 1210, duration: 1.614s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242200/600000: episode: 1211, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242400/600000: episode: 1212, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242600/600000: episode: 1213, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242800/600000: episode: 1214, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243000/600000: episode: 1215, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243200/600000: episode: 1216, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243400/600000: episode: 1217, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243600/600000: episode: 1218, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243800/600000: episode: 1219, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244000/600000: episode: 1220, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244200/600000: episode: 1221, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244400/600000: episode: 1222, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244600/600000: episode: 1223, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244800/600000: episode: 1224, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245000/600000: episode: 1225, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245200/600000: episode: 1226, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245400/600000: episode: 1227, duration: 1.755s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245600/600000: episode: 1228, duration: 1.647s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245800/600000: episode: 1229, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246000/600000: episode: 1230, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246200/600000: episode: 1231, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246400/600000: episode: 1232, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246600/600000: episode: 1233, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246800/600000: episode: 1234, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247000/600000: episode: 1235, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247200/600000: episode: 1236, duration: 1.605s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247400/600000: episode: 1237, duration: 1.676s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247600/600000: episode: 1238, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247800/600000: episode: 1239, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248000/600000: episode: 1240, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248200/600000: episode: 1241, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248400/600000: episode: 1242, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248600/600000: episode: 1243, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248800/600000: episode: 1244, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249000/600000: episode: 1245, duration: 1.404s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249200/600000: episode: 1246, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249400/600000: episode: 1247, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249600/600000: episode: 1248, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249800/600000: episode: 1249, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250000/600000: episode: 1250, duration: 1.201s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250200/600000: episode: 1251, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250400/600000: episode: 1252, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250600/600000: episode: 1253, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250800/600000: episode: 1254, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251000/600000: episode: 1255, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251200/600000: episode: 1256, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251400/600000: episode: 1257, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251600/600000: episode: 1258, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251800/600000: episode: 1259, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252000/600000: episode: 1260, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252200/600000: episode: 1261, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252400/600000: episode: 1262, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252600/600000: episode: 1263, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252800/600000: episode: 1264, duration: 1.729s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253000/600000: episode: 1265, duration: 1.525s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253200/600000: episode: 1266, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253400/600000: episode: 1267, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253600/600000: episode: 1268, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253800/600000: episode: 1269, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254000/600000: episode: 1270, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254200/600000: episode: 1271, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254400/600000: episode: 1272, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254600/600000: episode: 1273, duration: 1.688s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254800/600000: episode: 1274, duration: 1.626s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255000/600000: episode: 1275, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255200/600000: episode: 1276, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255400/600000: episode: 1277, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255600/600000: episode: 1278, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255800/600000: episode: 1279, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256000/600000: episode: 1280, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256200/600000: episode: 1281, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256400/600000: episode: 1282, duration: 1.611s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256600/600000: episode: 1283, duration: 1.643s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256800/600000: episode: 1284, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257000/600000: episode: 1285, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257200/600000: episode: 1286, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257400/600000: episode: 1287, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257600/600000: episode: 1288, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257800/600000: episode: 1289, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258000/600000: episode: 1290, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258200/600000: episode: 1291, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258400/600000: episode: 1292, duration: 1.839s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258600/600000: episode: 1293, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258800/600000: episode: 1294, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259000/600000: episode: 1295, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259200/600000: episode: 1296, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259400/600000: episode: 1297, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259600/600000: episode: 1298, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259800/600000: episode: 1299, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260000/600000: episode: 1300, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260200/600000: episode: 1301, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260400/600000: episode: 1302, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260600/600000: episode: 1303, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260800/600000: episode: 1304, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261000/600000: episode: 1305, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261200/600000: episode: 1306, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261400/600000: episode: 1307, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261600/600000: episode: 1308, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261800/600000: episode: 1309, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262000/600000: episode: 1310, duration: 1.732s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262200/600000: episode: 1311, duration: 1.517s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262400/600000: episode: 1312, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262600/600000: episode: 1313, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262800/600000: episode: 1314, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263000/600000: episode: 1315, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263200/600000: episode: 1316, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263400/600000: episode: 1317, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263600/600000: episode: 1318, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263800/600000: episode: 1319, duration: 1.642s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264000/600000: episode: 1320, duration: 1.642s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264200/600000: episode: 1321, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264400/600000: episode: 1322, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264600/600000: episode: 1323, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264800/600000: episode: 1324, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265000/600000: episode: 1325, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265200/600000: episode: 1326, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265400/600000: episode: 1327, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265600/600000: episode: 1328, duration: 1.663s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265800/600000: episode: 1329, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266000/600000: episode: 1330, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266200/600000: episode: 1331, duration: 1.208s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266400/600000: episode: 1332, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266600/600000: episode: 1333, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266800/600000: episode: 1334, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267000/600000: episode: 1335, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267200/600000: episode: 1336, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267400/600000: episode: 1337, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267600/600000: episode: 1338, duration: 1.757s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267800/600000: episode: 1339, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268000/600000: episode: 1340, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268200/600000: episode: 1341, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268400/600000: episode: 1342, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268600/600000: episode: 1343, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268800/600000: episode: 1344, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269000/600000: episode: 1345, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269200/600000: episode: 1346, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269400/600000: episode: 1347, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269600/600000: episode: 1348, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269800/600000: episode: 1349, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270000/600000: episode: 1350, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270200/600000: episode: 1351, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270400/600000: episode: 1352, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270600/600000: episode: 1353, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270800/600000: episode: 1354, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271000/600000: episode: 1355, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271200/600000: episode: 1356, duration: 1.810s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271400/600000: episode: 1357, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271600/600000: episode: 1358, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271800/600000: episode: 1359, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272000/600000: episode: 1360, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272200/600000: episode: 1361, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272400/600000: episode: 1362, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272600/600000: episode: 1363, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272800/600000: episode: 1364, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273000/600000: episode: 1365, duration: 1.665s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273200/600000: episode: 1366, duration: 1.604s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273400/600000: episode: 1367, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273600/600000: episode: 1368, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273800/600000: episode: 1369, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274000/600000: episode: 1370, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274200/600000: episode: 1371, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274400/600000: episode: 1372, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274600/600000: episode: 1373, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274800/600000: episode: 1374, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275000/600000: episode: 1375, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275200/600000: episode: 1376, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275400/600000: episode: 1377, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275600/600000: episode: 1378, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275800/600000: episode: 1379, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276000/600000: episode: 1380, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276200/600000: episode: 1381, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276400/600000: episode: 1382, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276600/600000: episode: 1383, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276800/600000: episode: 1384, duration: 1.812s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277000/600000: episode: 1385, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277200/600000: episode: 1386, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277400/600000: episode: 1387, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277600/600000: episode: 1388, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277800/600000: episode: 1389, duration: 1.238s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278000/600000: episode: 1390, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278200/600000: episode: 1391, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278400/600000: episode: 1392, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278600/600000: episode: 1393, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278800/600000: episode: 1394, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279000/600000: episode: 1395, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279200/600000: episode: 1396, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279400/600000: episode: 1397, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279600/600000: episode: 1398, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279800/600000: episode: 1399, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280000/600000: episode: 1400, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280200/600000: episode: 1401, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280400/600000: episode: 1402, duration: 1.543s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280600/600000: episode: 1403, duration: 1.671s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280800/600000: episode: 1404, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281000/600000: episode: 1405, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281200/600000: episode: 1406, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281400/600000: episode: 1407, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281600/600000: episode: 1408, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281800/600000: episode: 1409, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282000/600000: episode: 1410, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282200/600000: episode: 1411, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282400/600000: episode: 1412, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282600/600000: episode: 1413, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282800/600000: episode: 1414, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283000/600000: episode: 1415, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283200/600000: episode: 1416, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283400/600000: episode: 1417, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283600/600000: episode: 1418, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283800/600000: episode: 1419, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284000/600000: episode: 1420, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284200/600000: episode: 1421, duration: 1.726s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284400/600000: episode: 1422, duration: 1.506s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284600/600000: episode: 1423, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284800/600000: episode: 1424, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285000/600000: episode: 1425, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285200/600000: episode: 1426, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285400/600000: episode: 1427, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285600/600000: episode: 1428, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285800/600000: episode: 1429, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286000/600000: episode: 1430, duration: 1.584s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286200/600000: episode: 1431, duration: 1.638s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286400/600000: episode: 1432, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286600/600000: episode: 1433, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286800/600000: episode: 1434, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287000/600000: episode: 1435, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287200/600000: episode: 1436, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287400/600000: episode: 1437, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287600/600000: episode: 1438, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287800/600000: episode: 1439, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288000/600000: episode: 1440, duration: 1.681s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288200/600000: episode: 1441, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288400/600000: episode: 1442, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288600/600000: episode: 1443, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288800/600000: episode: 1444, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289000/600000: episode: 1445, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289200/600000: episode: 1446, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289400/600000: episode: 1447, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289600/600000: episode: 1448, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289800/600000: episode: 1449, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290000/600000: episode: 1450, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290200/600000: episode: 1451, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290400/600000: episode: 1452, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290600/600000: episode: 1453, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290800/600000: episode: 1454, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291000/600000: episode: 1455, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291200/600000: episode: 1456, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291400/600000: episode: 1457, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291600/600000: episode: 1458, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291800/600000: episode: 1459, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292000/600000: episode: 1460, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292200/600000: episode: 1461, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292400/600000: episode: 1462, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292600/600000: episode: 1463, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292800/600000: episode: 1464, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293000/600000: episode: 1465, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293200/600000: episode: 1466, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293400/600000: episode: 1467, duration: 1.807s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293600/600000: episode: 1468, duration: 1.471s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293800/600000: episode: 1469, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294000/600000: episode: 1470, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294200/600000: episode: 1471, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294400/600000: episode: 1472, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294600/600000: episode: 1473, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294800/600000: episode: 1474, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295000/600000: episode: 1475, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295200/600000: episode: 1476, duration: 1.658s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295400/600000: episode: 1477, duration: 1.626s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295600/600000: episode: 1478, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295800/600000: episode: 1479, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296000/600000: episode: 1480, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296200/600000: episode: 1481, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296400/600000: episode: 1482, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296600/600000: episode: 1483, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296800/600000: episode: 1484, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297000/600000: episode: 1485, duration: 1.496s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297200/600000: episode: 1486, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297400/600000: episode: 1487, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297600/600000: episode: 1488, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297800/600000: episode: 1489, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298000/600000: episode: 1490, duration: 1.206s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298200/600000: episode: 1491, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298400/600000: episode: 1492, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298600/600000: episode: 1493, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298800/600000: episode: 1494, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299000/600000: episode: 1495, duration: 1.834s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299200/600000: episode: 1496, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299400/600000: episode: 1497, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299600/600000: episode: 1498, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299800/600000: episode: 1499, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300000/600000: episode: 1500, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300200/600000: episode: 1501, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300400/600000: episode: 1502, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300600/600000: episode: 1503, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300800/600000: episode: 1504, duration: 1.585s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301000/600000: episode: 1505, duration: 1.663s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301200/600000: episode: 1506, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301400/600000: episode: 1507, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301600/600000: episode: 1508, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301800/600000: episode: 1509, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302000/600000: episode: 1510, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302200/600000: episode: 1511, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302400/600000: episode: 1512, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302600/600000: episode: 1513, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302800/600000: episode: 1514, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303000/600000: episode: 1515, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303200/600000: episode: 1516, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303400/600000: episode: 1517, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303600/600000: episode: 1518, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303800/600000: episode: 1519, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304000/600000: episode: 1520, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304200/600000: episode: 1521, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304400/600000: episode: 1522, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304600/600000: episode: 1523, duration: 1.672s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304800/600000: episode: 1524, duration: 1.537s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305000/600000: episode: 1525, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305200/600000: episode: 1526, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305400/600000: episode: 1527, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305600/600000: episode: 1528, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305800/600000: episode: 1529, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306000/600000: episode: 1530, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306200/600000: episode: 1531, duration: 1.190s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306400/600000: episode: 1532, duration: 1.511s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306600/600000: episode: 1533, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306800/600000: episode: 1534, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307000/600000: episode: 1535, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307200/600000: episode: 1536, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307400/600000: episode: 1537, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307600/600000: episode: 1538, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307800/600000: episode: 1539, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308000/600000: episode: 1540, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308200/600000: episode: 1541, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308400/600000: episode: 1542, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308600/600000: episode: 1543, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308800/600000: episode: 1544, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309000/600000: episode: 1545, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309200/600000: episode: 1546, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309400/600000: episode: 1547, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309600/600000: episode: 1548, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309800/600000: episode: 1549, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310000/600000: episode: 1550, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310200/600000: episode: 1551, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310400/600000: episode: 1552, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310600/600000: episode: 1553, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310800/600000: episode: 1554, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311000/600000: episode: 1555, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311200/600000: episode: 1556, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311400/600000: episode: 1557, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311600/600000: episode: 1558, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311800/600000: episode: 1559, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312000/600000: episode: 1560, duration: 1.779s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312200/600000: episode: 1561, duration: 1.559s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312400/600000: episode: 1562, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312600/600000: episode: 1563, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312800/600000: episode: 1564, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313000/600000: episode: 1565, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313200/600000: episode: 1566, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313400/600000: episode: 1567, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313600/600000: episode: 1568, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313800/600000: episode: 1569, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314000/600000: episode: 1570, duration: 1.537s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314200/600000: episode: 1571, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314400/600000: episode: 1572, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314600/600000: episode: 1573, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314800/600000: episode: 1574, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315000/600000: episode: 1575, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315200/600000: episode: 1576, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315400/600000: episode: 1577, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315600/600000: episode: 1578, duration: 1.592s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315800/600000: episode: 1579, duration: 1.631s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316000/600000: episode: 1580, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316200/600000: episode: 1581, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316400/600000: episode: 1582, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316600/600000: episode: 1583, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316800/600000: episode: 1584, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317000/600000: episode: 1585, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317200/600000: episode: 1586, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317400/600000: episode: 1587, duration: 1.508s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317600/600000: episode: 1588, duration: 1.779s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317800/600000: episode: 1589, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318000/600000: episode: 1590, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318200/600000: episode: 1591, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318400/600000: episode: 1592, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318600/600000: episode: 1593, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318800/600000: episode: 1594, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319000/600000: episode: 1595, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319200/600000: episode: 1596, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319400/600000: episode: 1597, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319600/600000: episode: 1598, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319800/600000: episode: 1599, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320000/600000: episode: 1600, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320200/600000: episode: 1601, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320400/600000: episode: 1602, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320600/600000: episode: 1603, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320800/600000: episode: 1604, duration: 1.211s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321000/600000: episode: 1605, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321200/600000: episode: 1606, duration: 1.688s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321400/600000: episode: 1607, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321600/600000: episode: 1608, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321800/600000: episode: 1609, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322000/600000: episode: 1610, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322200/600000: episode: 1611, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322400/600000: episode: 1612, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322600/600000: episode: 1613, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322800/600000: episode: 1614, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323000/600000: episode: 1615, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323200/600000: episode: 1616, duration: 1.747s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323400/600000: episode: 1617, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323600/600000: episode: 1618, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323800/600000: episode: 1619, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324000/600000: episode: 1620, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324200/600000: episode: 1621, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324400/600000: episode: 1622, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324600/600000: episode: 1623, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324800/600000: episode: 1624, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325000/600000: episode: 1625, duration: 1.816s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325200/600000: episode: 1626, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325400/600000: episode: 1627, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325600/600000: episode: 1628, duration: 1.204s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325800/600000: episode: 1629, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326000/600000: episode: 1630, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326200/600000: episode: 1631, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326400/600000: episode: 1632, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326600/600000: episode: 1633, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326800/600000: episode: 1634, duration: 1.761s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327000/600000: episode: 1635, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327200/600000: episode: 1636, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327400/600000: episode: 1637, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327600/600000: episode: 1638, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327800/600000: episode: 1639, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328000/600000: episode: 1640, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328200/600000: episode: 1641, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328400/600000: episode: 1642, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328600/600000: episode: 1643, duration: 1.526s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328800/600000: episode: 1644, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329000/600000: episode: 1645, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329200/600000: episode: 1646, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329400/600000: episode: 1647, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329600/600000: episode: 1648, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329800/600000: episode: 1649, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330000/600000: episode: 1650, duration: 1.688s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330200/600000: episode: 1651, duration: 1.776s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330400/600000: episode: 1652, duration: 2.095s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330600/600000: episode: 1653, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330800/600000: episode: 1654, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331000/600000: episode: 1655, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331200/600000: episode: 1656, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331400/600000: episode: 1657, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331600/600000: episode: 1658, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331800/600000: episode: 1659, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332000/600000: episode: 1660, duration: 1.465s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332200/600000: episode: 1661, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332400/600000: episode: 1662, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332600/600000: episode: 1663, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332800/600000: episode: 1664, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333000/600000: episode: 1665, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333200/600000: episode: 1666, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333400/600000: episode: 1667, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333600/600000: episode: 1668, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333800/600000: episode: 1669, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334000/600000: episode: 1670, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334200/600000: episode: 1671, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334400/600000: episode: 1672, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334600/600000: episode: 1673, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334800/600000: episode: 1674, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335000/600000: episode: 1675, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335200/600000: episode: 1676, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335400/600000: episode: 1677, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335600/600000: episode: 1678, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335800/600000: episode: 1679, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336000/600000: episode: 1680, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336200/600000: episode: 1681, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336400/600000: episode: 1682, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336600/600000: episode: 1683, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336800/600000: episode: 1684, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337000/600000: episode: 1685, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337200/600000: episode: 1686, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337400/600000: episode: 1687, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337600/600000: episode: 1688, duration: 1.636s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337800/600000: episode: 1689, duration: 1.645s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338000/600000: episode: 1690, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338200/600000: episode: 1691, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338400/600000: episode: 1692, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338600/600000: episode: 1693, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338800/600000: episode: 1694, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339000/600000: episode: 1695, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339200/600000: episode: 1696, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339400/600000: episode: 1697, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339600/600000: episode: 1698, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339800/600000: episode: 1699, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340000/600000: episode: 1700, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340200/600000: episode: 1701, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340400/600000: episode: 1702, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340600/600000: episode: 1703, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340800/600000: episode: 1704, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341000/600000: episode: 1705, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341200/600000: episode: 1706, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341400/600000: episode: 1707, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341600/600000: episode: 1708, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341800/600000: episode: 1709, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342000/600000: episode: 1710, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342200/600000: episode: 1711, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342400/600000: episode: 1712, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342600/600000: episode: 1713, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342800/600000: episode: 1714, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343000/600000: episode: 1715, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343200/600000: episode: 1716, duration: 1.689s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343400/600000: episode: 1717, duration: 1.545s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343600/600000: episode: 1718, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343800/600000: episode: 1719, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344000/600000: episode: 1720, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344200/600000: episode: 1721, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344400/600000: episode: 1722, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344600/600000: episode: 1723, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344800/600000: episode: 1724, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345000/600000: episode: 1725, duration: 1.602s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345200/600000: episode: 1726, duration: 1.647s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345400/600000: episode: 1727, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345600/600000: episode: 1728, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345800/600000: episode: 1729, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346000/600000: episode: 1730, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346200/600000: episode: 1731, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346400/600000: episode: 1732, duration: 1.201s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346600/600000: episode: 1733, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346800/600000: episode: 1734, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347000/600000: episode: 1735, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347200/600000: episode: 1736, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347400/600000: episode: 1737, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347600/600000: episode: 1738, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347800/600000: episode: 1739, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348000/600000: episode: 1740, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348200/600000: episode: 1741, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348400/600000: episode: 1742, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348600/600000: episode: 1743, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348800/600000: episode: 1744, duration: 1.724s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349000/600000: episode: 1745, duration: 1.498s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349200/600000: episode: 1746, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349400/600000: episode: 1747, duration: 1.216s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349600/600000: episode: 1748, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349800/600000: episode: 1749, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350000/600000: episode: 1750, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350200/600000: episode: 1751, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350400/600000: episode: 1752, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350600/600000: episode: 1753, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350800/600000: episode: 1754, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351000/600000: episode: 1755, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351200/600000: episode: 1756, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351400/600000: episode: 1757, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351600/600000: episode: 1758, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351800/600000: episode: 1759, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352000/600000: episode: 1760, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352200/600000: episode: 1761, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352400/600000: episode: 1762, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352600/600000: episode: 1763, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352800/600000: episode: 1764, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353000/600000: episode: 1765, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353200/600000: episode: 1766, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353400/600000: episode: 1767, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353600/600000: episode: 1768, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353800/600000: episode: 1769, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354000/600000: episode: 1770, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354200/600000: episode: 1771, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354400/600000: episode: 1772, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354600/600000: episode: 1773, duration: 1.555s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354800/600000: episode: 1774, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355000/600000: episode: 1775, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355200/600000: episode: 1776, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355400/600000: episode: 1777, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355600/600000: episode: 1778, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355800/600000: episode: 1779, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356000/600000: episode: 1780, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356200/600000: episode: 1781, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356400/600000: episode: 1782, duration: 1.753s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356600/600000: episode: 1783, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356800/600000: episode: 1784, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357000/600000: episode: 1785, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357200/600000: episode: 1786, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357400/600000: episode: 1787, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357600/600000: episode: 1788, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357800/600000: episode: 1789, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358000/600000: episode: 1790, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358200/600000: episode: 1791, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358400/600000: episode: 1792, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358600/600000: episode: 1793, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358800/600000: episode: 1794, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359000/600000: episode: 1795, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359200/600000: episode: 1796, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359400/600000: episode: 1797, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359600/600000: episode: 1798, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359800/600000: episode: 1799, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360000/600000: episode: 1800, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360200/600000: episode: 1801, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360400/600000: episode: 1802, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360600/600000: episode: 1803, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360800/600000: episode: 1804, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361000/600000: episode: 1805, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361200/600000: episode: 1806, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361400/600000: episode: 1807, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361600/600000: episode: 1808, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361800/600000: episode: 1809, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362000/600000: episode: 1810, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362200/600000: episode: 1811, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362400/600000: episode: 1812, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362600/600000: episode: 1813, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362800/600000: episode: 1814, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363000/600000: episode: 1815, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363200/600000: episode: 1816, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363400/600000: episode: 1817, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363600/600000: episode: 1818, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363800/600000: episode: 1819, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364000/600000: episode: 1820, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364200/600000: episode: 1821, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364400/600000: episode: 1822, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364600/600000: episode: 1823, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364800/600000: episode: 1824, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365000/600000: episode: 1825, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365200/600000: episode: 1826, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365400/600000: episode: 1827, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365600/600000: episode: 1828, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365800/600000: episode: 1829, duration: 1.477s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366000/600000: episode: 1830, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366200/600000: episode: 1831, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366400/600000: episode: 1832, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366600/600000: episode: 1833, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366800/600000: episode: 1834, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367000/600000: episode: 1835, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367200/600000: episode: 1836, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367400/600000: episode: 1837, duration: 1.696s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367600/600000: episode: 1838, duration: 1.569s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367800/600000: episode: 1839, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368000/600000: episode: 1840, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368200/600000: episode: 1841, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368400/600000: episode: 1842, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368600/600000: episode: 1843, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368800/600000: episode: 1844, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369000/600000: episode: 1845, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369200/600000: episode: 1846, duration: 1.531s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369400/600000: episode: 1847, duration: 1.679s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369600/600000: episode: 1848, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369800/600000: episode: 1849, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370000/600000: episode: 1850, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370200/600000: episode: 1851, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370400/600000: episode: 1852, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370600/600000: episode: 1853, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370800/600000: episode: 1854, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371000/600000: episode: 1855, duration: 1.365s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371200/600000: episode: 1856, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371400/600000: episode: 1857, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371600/600000: episode: 1858, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371800/600000: episode: 1859, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372000/600000: episode: 1860, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372200/600000: episode: 1861, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372400/600000: episode: 1862, duration: 1.210s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372600/600000: episode: 1863, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372800/600000: episode: 1864, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373000/600000: episode: 1865, duration: 1.770s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373200/600000: episode: 1866, duration: 1.490s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373400/600000: episode: 1867, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373600/600000: episode: 1868, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373800/600000: episode: 1869, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374000/600000: episode: 1870, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374200/600000: episode: 1871, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374400/600000: episode: 1872, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374600/600000: episode: 1873, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374800/600000: episode: 1874, duration: 1.534s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375000/600000: episode: 1875, duration: 1.742s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375200/600000: episode: 1876, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375400/600000: episode: 1877, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375600/600000: episode: 1878, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375800/600000: episode: 1879, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376000/600000: episode: 1880, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376200/600000: episode: 1881, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376400/600000: episode: 1882, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376600/600000: episode: 1883, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376800/600000: episode: 1884, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377000/600000: episode: 1885, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377200/600000: episode: 1886, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.795 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377400/600000: episode: 1887, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377600/600000: episode: 1888, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377800/600000: episode: 1889, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378000/600000: episode: 1890, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378200/600000: episode: 1891, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378400/600000: episode: 1892, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378600/600000: episode: 1893, duration: 1.797s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378800/600000: episode: 1894, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379000/600000: episode: 1895, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379200/600000: episode: 1896, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379400/600000: episode: 1897, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379600/600000: episode: 1898, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379800/600000: episode: 1899, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380000/600000: episode: 1900, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380200/600000: episode: 1901, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380400/600000: episode: 1902, duration: 1.737s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380600/600000: episode: 1903, duration: 1.565s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380800/600000: episode: 1904, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381000/600000: episode: 1905, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381200/600000: episode: 1906, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381400/600000: episode: 1907, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381600/600000: episode: 1908, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381800/600000: episode: 1909, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382000/600000: episode: 1910, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382200/600000: episode: 1911, duration: 1.626s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382400/600000: episode: 1912, duration: 1.668s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382600/600000: episode: 1913, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382800/600000: episode: 1914, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383000/600000: episode: 1915, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383200/600000: episode: 1916, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383400/600000: episode: 1917, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383600/600000: episode: 1918, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383800/600000: episode: 1919, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384000/600000: episode: 1920, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384200/600000: episode: 1921, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384400/600000: episode: 1922, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384600/600000: episode: 1923, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384800/600000: episode: 1924, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385000/600000: episode: 1925, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385200/600000: episode: 1926, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385400/600000: episode: 1927, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385600/600000: episode: 1928, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385800/600000: episode: 1929, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386000/600000: episode: 1930, duration: 1.834s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386200/600000: episode: 1931, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386400/600000: episode: 1932, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386600/600000: episode: 1933, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386800/600000: episode: 1934, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387000/600000: episode: 1935, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387200/600000: episode: 1936, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387400/600000: episode: 1937, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387600/600000: episode: 1938, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387800/600000: episode: 1939, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388000/600000: episode: 1940, duration: 1.547s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388200/600000: episode: 1941, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388400/600000: episode: 1942, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388600/600000: episode: 1943, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388800/600000: episode: 1944, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389000/600000: episode: 1945, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389200/600000: episode: 1946, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389400/600000: episode: 1947, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389600/600000: episode: 1948, duration: 1.539s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389800/600000: episode: 1949, duration: 1.745s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390000/600000: episode: 1950, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390200/600000: episode: 1951, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390400/600000: episode: 1952, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390600/600000: episode: 1953, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390800/600000: episode: 1954, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391000/600000: episode: 1955, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391200/600000: episode: 1956, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391400/600000: episode: 1957, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391600/600000: episode: 1958, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391800/600000: episode: 1959, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392000/600000: episode: 1960, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392200/600000: episode: 1961, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392400/600000: episode: 1962, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392600/600000: episode: 1963, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392800/600000: episode: 1964, duration: 1.214s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393000/600000: episode: 1965, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393200/600000: episode: 1966, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393400/600000: episode: 1967, duration: 1.815s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393600/600000: episode: 1968, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393800/600000: episode: 1969, duration: 1.286s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394000/600000: episode: 1970, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394200/600000: episode: 1971, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394400/600000: episode: 1972, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394600/600000: episode: 1973, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394800/600000: episode: 1974, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395000/600000: episode: 1975, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395200/600000: episode: 1976, duration: 1.650s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395400/600000: episode: 1977, duration: 1.608s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395600/600000: episode: 1978, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395800/600000: episode: 1979, duration: 1.203s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396000/600000: episode: 1980, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396200/600000: episode: 1981, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396400/600000: episode: 1982, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396600/600000: episode: 1983, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396800/600000: episode: 1984, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397000/600000: episode: 1985, duration: 1.470s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397200/600000: episode: 1986, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397400/600000: episode: 1987, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397600/600000: episode: 1988, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397800/600000: episode: 1989, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398000/600000: episode: 1990, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398200/600000: episode: 1991, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398400/600000: episode: 1992, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398600/600000: episode: 1993, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398800/600000: episode: 1994, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399000/600000: episode: 1995, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399200/600000: episode: 1996, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399400/600000: episode: 1997, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399600/600000: episode: 1998, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399800/600000: episode: 1999, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400000/600000: episode: 2000, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400200/600000: episode: 2001, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400400/600000: episode: 2002, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400600/600000: episode: 2003, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400800/600000: episode: 2004, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401000/600000: episode: 2005, duration: 1.628s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401200/600000: episode: 2006, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401400/600000: episode: 2007, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401600/600000: episode: 2008, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401800/600000: episode: 2009, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402000/600000: episode: 2010, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402200/600000: episode: 2011, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402400/600000: episode: 2012, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402600/600000: episode: 2013, duration: 1.459s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402800/600000: episode: 2014, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403000/600000: episode: 2015, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403200/600000: episode: 2016, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403400/600000: episode: 2017, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403600/600000: episode: 2018, duration: 1.216s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403800/600000: episode: 2019, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404000/600000: episode: 2020, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404200/600000: episode: 2021, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404400/600000: episode: 2022, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404600/600000: episode: 2023, duration: 1.826s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404800/600000: episode: 2024, duration: 1.404s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405000/600000: episode: 2025, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405200/600000: episode: 2026, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405400/600000: episode: 2027, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405600/600000: episode: 2028, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405800/600000: episode: 2029, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406000/600000: episode: 2030, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406200/600000: episode: 2031, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406400/600000: episode: 2032, duration: 1.598s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406600/600000: episode: 2033, duration: 1.726s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406800/600000: episode: 2034, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407000/600000: episode: 2035, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407200/600000: episode: 2036, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407400/600000: episode: 2037, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407600/600000: episode: 2038, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407800/600000: episode: 2039, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408000/600000: episode: 2040, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408200/600000: episode: 2041, duration: 1.527s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408400/600000: episode: 2042, duration: 1.814s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408600/600000: episode: 2043, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408800/600000: episode: 2044, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409000/600000: episode: 2045, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409200/600000: episode: 2046, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409400/600000: episode: 2047, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409600/600000: episode: 2048, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409800/600000: episode: 2049, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410000/600000: episode: 2050, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410200/600000: episode: 2051, duration: 1.830s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410400/600000: episode: 2052, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410600/600000: episode: 2053, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410800/600000: episode: 2054, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411000/600000: episode: 2055, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411200/600000: episode: 2056, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411400/600000: episode: 2057, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411600/600000: episode: 2058, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411800/600000: episode: 2059, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412000/600000: episode: 2060, duration: 1.747s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412200/600000: episode: 2061, duration: 1.488s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412400/600000: episode: 2062, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412600/600000: episode: 2063, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412800/600000: episode: 2064, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413000/600000: episode: 2065, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413200/600000: episode: 2066, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413400/600000: episode: 2067, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413600/600000: episode: 2068, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413800/600000: episode: 2069, duration: 1.635s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414000/600000: episode: 2070, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414200/600000: episode: 2071, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414400/600000: episode: 2072, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414600/600000: episode: 2073, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414800/600000: episode: 2074, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415000/600000: episode: 2075, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415200/600000: episode: 2076, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415400/600000: episode: 2077, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415600/600000: episode: 2078, duration: 1.550s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415800/600000: episode: 2079, duration: 1.807s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416000/600000: episode: 2080, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416200/600000: episode: 2081, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416400/600000: episode: 2082, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416600/600000: episode: 2083, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416800/600000: episode: 2084, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417000/600000: episode: 2085, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417200/600000: episode: 2086, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417400/600000: episode: 2087, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417600/600000: episode: 2088, duration: 1.797s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417800/600000: episode: 2089, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418000/600000: episode: 2090, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418200/600000: episode: 2091, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418400/600000: episode: 2092, duration: 1.206s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418600/600000: episode: 2093, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418800/600000: episode: 2094, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419000/600000: episode: 2095, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419200/600000: episode: 2096, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419400/600000: episode: 2097, duration: 1.699s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419600/600000: episode: 2098, duration: 1.571s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419800/600000: episode: 2099, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420000/600000: episode: 2100, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420200/600000: episode: 2101, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420400/600000: episode: 2102, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420600/600000: episode: 2103, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420800/600000: episode: 2104, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421000/600000: episode: 2105, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421200/600000: episode: 2106, duration: 1.634s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421400/600000: episode: 2107, duration: 1.656s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421600/600000: episode: 2108, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421800/600000: episode: 2109, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422000/600000: episode: 2110, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422200/600000: episode: 2111, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422400/600000: episode: 2112, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422600/600000: episode: 2113, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422800/600000: episode: 2114, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423000/600000: episode: 2115, duration: 1.505s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423200/600000: episode: 2116, duration: 1.727s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423400/600000: episode: 2117, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423600/600000: episode: 2118, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423800/600000: episode: 2119, duration: 1.203s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424000/600000: episode: 2120, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424200/600000: episode: 2121, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424400/600000: episode: 2122, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424600/600000: episode: 2123, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424800/600000: episode: 2124, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425000/600000: episode: 2125, duration: 1.837s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425200/600000: episode: 2126, duration: 1.449s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425400/600000: episode: 2127, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425600/600000: episode: 2128, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425800/600000: episode: 2129, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426000/600000: episode: 2130, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426200/600000: episode: 2131, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426400/600000: episode: 2132, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426600/600000: episode: 2133, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426800/600000: episode: 2134, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427000/600000: episode: 2135, duration: 1.633s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427200/600000: episode: 2136, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427400/600000: episode: 2137, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427600/600000: episode: 2138, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427800/600000: episode: 2139, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428000/600000: episode: 2140, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428200/600000: episode: 2141, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428400/600000: episode: 2142, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428600/600000: episode: 2143, duration: 1.522s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428800/600000: episode: 2144, duration: 1.720s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429000/600000: episode: 2145, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429200/600000: episode: 2146, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429400/600000: episode: 2147, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429600/600000: episode: 2148, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429800/600000: episode: 2149, duration: 1.209s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430000/600000: episode: 2150, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430200/600000: episode: 2151, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430400/600000: episode: 2152, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430600/600000: episode: 2153, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430800/600000: episode: 2154, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431000/600000: episode: 2155, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431200/600000: episode: 2156, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431400/600000: episode: 2157, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431600/600000: episode: 2158, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431800/600000: episode: 2159, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432000/600000: episode: 2160, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432200/600000: episode: 2161, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432400/600000: episode: 2162, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432600/600000: episode: 2163, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432800/600000: episode: 2164, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433000/600000: episode: 2165, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433200/600000: episode: 2166, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433400/600000: episode: 2167, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433600/600000: episode: 2168, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433800/600000: episode: 2169, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434000/600000: episode: 2170, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434200/600000: episode: 2171, duration: 1.642s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434400/600000: episode: 2172, duration: 1.608s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434600/600000: episode: 2173, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434800/600000: episode: 2174, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435000/600000: episode: 2175, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435200/600000: episode: 2176, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435400/600000: episode: 2177, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435600/600000: episode: 2178, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435800/600000: episode: 2179, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436000/600000: episode: 2180, duration: 1.512s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436200/600000: episode: 2181, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436400/600000: episode: 2182, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436600/600000: episode: 2183, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436800/600000: episode: 2184, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437000/600000: episode: 2185, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437200/600000: episode: 2186, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437400/600000: episode: 2187, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437600/600000: episode: 2188, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437800/600000: episode: 2189, duration: 1.388s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438000/600000: episode: 2190, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438200/600000: episode: 2191, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438400/600000: episode: 2192, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438600/600000: episode: 2193, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438800/600000: episode: 2194, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439000/600000: episode: 2195, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439200/600000: episode: 2196, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439400/600000: episode: 2197, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439600/600000: episode: 2198, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439800/600000: episode: 2199, duration: 1.693s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440000/600000: episode: 2200, duration: 1.603s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440200/600000: episode: 2201, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440400/600000: episode: 2202, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440600/600000: episode: 2203, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440800/600000: episode: 2204, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441000/600000: episode: 2205, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441200/600000: episode: 2206, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441400/600000: episode: 2207, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441600/600000: episode: 2208, duration: 1.642s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441800/600000: episode: 2209, duration: 1.637s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442000/600000: episode: 2210, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442200/600000: episode: 2211, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442400/600000: episode: 2212, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442600/600000: episode: 2213, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442800/600000: episode: 2214, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443000/600000: episode: 2215, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443200/600000: episode: 2216, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443400/600000: episode: 2217, duration: 1.784s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443600/600000: episode: 2218, duration: 1.824s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443800/600000: episode: 2219, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444000/600000: episode: 2220, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444200/600000: episode: 2221, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444400/600000: episode: 2222, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444600/600000: episode: 2223, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444800/600000: episode: 2224, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445000/600000: episode: 2225, duration: 1.500s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445200/600000: episode: 2226, duration: 1.999s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445400/600000: episode: 2227, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445600/600000: episode: 2228, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445800/600000: episode: 2229, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446000/600000: episode: 2230, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446200/600000: episode: 2231, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446400/600000: episode: 2232, duration: 1.428s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446600/600000: episode: 2233, duration: 1.494s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446800/600000: episode: 2234, duration: 1.942s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447000/600000: episode: 2235, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447200/600000: episode: 2236, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447400/600000: episode: 2237, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447600/600000: episode: 2238, duration: 1.488s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447800/600000: episode: 2239, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448000/600000: episode: 2240, duration: 1.329s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448200/600000: episode: 2241, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448400/600000: episode: 2242, duration: 1.545s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448600/600000: episode: 2243, duration: 1.979s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448800/600000: episode: 2244, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449000/600000: episode: 2245, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449200/600000: episode: 2246, duration: 1.347s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449400/600000: episode: 2247, duration: 1.384s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449600/600000: episode: 2248, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449800/600000: episode: 2249, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450000/600000: episode: 2250, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450200/600000: episode: 2251, duration: 1.773s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450400/600000: episode: 2252, duration: 1.629s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450600/600000: episode: 2253, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450800/600000: episode: 2254, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451000/600000: episode: 2255, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451200/600000: episode: 2256, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451400/600000: episode: 2257, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451600/600000: episode: 2258, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451800/600000: episode: 2259, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452000/600000: episode: 2260, duration: 2.022s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452200/600000: episode: 2261, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452400/600000: episode: 2262, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452600/600000: episode: 2263, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452800/600000: episode: 2264, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453000/600000: episode: 2265, duration: 1.347s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453200/600000: episode: 2266, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453400/600000: episode: 2267, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453600/600000: episode: 2268, duration: 1.646s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453800/600000: episode: 2269, duration: 2.111s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454000/600000: episode: 2270, duration: 1.668s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454200/600000: episode: 2271, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454400/600000: episode: 2272, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454600/600000: episode: 2273, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454800/600000: episode: 2274, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455000/600000: episode: 2275, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455200/600000: episode: 2276, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455400/600000: episode: 2277, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455600/600000: episode: 2278, duration: 1.432s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455800/600000: episode: 2279, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456000/600000: episode: 2280, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456200/600000: episode: 2281, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456400/600000: episode: 2282, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456600/600000: episode: 2283, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456800/600000: episode: 2284, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457000/600000: episode: 2285, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457200/600000: episode: 2286, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457400/600000: episode: 2287, duration: 1.454s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457600/600000: episode: 2288, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457800/600000: episode: 2289, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458000/600000: episode: 2290, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458200/600000: episode: 2291, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458400/600000: episode: 2292, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458600/600000: episode: 2293, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458800/600000: episode: 2294, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459000/600000: episode: 2295, duration: 1.776s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459200/600000: episode: 2296, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459400/600000: episode: 2297, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459600/600000: episode: 2298, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459800/600000: episode: 2299, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460000/600000: episode: 2300, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460200/600000: episode: 2301, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460400/600000: episode: 2302, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460600/600000: episode: 2303, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460800/600000: episode: 2304, duration: 1.656s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461000/600000: episode: 2305, duration: 1.624s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461200/600000: episode: 2306, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461400/600000: episode: 2307, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461600/600000: episode: 2308, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461800/600000: episode: 2309, duration: 1.423s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462000/600000: episode: 2310, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462200/600000: episode: 2311, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462400/600000: episode: 2312, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462600/600000: episode: 2313, duration: 1.698s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462800/600000: episode: 2314, duration: 1.569s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463000/600000: episode: 2315, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463200/600000: episode: 2316, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463400/600000: episode: 2317, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463600/600000: episode: 2318, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463800/600000: episode: 2319, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464000/600000: episode: 2320, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464200/600000: episode: 2321, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464400/600000: episode: 2322, duration: 1.667s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464600/600000: episode: 2323, duration: 1.654s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464800/600000: episode: 2324, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465000/600000: episode: 2325, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465200/600000: episode: 2326, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465400/600000: episode: 2327, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465600/600000: episode: 2328, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465800/600000: episode: 2329, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466000/600000: episode: 2330, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466200/600000: episode: 2331, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466400/600000: episode: 2332, duration: 1.728s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466600/600000: episode: 2333, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466800/600000: episode: 2334, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467000/600000: episode: 2335, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467200/600000: episode: 2336, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467400/600000: episode: 2337, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467600/600000: episode: 2338, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467800/600000: episode: 2339, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468000/600000: episode: 2340, duration: 1.664s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468200/600000: episode: 2341, duration: 1.714s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468400/600000: episode: 2342, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468600/600000: episode: 2343, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468800/600000: episode: 2344, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469000/600000: episode: 2345, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469200/600000: episode: 2346, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469400/600000: episode: 2347, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469600/600000: episode: 2348, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469800/600000: episode: 2349, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470000/600000: episode: 2350, duration: 1.725s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470200/600000: episode: 2351, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470400/600000: episode: 2352, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470600/600000: episode: 2353, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470800/600000: episode: 2354, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471000/600000: episode: 2355, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471200/600000: episode: 2356, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471400/600000: episode: 2357, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471600/600000: episode: 2358, duration: 1.723s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471800/600000: episode: 2359, duration: 1.675s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472000/600000: episode: 2360, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472200/600000: episode: 2361, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472400/600000: episode: 2362, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472600/600000: episode: 2363, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472800/600000: episode: 2364, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473000/600000: episode: 2365, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473200/600000: episode: 2366, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473400/600000: episode: 2367, duration: 1.757s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473600/600000: episode: 2368, duration: 1.610s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473800/600000: episode: 2369, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474000/600000: episode: 2370, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474200/600000: episode: 2371, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474400/600000: episode: 2372, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474600/600000: episode: 2373, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474800/600000: episode: 2374, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475000/600000: episode: 2375, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475200/600000: episode: 2376, duration: 1.701s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475400/600000: episode: 2377, duration: 1.615s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475600/600000: episode: 2378, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475800/600000: episode: 2379, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476000/600000: episode: 2380, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476200/600000: episode: 2381, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476400/600000: episode: 2382, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476600/600000: episode: 2383, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476800/600000: episode: 2384, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477000/600000: episode: 2385, duration: 1.746s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477200/600000: episode: 2386, duration: 1.575s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477400/600000: episode: 2387, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477600/600000: episode: 2388, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477800/600000: episode: 2389, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478000/600000: episode: 2390, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478200/600000: episode: 2391, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478400/600000: episode: 2392, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478600/600000: episode: 2393, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478800/600000: episode: 2394, duration: 1.826s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479000/600000: episode: 2395, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479200/600000: episode: 2396, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479400/600000: episode: 2397, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479600/600000: episode: 2398, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479800/600000: episode: 2399, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480000/600000: episode: 2400, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480200/600000: episode: 2401, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480400/600000: episode: 2402, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480600/600000: episode: 2403, duration: 1.772s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480800/600000: episode: 2404, duration: 1.609s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481000/600000: episode: 2405, duration: 1.418s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481200/600000: episode: 2406, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481400/600000: episode: 2407, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481600/600000: episode: 2408, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481800/600000: episode: 2409, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482000/600000: episode: 2410, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482200/600000: episode: 2411, duration: 1.511s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482400/600000: episode: 2412, duration: 1.949s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482600/600000: episode: 2413, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482800/600000: episode: 2414, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483000/600000: episode: 2415, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483200/600000: episode: 2416, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483400/600000: episode: 2417, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483600/600000: episode: 2418, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483800/600000: episode: 2419, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484000/600000: episode: 2420, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484200/600000: episode: 2421, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484400/600000: episode: 2422, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484600/600000: episode: 2423, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484800/600000: episode: 2424, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485000/600000: episode: 2425, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485200/600000: episode: 2426, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485400/600000: episode: 2427, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485600/600000: episode: 2428, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485800/600000: episode: 2429, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486000/600000: episode: 2430, duration: 1.898s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486200/600000: episode: 2431, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486400/600000: episode: 2432, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486600/600000: episode: 2433, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486800/600000: episode: 2434, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487000/600000: episode: 2435, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487200/600000: episode: 2436, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487400/600000: episode: 2437, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487600/600000: episode: 2438, duration: 1.596s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487800/600000: episode: 2439, duration: 1.852s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488000/600000: episode: 2440, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488200/600000: episode: 2441, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488400/600000: episode: 2442, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488600/600000: episode: 2443, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488800/600000: episode: 2444, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489000/600000: episode: 2445, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489200/600000: episode: 2446, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489400/600000: episode: 2447, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489600/600000: episode: 2448, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489800/600000: episode: 2449, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490000/600000: episode: 2450, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490200/600000: episode: 2451, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490400/600000: episode: 2452, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490600/600000: episode: 2453, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490800/600000: episode: 2454, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491000/600000: episode: 2455, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491200/600000: episode: 2456, duration: 1.664s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491400/600000: episode: 2457, duration: 1.674s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491600/600000: episode: 2458, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491800/600000: episode: 2459, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492000/600000: episode: 2460, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492200/600000: episode: 2461, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492400/600000: episode: 2462, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492600/600000: episode: 2463, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492800/600000: episode: 2464, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493000/600000: episode: 2465, duration: 1.728s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493200/600000: episode: 2466, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493400/600000: episode: 2467, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493600/600000: episode: 2468, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493800/600000: episode: 2469, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494000/600000: episode: 2470, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494200/600000: episode: 2471, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494400/600000: episode: 2472, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494600/600000: episode: 2473, duration: 1.278s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494800/600000: episode: 2474, duration: 1.716s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495000/600000: episode: 2475, duration: 1.607s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495200/600000: episode: 2476, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495400/600000: episode: 2477, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495600/600000: episode: 2478, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495800/600000: episode: 2479, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496000/600000: episode: 2480, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496200/600000: episode: 2481, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496400/600000: episode: 2482, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496600/600000: episode: 2483, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496800/600000: episode: 2484, duration: 1.645s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497000/600000: episode: 2485, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497200/600000: episode: 2486, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497400/600000: episode: 2487, duration: 1.384s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497600/600000: episode: 2488, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497800/600000: episode: 2489, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498000/600000: episode: 2490, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498200/600000: episode: 2491, duration: 1.553s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498400/600000: episode: 2492, duration: 1.976s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498600/600000: episode: 2493, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498800/600000: episode: 2494, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499000/600000: episode: 2495, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499200/600000: episode: 2496, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499400/600000: episode: 2497, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499600/600000: episode: 2498, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499800/600000: episode: 2499, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500000/600000: episode: 2500, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500200/600000: episode: 2501, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500400/600000: episode: 2502, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500600/600000: episode: 2503, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500800/600000: episode: 2504, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501000/600000: episode: 2505, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501200/600000: episode: 2506, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501400/600000: episode: 2507, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501600/600000: episode: 2508, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501800/600000: episode: 2509, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502000/600000: episode: 2510, duration: 1.804s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502200/600000: episode: 2511, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502400/600000: episode: 2512, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502600/600000: episode: 2513, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502800/600000: episode: 2514, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503000/600000: episode: 2515, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503200/600000: episode: 2516, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503400/600000: episode: 2517, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503600/600000: episode: 2518, duration: 1.588s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503800/600000: episode: 2519, duration: 1.811s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504000/600000: episode: 2520, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504200/600000: episode: 2521, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504400/600000: episode: 2522, duration: 1.278s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504600/600000: episode: 2523, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504800/600000: episode: 2524, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505000/600000: episode: 2525, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505200/600000: episode: 2526, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505400/600000: episode: 2527, duration: 1.553s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505600/600000: episode: 2528, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505800/600000: episode: 2529, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506000/600000: episode: 2530, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506200/600000: episode: 2531, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506400/600000: episode: 2532, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506600/600000: episode: 2533, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506800/600000: episode: 2534, duration: 1.427s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507000/600000: episode: 2535, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507200/600000: episode: 2536, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507400/600000: episode: 2537, duration: 1.467s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507600/600000: episode: 2538, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507800/600000: episode: 2539, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508000/600000: episode: 2540, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508200/600000: episode: 2541, duration: 1.329s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508400/600000: episode: 2542, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508600/600000: episode: 2543, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508800/600000: episode: 2544, duration: 1.398s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509000/600000: episode: 2545, duration: 2.044s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509200/600000: episode: 2546, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509400/600000: episode: 2547, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509600/600000: episode: 2548, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509800/600000: episode: 2549, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510000/600000: episode: 2550, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510200/600000: episode: 2551, duration: 1.536s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510400/600000: episode: 2552, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510600/600000: episode: 2553, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510800/600000: episode: 2554, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511000/600000: episode: 2555, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511200/600000: episode: 2556, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511400/600000: episode: 2557, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511600/600000: episode: 2558, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511800/600000: episode: 2559, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512000/600000: episode: 2560, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512200/600000: episode: 2561, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512400/600000: episode: 2562, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512600/600000: episode: 2563, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512800/600000: episode: 2564, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513000/600000: episode: 2565, duration: 1.396s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513200/600000: episode: 2566, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513400/600000: episode: 2567, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513600/600000: episode: 2568, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513800/600000: episode: 2569, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514000/600000: episode: 2570, duration: 1.470s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514200/600000: episode: 2571, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514400/600000: episode: 2572, duration: 1.383s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514600/600000: episode: 2573, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514800/600000: episode: 2574, duration: 1.365s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515000/600000: episode: 2575, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515200/600000: episode: 2576, duration: 1.450s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515400/600000: episode: 2577, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515600/600000: episode: 2578, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515800/600000: episode: 2579, duration: 1.641s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516000/600000: episode: 2580, duration: 1.717s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516200/600000: episode: 2581, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516400/600000: episode: 2582, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516600/600000: episode: 2583, duration: 1.278s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516800/600000: episode: 2584, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517000/600000: episode: 2585, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517200/600000: episode: 2586, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517400/600000: episode: 2587, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517600/600000: episode: 2588, duration: 1.636s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517800/600000: episode: 2589, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518000/600000: episode: 2590, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518200/600000: episode: 2591, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518400/600000: episode: 2592, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518600/600000: episode: 2593, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518800/600000: episode: 2594, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519000/600000: episode: 2595, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519200/600000: episode: 2596, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519400/600000: episode: 2597, duration: 1.602s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519600/600000: episode: 2598, duration: 1.778s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519800/600000: episode: 2599, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520000/600000: episode: 2600, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520200/600000: episode: 2601, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520400/600000: episode: 2602, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520600/600000: episode: 2603, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520800/600000: episode: 2604, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521000/600000: episode: 2605, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521200/600000: episode: 2606, duration: 1.692s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521400/600000: episode: 2607, duration: 1.679s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521600/600000: episode: 2608, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521800/600000: episode: 2609, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522000/600000: episode: 2610, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522200/600000: episode: 2611, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522400/600000: episode: 2612, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522600/600000: episode: 2613, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522800/600000: episode: 2614, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523000/600000: episode: 2615, duration: 1.540s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523200/600000: episode: 2616, duration: 1.779s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523400/600000: episode: 2617, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523600/600000: episode: 2618, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523800/600000: episode: 2619, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524000/600000: episode: 2620, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524200/600000: episode: 2621, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524400/600000: episode: 2622, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524600/600000: episode: 2623, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524800/600000: episode: 2624, duration: 1.606s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525000/600000: episode: 2625, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525200/600000: episode: 2626, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525400/600000: episode: 2627, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525600/600000: episode: 2628, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525800/600000: episode: 2629, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526000/600000: episode: 2630, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526200/600000: episode: 2631, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526400/600000: episode: 2632, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526600/600000: episode: 2633, duration: 1.502s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526800/600000: episode: 2634, duration: 1.820s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527000/600000: episode: 2635, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527200/600000: episode: 2636, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527400/600000: episode: 2637, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527600/600000: episode: 2638, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527800/600000: episode: 2639, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528000/600000: episode: 2640, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528200/600000: episode: 2641, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528400/600000: episode: 2642, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528600/600000: episode: 2643, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528800/600000: episode: 2644, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529000/600000: episode: 2645, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529200/600000: episode: 2646, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529400/600000: episode: 2647, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529600/600000: episode: 2648, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529800/600000: episode: 2649, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530000/600000: episode: 2650, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530200/600000: episode: 2651, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530400/600000: episode: 2652, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530600/600000: episode: 2653, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530800/600000: episode: 2654, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531000/600000: episode: 2655, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531200/600000: episode: 2656, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531400/600000: episode: 2657, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531600/600000: episode: 2658, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531800/600000: episode: 2659, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532000/600000: episode: 2660, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532200/600000: episode: 2661, duration: 1.742s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532400/600000: episode: 2662, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532600/600000: episode: 2663, duration: 1.475s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532800/600000: episode: 2664, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533000/600000: episode: 2665, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533200/600000: episode: 2666, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533400/600000: episode: 2667, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533600/600000: episode: 2668, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533800/600000: episode: 2669, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534000/600000: episode: 2670, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534200/600000: episode: 2671, duration: 1.511s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534400/600000: episode: 2672, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534600/600000: episode: 2673, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534800/600000: episode: 2674, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535000/600000: episode: 2675, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535200/600000: episode: 2676, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535400/600000: episode: 2677, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535600/600000: episode: 2678, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535800/600000: episode: 2679, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536000/600000: episode: 2680, duration: 1.613s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536200/600000: episode: 2681, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536400/600000: episode: 2682, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536600/600000: episode: 2683, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536800/600000: episode: 2684, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537000/600000: episode: 2685, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537200/600000: episode: 2686, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537400/600000: episode: 2687, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537600/600000: episode: 2688, duration: 1.581s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537800/600000: episode: 2689, duration: 1.713s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538000/600000: episode: 2690, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538200/600000: episode: 2691, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538400/600000: episode: 2692, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538600/600000: episode: 2693, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538800/600000: episode: 2694, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539000/600000: episode: 2695, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539200/600000: episode: 2696, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539400/600000: episode: 2697, duration: 1.496s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539600/600000: episode: 2698, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539800/600000: episode: 2699, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540000/600000: episode: 2700, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540200/600000: episode: 2701, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540400/600000: episode: 2702, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540600/600000: episode: 2703, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540800/600000: episode: 2704, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541000/600000: episode: 2705, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541200/600000: episode: 2706, duration: 1.430s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541400/600000: episode: 2707, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541600/600000: episode: 2708, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541800/600000: episode: 2709, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542000/600000: episode: 2710, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542200/600000: episode: 2711, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542400/600000: episode: 2712, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542600/600000: episode: 2713, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542800/600000: episode: 2714, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543000/600000: episode: 2715, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543200/600000: episode: 2716, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543400/600000: episode: 2717, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543600/600000: episode: 2718, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543800/600000: episode: 2719, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544000/600000: episode: 2720, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544200/600000: episode: 2721, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544400/600000: episode: 2722, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544600/600000: episode: 2723, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544800/600000: episode: 2724, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545000/600000: episode: 2725, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545200/600000: episode: 2726, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545400/600000: episode: 2727, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545600/600000: episode: 2728, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545800/600000: episode: 2729, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546000/600000: episode: 2730, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546200/600000: episode: 2731, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546400/600000: episode: 2732, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546600/600000: episode: 2733, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546800/600000: episode: 2734, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547000/600000: episode: 2735, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547200/600000: episode: 2736, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547400/600000: episode: 2737, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547600/600000: episode: 2738, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547800/600000: episode: 2739, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548000/600000: episode: 2740, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548200/600000: episode: 2741, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548400/600000: episode: 2742, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548600/600000: episode: 2743, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548800/600000: episode: 2744, duration: 1.567s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549000/600000: episode: 2745, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549200/600000: episode: 2746, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549400/600000: episode: 2747, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549600/600000: episode: 2748, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549800/600000: episode: 2749, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550000/600000: episode: 2750, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550200/600000: episode: 2751, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550400/600000: episode: 2752, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550600/600000: episode: 2753, duration: 1.616s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550800/600000: episode: 2754, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551000/600000: episode: 2755, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551200/600000: episode: 2756, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551400/600000: episode: 2757, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551600/600000: episode: 2758, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551800/600000: episode: 2759, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552000/600000: episode: 2760, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552200/600000: episode: 2761, duration: 1.638s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552400/600000: episode: 2762, duration: 1.662s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552600/600000: episode: 2763, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552800/600000: episode: 2764, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553000/600000: episode: 2765, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553200/600000: episode: 2766, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553400/600000: episode: 2767, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553600/600000: episode: 2768, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553800/600000: episode: 2769, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554000/600000: episode: 2770, duration: 1.699s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554200/600000: episode: 2771, duration: 1.633s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554400/600000: episode: 2772, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554600/600000: episode: 2773, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554800/600000: episode: 2774, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555000/600000: episode: 2775, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555200/600000: episode: 2776, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555400/600000: episode: 2777, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555600/600000: episode: 2778, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555800/600000: episode: 2779, duration: 1.619s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556000/600000: episode: 2780, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556200/600000: episode: 2781, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556400/600000: episode: 2782, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556600/600000: episode: 2783, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556800/600000: episode: 2784, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557000/600000: episode: 2785, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557200/600000: episode: 2786, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557400/600000: episode: 2787, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557600/600000: episode: 2788, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557800/600000: episode: 2789, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558000/600000: episode: 2790, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558200/600000: episode: 2791, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558400/600000: episode: 2792, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558600/600000: episode: 2793, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558800/600000: episode: 2794, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559000/600000: episode: 2795, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559200/600000: episode: 2796, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559400/600000: episode: 2797, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559600/600000: episode: 2798, duration: 1.852s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559800/600000: episode: 2799, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560000/600000: episode: 2800, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560200/600000: episode: 2801, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560400/600000: episode: 2802, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560600/600000: episode: 2803, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560800/600000: episode: 2804, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561000/600000: episode: 2805, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561200/600000: episode: 2806, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561400/600000: episode: 2807, duration: 1.705s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561600/600000: episode: 2808, duration: 1.566s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561800/600000: episode: 2809, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562000/600000: episode: 2810, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562200/600000: episode: 2811, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562400/600000: episode: 2812, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562600/600000: episode: 2813, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562800/600000: episode: 2814, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563000/600000: episode: 2815, duration: 1.216s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563200/600000: episode: 2816, duration: 1.530s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563400/600000: episode: 2817, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563600/600000: episode: 2818, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563800/600000: episode: 2819, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564000/600000: episode: 2820, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564200/600000: episode: 2821, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564400/600000: episode: 2822, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564600/600000: episode: 2823, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564800/600000: episode: 2824, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565000/600000: episode: 2825, duration: 1.487s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565200/600000: episode: 2826, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565400/600000: episode: 2827, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565600/600000: episode: 2828, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565800/600000: episode: 2829, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566000/600000: episode: 2830, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566200/600000: episode: 2831, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566400/600000: episode: 2832, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566600/600000: episode: 2833, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566800/600000: episode: 2834, duration: 1.401s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567000/600000: episode: 2835, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567200/600000: episode: 2836, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567400/600000: episode: 2837, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567600/600000: episode: 2838, duration: 1.214s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567800/600000: episode: 2839, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568000/600000: episode: 2840, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568200/600000: episode: 2841, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568400/600000: episode: 2842, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568600/600000: episode: 2843, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568800/600000: episode: 2844, duration: 1.730s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569000/600000: episode: 2845, duration: 1.495s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569200/600000: episode: 2846, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569400/600000: episode: 2847, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569600/600000: episode: 2848, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569800/600000: episode: 2849, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570000/600000: episode: 2850, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570200/600000: episode: 2851, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570400/600000: episode: 2852, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570600/600000: episode: 2853, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570800/600000: episode: 2854, duration: 1.630s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571000/600000: episode: 2855, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571200/600000: episode: 2856, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571400/600000: episode: 2857, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571600/600000: episode: 2858, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571800/600000: episode: 2859, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572000/600000: episode: 2860, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572200/600000: episode: 2861, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572400/600000: episode: 2862, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572600/600000: episode: 2863, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572800/600000: episode: 2864, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573000/600000: episode: 2865, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573200/600000: episode: 2866, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573400/600000: episode: 2867, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573600/600000: episode: 2868, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573800/600000: episode: 2869, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574000/600000: episode: 2870, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574200/600000: episode: 2871, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574400/600000: episode: 2872, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574600/600000: episode: 2873, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574800/600000: episode: 2874, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575000/600000: episode: 2875, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575200/600000: episode: 2876, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575400/600000: episode: 2877, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575600/600000: episode: 2878, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575800/600000: episode: 2879, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576000/600000: episode: 2880, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576200/600000: episode: 2881, duration: 1.663s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576400/600000: episode: 2882, duration: 1.571s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576600/600000: episode: 2883, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576800/600000: episode: 2884, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577000/600000: episode: 2885, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577200/600000: episode: 2886, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577400/600000: episode: 2887, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577600/600000: episode: 2888, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577800/600000: episode: 2889, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578000/600000: episode: 2890, duration: 1.561s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578200/600000: episode: 2891, duration: 1.669s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578400/600000: episode: 2892, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578600/600000: episode: 2893, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578800/600000: episode: 2894, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579000/600000: episode: 2895, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579200/600000: episode: 2896, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579400/600000: episode: 2897, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579600/600000: episode: 2898, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579800/600000: episode: 2899, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580000/600000: episode: 2900, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580200/600000: episode: 2901, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580400/600000: episode: 2902, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580600/600000: episode: 2903, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580800/600000: episode: 2904, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581000/600000: episode: 2905, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581200/600000: episode: 2906, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581400/600000: episode: 2907, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581600/600000: episode: 2908, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581800/600000: episode: 2909, duration: 1.693s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582000/600000: episode: 2910, duration: 1.608s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582200/600000: episode: 2911, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582400/600000: episode: 2912, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582600/600000: episode: 2913, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582800/600000: episode: 2914, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583000/600000: episode: 2915, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583200/600000: episode: 2916, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583400/600000: episode: 2917, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583600/600000: episode: 2918, duration: 1.445s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583800/600000: episode: 2919, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584000/600000: episode: 2920, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584200/600000: episode: 2921, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584400/600000: episode: 2922, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584600/600000: episode: 2923, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584800/600000: episode: 2924, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585000/600000: episode: 2925, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585200/600000: episode: 2926, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585400/600000: episode: 2927, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585600/600000: episode: 2928, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585800/600000: episode: 2929, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586000/600000: episode: 2930, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586200/600000: episode: 2931, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586400/600000: episode: 2932, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586600/600000: episode: 2933, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586800/600000: episode: 2934, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587000/600000: episode: 2935, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587200/600000: episode: 2936, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587400/600000: episode: 2937, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587600/600000: episode: 2938, duration: 1.465s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587800/600000: episode: 2939, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588000/600000: episode: 2940, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588200/600000: episode: 2941, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588400/600000: episode: 2942, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588600/600000: episode: 2943, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588800/600000: episode: 2944, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589000/600000: episode: 2945, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589200/600000: episode: 2946, duration: 1.682s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589400/600000: episode: 2947, duration: 1.681s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589600/600000: episode: 2948, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589800/600000: episode: 2949, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590000/600000: episode: 2950, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590200/600000: episode: 2951, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590400/600000: episode: 2952, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590600/600000: episode: 2953, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590800/600000: episode: 2954, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591000/600000: episode: 2955, duration: 1.689s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591200/600000: episode: 2956, duration: 1.694s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591400/600000: episode: 2957, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591600/600000: episode: 2958, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591800/600000: episode: 2959, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592000/600000: episode: 2960, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592200/600000: episode: 2961, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592400/600000: episode: 2962, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592600/600000: episode: 2963, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592800/600000: episode: 2964, duration: 1.523s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593000/600000: episode: 2965, duration: 1.746s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593200/600000: episode: 2966, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593400/600000: episode: 2967, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593600/600000: episode: 2968, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593800/600000: episode: 2969, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594000/600000: episode: 2970, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594200/600000: episode: 2971, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594400/600000: episode: 2972, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594600/600000: episode: 2973, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594800/600000: episode: 2974, duration: 2.440s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595000/600000: episode: 2975, duration: 1.743s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595200/600000: episode: 2976, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595400/600000: episode: 2977, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595600/600000: episode: 2978, duration: 1.384s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595800/600000: episode: 2979, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596000/600000: episode: 2980, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596200/600000: episode: 2981, duration: 1.394s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596400/600000: episode: 2982, duration: 1.939s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596600/600000: episode: 2983, duration: 1.402s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596800/600000: episode: 2984, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597000/600000: episode: 2985, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597200/600000: episode: 2986, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597400/600000: episode: 2987, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597600/600000: episode: 2988, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597800/600000: episode: 2989, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598000/600000: episode: 2990, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598200/600000: episode: 2991, duration: 1.816s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598400/600000: episode: 2992, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598600/600000: episode: 2993, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598800/600000: episode: 2994, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599000/600000: episode: 2995, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599200/600000: episode: 2996, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599400/600000: episode: 2997, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599600/600000: episode: 2998, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599800/600000: episode: 2999, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 600000/600000: episode: 3000, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "done, took 4054.172 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################ Validación ###############################\n",
        "sarsa_solucion_1.load_weights('model_sarsa_solucion_1_valoresfinales.h5f') #Carga los pesos del entrenamiento (buena practica)\n",
        "env.reset() #reinicia el ambiente\n",
        "test_episodes = 5  # Número de episodios de validación\n",
        "sarsa_solucion_1.test(env, nb_episodes=test_episodes, visualize=True) #Simula con la política obtenida\n",
        "\n",
        "observation, info = env_test_render.reset() #inicializa el ambiente\n",
        "\n",
        "while True: #simula\n",
        "  action = sarsa_solucion_1.forward(observation) #elige una acción de acuerdo a sarsa 4 (se usan los resultados de la primera red)\n",
        "  observation, reward, terminated, truncated, info = env_test_render.step(action) #obtiene la observación tras tomar la acción\n",
        "  if terminated or truncated: #si pierde o pasan 200 pasos\n",
        "      break #termina\n",
        "\n",
        "env_test_render.play() #muestra el video\n",
        "\n",
        "############################ Validación ###############################\n",
        "sarsa_solucion_2.load_weights('model_sarsa_solucion_2_valoresfinales.h5f') #Carga los pesos del entrenamiento (buena practica)\n",
        "env.reset() #reinicia el ambiente\n",
        "test_episodes = 5  # Número de episodios de validación\n",
        "sarsa_solucion_2.test(env, nb_episodes=test_episodes, visualize=True) #Simula con la política obtenida\n",
        "\n",
        "observation, info = env_test_render.reset() #inicializa el ambiente\n",
        "\n",
        "while True: #simula\n",
        "  action = sarsa_solucion_2.forward(observation) #elige una acción de acuerdo a sarsa 4 (se usan los resultados de la primera red)\n",
        "  observation, reward, terminated, truncated, info = env_test_render.step(action) #obtiene la observación tras tomar la acción\n",
        "  if terminated or truncated: #si pierde o pasan 200 pasos\n",
        "      break #termina\n",
        "\n",
        "env_test_render.play() #muestra el video\n",
        "\n",
        "\n",
        "############################ Validación ###############################\n",
        "sarsa_solucion_3.load_weights('model_sarsa_solucion_3_valoresfinales.h5f') #Carga los pesos del entrenamiento (buena practica)\n",
        "env.reset() #reinicia el ambiente\n",
        "test_episodes = 5  # Número de episodios de validación\n",
        "sarsa_solucion_3.test(env, nb_episodes=test_episodes, visualize=True) #Simula con la política obtenida\n",
        "\n",
        "observation, info = env_test_render.reset() #inicializa el ambiente\n",
        "\n",
        "while True: #simula\n",
        "  action = sarsa_solucion_3.forward(observation) #elige una acción de acuerdo a sarsa 4 (se usan los resultados de la primera red)\n",
        "  observation, reward, terminated, truncated, info = env_test_render.step(action) #obtiene la observación tras tomar la acción\n",
        "  if terminated or truncated: #si pierde o pasan 200 pasos\n",
        "      break #termina\n",
        "\n",
        "env_test_render.play() #muestra el video"
      ],
      "metadata": {
        "id": "TLS6LxtRztbh",
        "outputId": "d08ec937-ed28-4314-afb6-fc06f5e8227d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -200.000, steps: 200\n",
            "Episode 2: reward: -200.000, steps: 200\n",
            "Episode 3: reward: -200.000, steps: 200\n",
            "Episode 4: reward: -200.000, steps: 200\n",
            "Episode 5: reward: -200.000, steps: 200\n",
            "Moviepy - Building video temp-{start}.mp4.\n",
            "Moviepy - Writing video temp-{start}.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready temp-{start}.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls  >\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -200.000, steps: 200\n",
            "Episode 2: reward: -200.000, steps: 200\n",
            "Episode 3: reward: -200.000, steps: 200\n",
            "Episode 4: reward: -200.000, steps: 200\n",
            "Episode 5: reward: -200.000, steps: 200\n",
            "Moviepy - Building video temp-{start}.mp4.\n",
            "Moviepy - Writing video temp-{start}.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready temp-{start}.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls  >\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -200.000, steps: 200\n",
            "Episode 2: reward: -200.000, steps: 200\n",
            "Episode 3: reward: -200.000, steps: 200\n",
            "Episode 4: reward: -200.000, steps: 200\n",
            "Episode 5: reward: -200.000, steps: 200\n",
            "Moviepy - Building video temp-{start}.mp4.\n",
            "Moviepy - Writing video temp-{start}.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready temp-{start}.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls  >\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3LFlj-fz0SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflexiones Finales\n",
        "\n"
      ],
      "metadata": {
        "id": "G5ZVj1xuPWoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias\n",
        "\n",
        "[1] Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. The MIT Press, second edition.\n",
        "\n",
        "[2]\n",
        "\n",
        "[3] Gym Documentation, Mountain Car. `https://gymnasium.farama.org/environments/classic_control/mountain_car/`\n",
        "\n",
        "\n",
        "[4] keras-rl2 Documentation, SARSA. `https://github.com/inarikami/keras-rl2/blob/master/rl/agents/sarsa.py`"
      ],
      "metadata": {
        "id": "HhVGfkzU6KRH"
      }
    }
  ]
}