{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmmOHLMWnqWUnBnrpsPdRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/blob/main/TareaSemana2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![MAIA banner](https://raw.githubusercontent.com/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/main/Images/Aprendizaje_refuerzo_profundo_Banner_V1.png)\n",
        "\n",
        "# <h1><center>Tarea Tutorial - Semana 2 <a href=\"https://colab.research.google.com/github/SSolanoRuniandes/Notebooks-Aprendizaje-por-Refuerzo-Profundo/blob/main/TareaSemana2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"140\" align=\"center\"/></a></center></h1>\n",
        "\n",
        "<center><h1>Predicción y Control On-Policy</h1></center>\n",
        "\n",
        "En este notebook tutorial vas a aprender sobre algunas técnicas <i>on-policy</i> que existen para aproximar la función de valor en un problema de aprendizaje por refuerzo y algoritmos de aprendizaje on-policy. Mostraremos el funcionamiento de estas técnica sy algoritmos utilizando el ambiente de <a href=\"https://gymnasium.farama.org/environments/classic_control/mountain_car/\">Mountain Car</a>, incluido en las librerías de Gym, e implementaremos redes nuronales con el framework <a href=\"https://github.com/inarikami/keras-rl2\">keras-rl2</a>, que incluye nuestros algoritmos de interés. La misión central de este notebook es evidenciar de forma visual cómo se puede representar matemáticamente un problema y cómo se estructura una solución cuando se utilizan técnicas <i>on-policy</i>. El tutorial se divide en:\n",
        "\n",
        "\n",
        "# Tabla de Contenidos\n",
        "1. [Objetivos de Aprendizaje](#scrollTo=Objetivos_de_Aprendizaje)  \n",
        "2. [Marco Teórico](#scrollTo=Marco_Te_rico)  \n",
        "3. [Instalación de Librerías](#scrollTo=Instalaci_n_de_Librer_as)  \n",
        "4. [Familiarización con el Entorno de Gym](#scrollTo=Familiarizaci_n_con_el_Entorno_de_Gym)  \n",
        "5. [Predicción on-policy](#scrollTo=Predicci_n_on_policy)  \n",
        "6. [Control on-policy](#scrollTo=Control_on_policy)  \n",
        "7. [Reflexiones Finales](#scrollTo=Reflexiones_Finales)  \n",
        "8. [Referencias](#scrollTo=Referencias)"
      ],
      "metadata": {
        "id": "oblzmhF6SCZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objetivos de Aprendizaje  \n",
        "  \n",
        "* Conocer algunas formas matemáticas que se utilizan para representar problemas complejos de aprendizaje por refuerzo.\n",
        "* Familiarizarse con los entornos de simulación de Gym.\n",
        "* Entrenar y validar algoritmos de aprendizaje por refuerzo <i>on-policy</i> con redes neuronales (Deep SARSA).\n",
        "\n"
      ],
      "metadata": {
        "id": "k8OPdsC0AxgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Marco Teórico  \n",
        "\n",
        "_-_\n",
        "\n",
        "_-_\n",
        "\n",
        "_-_\n",
        "\n",
        "_-_\n",
        "\n",
        "\n",
        "Ahora, es necesario redirigir estos conceptos detrás de la aproximación de una función de valor al problema de control de política, es decir, al aprendizaje. Primero, recordemos que en el proceso de control de política un algoritmo busca construir una aproximación de la función de valor de acción $\\hat{q}(s,a,\\mathbf{w}) \\approx q_*(s,a)$, donde $\\mathbf{w}$ es un vector de tamaño finito con los pesos que definen la aproximación que se utilice. [1]\n",
        "\n",
        "Para el caso de tareas episódicas, la extensión de los métodos de predicción de semi-gradiente a las funciones de valor de acciones es directa: $\\hat{q}(s,a,\\mathbf{w}) \\approx q_{\\pi}(s,a)$. Después, si se acoplan esas predicciones con técnicas de mejoramiento de política y selección de acciones, obtenemos el control de política. En tareas con un espacio de acciones discreto y relativamente pequeño, para cada posible acción $a$ posible en el siguiente estado $S_{t+1}$, se puede calcular $\\hat{q}(S_{t+1},a,\\mathbf{w}_t) \\approx q_{\\pi}(s,a)$ y encontrar la acción <i>greedy</i> ($A^*_{t+1} = \\text{arg max}_a\n",
        " \\hat{q}(s,a,\\mathbf{w}))$. Para <i>algoritmos on-policy</i>, la política se mejora cambiando la política de estimación a una aproximación <i>soft</i> (que puede escoger cualquier acción) de la política <i>greedy</i> (que únicamente escoge la mejor acción). [1]\n",
        "\n",
        " La siguiente es la regla de actualización de SARSA de un paso, y el pseudoalgoritmo correspondiente se muestra en la Figura 4:\n",
        "\n",
        "\n",
        "\n",
        " <center>  $\\mathbf{w}_{t+1} = \\mathbf{w}_{t+1} + \\alpha [R_{t+1} + \\gamma \\hat{q}(S_{t+1},a,\\mathbf{w}_t) - \\hat{q}(S_{t+1},a,\\mathbf{w}_t) ] \\nabla \\hat{q}(S_{t+1},a,\\mathbf{w}_t)$  &emsp;&emsp;&emsp;$(1)$ </center>\n",
        "\n",
        "\n",
        "![SARSAnn](https://raw.githubusercontent.com/MAIA4361-Aprendizaje-refuerzo-profundo/Notebooks_Tareas/main/Images/SARSAnn_dF.png)\n",
        "\n",
        "\n",
        "<center>Figura 4. Algoritmo de SARSA con aproximación de funciones para control de política.</center>\n"
      ],
      "metadata": {
        "id": "9lCj3GovPOcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación de Librerías  \n",
        "\n",
        "Ejecute los siguientes bloques de código para instalar e importar las librerías requeridas en el tutorial. Aquí se instalan versiones compatibles de <i>keras</i>, <i>tensorflow</i> y <i>Gymnasium</i> para poder realizar los entrenamientos de SARSA con redes neuronales. La primera ejecución puede demorar un par de minutos en finalizar. Si la segunda celda le da un error, pruebe volver a ejecutar la celda."
      ],
      "metadata": {
        "id": "chWp-3WSPQlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instala el framework de keras-rl2 y requisito de tensorflow\n",
        "!pip install keras-rl2==1.0.5\n",
        "!pip install tensorflow==2.15.0\n",
        "#Instala renderlab para renderizar videos de gym\n",
        "!pip install renderlab\n",
        "\n",
        "#importa librerías básicas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import tensorflow as tf #importa tensorflow\n",
        "import renderlab #importa renderlab para videos\n",
        "import gym as gym #importa gym (usado en entrenamiento)\n",
        "import gymnasium as gymnasium #importa gymnasium (usado en render)\n",
        "\n",
        "#esta líneas evitan conflictos entre la versión de keras y tensorflow\n",
        "import tensorflow.keras as keras\n",
        "from keras import __version__\n",
        "tf.keras.__version__ = __version__\n",
        "\n",
        "#importa las herramientas de redes neuronales necesarias de keras y el agente de SARSA\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Input\n",
        "from keras.optimizers.legacy import Adam\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.agents.sarsa import SARSAAgent\n",
        "\n",
        "#limpia registros\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "print(\"Todas las librerías han sido descargadas correctamente.\")"
      ],
      "metadata": {
        "id": "Lt7lZo5UB-Nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab3d29f-4028-4cc9-d31d-c72d7410ad91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todas las librerías han sido descargadas correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Familiarización con el Entorno de Gym\n",
        "\n",
        "El ambiente de Gym de <a href=\"https://gymnasium.farama.org/environments/classic_control/mountain_car/\">Mountain Car</a> consiste de un carro atrapado en el fondo de un valle, y debe acelerar a la izquierda o a la derecha para intentar ganar el impulso suficiente para salir. Puede leer más detalladamente la documentación de este ambiente en los foros oficiales de Gymnasium.\n",
        "\n",
        "![Observation_space_cartpole](https://raw.githubusercontent.com/MAIA4361-Aprendizaje-refuerzo-profundo/Notebooks_Tareas/main/Images/Observation_space_mountaincar.png)\n",
        "\n",
        "<center>Figura 5. Espacio de observación del ambiente de <i>Mountain Car</i>. [3]</center>\n",
        "\n",
        "El espacio de estados del ambiente está definido en 2 dimensiones continuas, que definen la posición en el eje x y la velocidad del vehículo. Los límites de ambas variables puede observarse en la Figura 5. Por otra parte, el espacio de acciones consiste simplemente en 3 acciones discretas y determinísticas:\n",
        "\n",
        "*   0: Acelera a la izquierda\n",
        "*   1: No acelera\n",
        "*   2: Acelera a la derecha\n",
        "\n",
        "En este ambiente, el agente recibe una recompensa cuando alcanza una meta por fuera del valle, y por cada paso de tiempo que le toma alcanzarla recibe una recompensa negativa de -1. Si el auto llega a la meta (posición mayor a 0.5), el episodio se da por terminado, mientras que si el episodio supera los 200 pasos, se da por truncado; finalizando la simulación. Esto quiere decir que la peor recompensa posible será -200.\n"
      ],
      "metadata": {
        "id": "xdQ5A4KbPSOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo\n",
        "\n",
        "En esta sección se muestra un ejemplo de simulación de un episodio del <i>Mountain Car</i>. En este caso, el carro alterna acelerar a la izquierda y a la derecha en cada paso de tiempo."
      ],
      "metadata": {
        "id": "1CxFdNryYVwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de simulación de un episodio de Mountain Car\n",
        "env_prueba_1 = gymnasium.make(\"MountainCar-v0\", render_mode=\"rgb_array\") #Se crea el ambiente. Para este tutorial, utilice gymnasium si va a renderizar.\n",
        "env_prueba_1 = renderlab.RenderFrame(env_prueba_1, \"./output\") #Se crea una copia que se pueda renderizar con renderlab\n",
        "\n",
        "obs , info = env_prueba_1.reset() #Se reinicia el estado para comenzar. En obs se almacena el estado observado (continuo, 2 dimensiones)\n",
        "terminated = False #Inicializa una condición para el loop\n",
        "truncated = False #Inicializa una condición para el loop\n",
        "total_reward=0 #Inicializa contador del retorno\n",
        "action=0 #Inicializamos una variable de acción para alternar la selección\n",
        "\n",
        "while not (terminated or truncated): #Simula hasta que el carro salga del valle o hasta que pasen 200 episodios\n",
        "\n",
        "  #Decide una acción. En este caso alterna entre izquierda y derecha\n",
        "  if(action==0):\n",
        "    action=2\n",
        "  else:\n",
        "    action=0\n",
        "\n",
        "  obs, reward, terminated, truncated , info = env_prueba_1.step(action) #Con la función step el ambiente da un paso. Se obtiene el estado, recompensa y banderas de información\n",
        "  total_reward+=reward #Llevamos una cuenta de la recompensa total\n",
        "\n",
        "print(\"Recompensa obtenida en el episodio:\",total_reward) #Se imprime la recompensa obtenida\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "env_prueba_1.play() #Con esta función se obtiene el video de la simulación"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "2CB_sJpPOoJZ",
        "outputId": "ffbde538-a576-489c-aa3c-8f31d202c86b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recompensa obtenida en el episodio: -200.0\n",
            "\n",
            "\n",
            "\n",
            "Moviepy - Building video temp-{start}.mp4.\n",
            "Moviepy - Writing video temp-{start}.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready temp-{start}.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls  >\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puede observar que el retorno obtenido fue de -200, la peor recompensa posible. Alternar inmediatamente entre izquierda y derecha no es una estrategia útil."
      ],
      "metadata": {
        "id": "cjpcdfBJbir2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio Práctico\n",
        "\n",
        "Ahora, para comprobar el entendimiento y familiarización con el ambiente, intente por unos pocos minutos generar manualmente alguna estrategia para hacer que el carro logre salir del pozo (este será el problema que solucionaremos después con redes neuronales)."
      ],
      "metadata": {
        "id": "ZQd8kQn9cYpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Genere alguna estrategia para intentar salir del pozo.\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETAR ===========================================\n",
        "#\n",
        "\n",
        "# ====================================================="
      ],
      "metadata": {
        "id": "v6MT0vr3dNe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicción on-policy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MbY4Th6pPUbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Control on-policy\n",
        "\n",
        "En este apartado se busca comprender la construcción de arquitecturas de redes neuronales profundas y luego realizar un entrenamiento con el algoritmo <i>on-policy</i> de SARSA. Se va a revisar el efecto que tienen los cambios de hiperparámetros y se busca luego visualizar la función de valor discutida en la sección anterior.\n"
      ],
      "metadata": {
        "id": "4FrsG3G_PVot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo\n",
        "\n",
        "En este apartado se muestra un breve ejemplo de cómo se organiza un código para entrenar una red neuronal utilizando el algoritmo de SARSA.\n",
        "\n",
        "El primer paso consiste siemplemente en crear el ambiente con funciones ya vistas anteriormente. Después, el segundo paso es plantear una red neuronal utilizando <i>keras</i>. La capa de entrada de la red neuronal tendrá que recibir la representación del estado, mientras que la capa de salida debe establecer la acción a tomar. En medio, podemos definir cualquier arquitectura que deseemos o creamos puede ser eficiente y efectiva en la solución del problema: la cantidad de capas escondidas, neuronas en cada capa, y funciones de activación, es una decisión del programador. En este ejemplo se utilizan únicamente 3 capas con 8 neuronas cada una y función de activación ReLU.\n",
        "\n",
        "La siguiente etapa es configurar el modelo de SARSA. Especificaciones sobre la política inicial y la cantidad de memoria utilizada en el algoritmo son algunos de los parámetros que se pueden configurar usando el agente incluido en <i>keras-rl2</i>. Una buena práctica es cargar los pesos iniciales de la red neuronal antes de comenzar a entrenar el modelo. Para comenzar el entrenamiento simplemente se usa la función <i>fit</i>, donde especificamos la cantidad de pasos a realizar en el entrenamiento. Tome en consideración que para este problema la mayoría de episodios al inicio del entrenamiento tomarán 200 pasos.\n"
      ],
      "metadata": {
        "id": "PSxQ5PPofVIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de construcción de una red neuronal y entrenamiento con SARSA\n",
        "\n",
        "########################## Creación del Ambiente ##########################\n",
        "env = gym.make('MountainCar-v0') #crea el ambiente utilizando gym.\n",
        "np.random.seed(613) #semilla aleatoria\n",
        "env.seed(613)\n",
        "nb_actions = env.action_space.n #obtiene la cantidad de acciones (Discrete (3))\n",
        "\n",
        "###################### Creación de la Red Neuronal ########################\n",
        "# Modelo secuencial, una capa después de otra\n",
        "model = Sequential()\n",
        "\n",
        "#Capa de Entrada\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "#Capas \"Escondidas\". Aquí se pueden añadir varias capas con distinta cantidad de neuronas\n",
        "model.add(Dense(8,activation='relu'))\n",
        "model.add(Dense(4,activation='relu'))\n",
        "model.add(Dense(4,activation='relu'))\n",
        "\n",
        "#Capa de Salida\n",
        "model.add(Dense(nb_actions,activation='relu'))\n",
        "\n",
        "print(model.summary()) #muestra las neuronas y parámetros de la red\n",
        "\n",
        "###################### Configuración del Modelo ##########################\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1) #límite de la memoria\n",
        "policy = BoltzmannQPolicy() #política inicial\n",
        "\n",
        "#se crea el agente de SARSA\n",
        "sarsa_ejemplo_1 = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=100, policy=policy)\n",
        "sarsa_ejemplo_1.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "\n",
        "model.save_weights('model_sarsa_ejemplo_1_valoresiniciales.h5') #guarda los valores iniciales\n",
        "\n",
        "############################ Entrenamiento ###############################\n",
        "#Se entrena el agente por 10,000 pasos de tiempo y se guardan los resultados\n",
        "\n",
        "#carga los pesos iniciales para entrenar desde 0 (buena practica)\n",
        "sarsa_ejemplo_1.load_weights('model_sarsa_ejemplo_1_valoresiniciales.h5')\n",
        "\n",
        "#Comienza a entrenar\n",
        "#Guarda el historial y muestra progreso en pantalla\n",
        "historial_entrenamiento_sarsa_ejemplo_1 = sarsa_ejemplo_1.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n",
        "#guarda los pesos finales (buena practica)\n",
        "sarsa_ejemplo_1.save_weights('model_sarsa_ejemplo_1_valoresfinales.h5f', overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-f1j-Dud33A",
        "outputId": "1f5a93db-bcde-4659-80d4-2fde483a54e7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_4 (Flatten)         (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 8)                 24        \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 4)                 20        \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 3)                 15        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 95 (380.00 Byte)\n",
            "Trainable params: 95 (380.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 10000 steps ...\n",
            "  200/10000: episode: 1, duration: 7.579s, episode steps: 200, steps per second:  26, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500113, mae: 0.338317, mean_q: 0.021680\n",
            "  400/10000: episode: 2, duration: 6.662s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  600/10000: episode: 3, duration: 6.648s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  800/10000: episode: 4, duration: 6.664s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 1000/10000: episode: 5, duration: 6.668s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 1200/10000: episode: 6, duration: 6.674s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 1400/10000: episode: 7, duration: 6.660s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 1600/10000: episode: 8, duration: 6.650s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 1800/10000: episode: 9, duration: 6.676s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 2000/10000: episode: 10, duration: 6.656s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 2200/10000: episode: 11, duration: 6.669s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 2400/10000: episode: 12, duration: 6.657s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 2600/10000: episode: 13, duration: 6.671s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 2800/10000: episode: 14, duration: 6.659s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 3000/10000: episode: 15, duration: 6.664s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 3200/10000: episode: 16, duration: 6.666s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 3400/10000: episode: 17, duration: 6.657s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 3600/10000: episode: 18, duration: 6.675s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 3800/10000: episode: 19, duration: 6.661s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 4000/10000: episode: 20, duration: 6.661s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 4200/10000: episode: 21, duration: 6.666s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 4400/10000: episode: 22, duration: 6.656s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 4600/10000: episode: 23, duration: 6.664s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 4800/10000: episode: 24, duration: 6.656s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 5000/10000: episode: 25, duration: 6.692s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 5200/10000: episode: 26, duration: 6.862s, episode steps: 200, steps per second:  29, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 5400/10000: episode: 27, duration: 6.874s, episode steps: 200, steps per second:  29, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 5600/10000: episode: 28, duration: 6.747s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 5800/10000: episode: 29, duration: 6.832s, episode steps: 200, steps per second:  29, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 6000/10000: episode: 30, duration: 6.710s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 6200/10000: episode: 31, duration: 6.674s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 6400/10000: episode: 32, duration: 6.755s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 6600/10000: episode: 33, duration: 6.740s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 6800/10000: episode: 34, duration: 6.912s, episode steps: 200, steps per second:  29, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 7000/10000: episode: 35, duration: 6.737s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 7200/10000: episode: 36, duration: 6.662s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 7400/10000: episode: 37, duration: 6.666s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 7600/10000: episode: 38, duration: 6.751s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 7800/10000: episode: 39, duration: 6.681s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 8000/10000: episode: 40, duration: 6.779s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 8200/10000: episode: 41, duration: 6.756s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 8400/10000: episode: 42, duration: 6.748s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 8600/10000: episode: 43, duration: 6.713s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 8800/10000: episode: 44, duration: 6.667s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 9000/10000: episode: 45, duration: 6.654s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 9200/10000: episode: 46, duration: 6.717s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 9400/10000: episode: 47, duration: 6.665s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 9600/10000: episode: 48, duration: 6.653s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 9800/10000: episode: 49, duration: 6.667s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10000/10000: episode: 50, duration: 6.680s, episode steps: 200, steps per second:  30, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "done, took 335.877 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de haber completado las iteraciones de entrenamiento de la red, se procede a validar los resultados. Tras salvar y cargar los pesos finales del entrenamiento, se usa la función de <i>test</i> para simular cierta cantidad de episodios con la política obtenida y se obtiene la recompensa obtenida. Si la recompensa es mayor a -200, significa que el vehículo logró escapar del valle; en caso contrario el episodio terminó sin completar el objetivo."
      ],
      "metadata": {
        "id": "XNnBffWS9WAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################ Validación ###############################\n",
        "sarsa_ejemplo_1.load_weights('model_sarsa_ejemplo_1_valoresfinales.h5f') #Carga los pesos del entrenamiento (buena practica)\n",
        "env.reset() #reinicia el ambiente\n",
        "test_episodes = 5  # Número de episodios de validación\n",
        "\n",
        "sarsa_ejemplo_1.test(env, nb_episodes=test_episodes, visualize=True) #Simula con la política obtenida"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQQpo0iKfgB3",
        "outputId": "b1400330-a540-42a7-a103-2bc5df9b2f42"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: -200.000, steps: 200\n",
            "Episode 2: reward: -200.000, steps: 200\n",
            "Episode 3: reward: -200.000, steps: 200\n",
            "Episode 4: reward: -200.000, steps: 200\n",
            "Episode 5: reward: -200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7814fc5de410>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "También se puede simular un episodio y renderizar el resultado. Usando una estructura similar a la usada anteriormente, se simula el episodio escogiendo las acciones con base en la política encontrada con SARSA y las observaciones del ambiente.  "
      ],
      "metadata": {
        "id": "OKP8q_xi9Xbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################## Video #################################\n",
        "sarsa_ejemplo_1.load_weights('model_sarsa_ejemplo_1_valoresfinales.h5f') #Carga los pesos del entrenamiento (buena practica)\n",
        "\n",
        "env_test_render = gymnasium.make('MountainCar-v0', render_mode = \"rgb_array\") #crea una ambiente de prueba (con gymnasium para renderizar)\n",
        "env_test_render = renderlab.RenderFrame(env_test_render, \"./output\") #crea copia de renderlab\n",
        "observation, info = env_test_render.reset() #inicializa el ambiente\n",
        "\n",
        "while True: #simula\n",
        "  action = sarsa_ejemplo_1.forward(observation) #elige una acción de acuerdo a sarsa 4 (se usan los resultados de la primera red)\n",
        "  observation, reward, terminated, truncated, info = env_test_render.step(action) #obtiene la observación tras tomar la acción\n",
        "  if terminated or truncated: #si pierde o pasan 200 pasos\n",
        "      break #termina\n",
        "\n",
        "env_test_render.play() #muestra el video"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "Z9MQMzQVjeh0",
        "outputId": "11a0de6d-5bc8-4901-e0c2-d949be050945"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video temp-{start}.mp4.\n",
            "Moviepy - Writing video temp-{start}.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready temp-{start}.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls  >\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio Práctico\n",
        "\n",
        "En el ejemplo, el carrito no logró escapar del valle. Esto se debe a que la red neuronal puede no ser lo suficientemente profunda o no tiene las suficientes neuronas para lograr una correcta representación del ambiente, y también porque la cantidad de pasos invertidos en el entrenamiento no es suficiente. Ahora usted intente modificar la red neuronal, añadiendo más neuronas por capa o haciéndola más profunda, e incremente la cantidad de episodios de entrenamiento. Luego valide sus resultados igual que en el ejemplo y compare el resultado final."
      ],
      "metadata": {
        "id": "slZCdTaNlaWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SARSA TRAIN - Genere una nueva red neuronal y entrenamiento con SARSA\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETAR ===========================================\n",
        "#\n",
        "\n",
        "# ====================================================="
      ],
      "metadata": {
        "id": "xUpgdw_m_Zfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SARSA TEST - Valide sus resultados y genere un video\n",
        "\n",
        "# =====================================================\n",
        "# COMPLETAR ===========================================\n",
        "#\n",
        "\n",
        "# ====================================================="
      ],
      "metadata": {
        "id": "5uPLGtuk_Z7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ctq3AXa3_kOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de construcción de una red neuronal y entrenamiento con SARSA\n",
        "\n",
        "########################## Creación del Ambiente ##########################\n",
        "env = gym.make('MountainCar-v0') #crea el ambiente utilizando gym.\n",
        "np.random.seed(613) #semilla aleatoria\n",
        "env.seed(613)\n",
        "nb_actions = env.action_space.n #obtiene la cantidad de acciones (3)\n",
        "\n",
        "###################### Creación de la Red Neuronal ########################\n",
        "# Modelo secuencial, una capa después de otra\n",
        "model = Sequential()\n",
        "\n",
        "#Capa de Entrada\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "#Capas \"Escondidas\". Aquí se pueden añadir varias capas con distinta cantidad de neuronas\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(16,activation='relu'))\n",
        "\n",
        "#Capa de Salida\n",
        "model.add(Dense(nb_actions,activation='relu'))\n",
        "\n",
        "print(model.summary()) #muestra las neuronas y parámetros de la red\n",
        "\n",
        "###################### Configuración del Modelo ##########################\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1) #límite de la memoria\n",
        "policy = BoltzmannQPolicy() #política inicial\n",
        "\n",
        "#se crea el agente de SARSA\n",
        "sarsa_solucion_1 = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=100, policy=policy)\n",
        "sarsa_solucion_1.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "\n",
        "model.save_weights('model_sarsa_solucion_1_valoresiniciales.h5') #guarda los valores iniciales\n",
        "\n",
        "############################ Entrenamiento ###############################\n",
        "#Se entrena el agente por 10,000 pasos de tiempo y se guardan los resultados\n",
        "\n",
        "#carga los pesos iniciales para entrenar desde 0 (buena practica)\n",
        "sarsa_solucion_1.load_weights('model_sarsa_solucion_1_valoresiniciales.h5')\n",
        "\n",
        "#Comienza a entrenar\n",
        "#Guarda el historial y muestra progreso en pantalla\n",
        "historial_solucion_sarsa_ejemplo_1 = sarsa_solucion_1.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
        "\n",
        "#guarda los pesos finales (buena practica)\n",
        "sarsa_solucion_1.save_weights('model_sarsa_solucion_1_valoresfinales.h5f', overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aYCb-uylajm",
        "outputId": "1a2e0979-1c7b-44ef-a8ec-3c62503ce3d6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_8 (Flatten)         (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 32)                96        \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 675 (2.64 KB)\n",
            "Trainable params: 675 (2.64 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   200/100000: episode: 1, duration: 2.790s, episode steps: 200, steps per second:  72, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500272, mae: 0.336641, mean_q: 0.013720\n",
            "   400/100000: episode: 2, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   600/100000: episode: 3, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   800/100000: episode: 4, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1000/100000: episode: 5, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1200/100000: episode: 6, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1400/100000: episode: 7, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1600/100000: episode: 8, duration: 1.567s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1800/100000: episode: 9, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2000/100000: episode: 10, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2200/100000: episode: 11, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2400/100000: episode: 12, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2600/100000: episode: 13, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2800/100000: episode: 14, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3000/100000: episode: 15, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3200/100000: episode: 16, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3400/100000: episode: 17, duration: 1.750s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3600/100000: episode: 18, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3800/100000: episode: 19, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4000/100000: episode: 20, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4200/100000: episode: 21, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4400/100000: episode: 22, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4600/100000: episode: 23, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4800/100000: episode: 24, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5000/100000: episode: 25, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5200/100000: episode: 26, duration: 1.804s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5400/100000: episode: 27, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5600/100000: episode: 28, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5800/100000: episode: 29, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6000/100000: episode: 30, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6200/100000: episode: 31, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6400/100000: episode: 32, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6600/100000: episode: 33, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6800/100000: episode: 34, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7000/100000: episode: 35, duration: 1.808s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7200/100000: episode: 36, duration: 1.624s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7400/100000: episode: 37, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7600/100000: episode: 38, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7800/100000: episode: 39, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8000/100000: episode: 40, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8200/100000: episode: 41, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8400/100000: episode: 42, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8600/100000: episode: 43, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8800/100000: episode: 44, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9000/100000: episode: 45, duration: 1.529s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9200/100000: episode: 46, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9400/100000: episode: 47, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9600/100000: episode: 48, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9800/100000: episode: 49, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10000/100000: episode: 50, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10200/100000: episode: 51, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10400/100000: episode: 52, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10600/100000: episode: 53, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10800/100000: episode: 54, duration: 1.542s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11000/100000: episode: 55, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11200/100000: episode: 56, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11400/100000: episode: 57, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11600/100000: episode: 58, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11800/100000: episode: 59, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12000/100000: episode: 60, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12200/100000: episode: 61, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12400/100000: episode: 62, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12600/100000: episode: 63, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12800/100000: episode: 64, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13000/100000: episode: 65, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13200/100000: episode: 66, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13400/100000: episode: 67, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13600/100000: episode: 68, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13800/100000: episode: 69, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14000/100000: episode: 70, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14200/100000: episode: 71, duration: 1.759s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14400/100000: episode: 72, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14600/100000: episode: 73, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14800/100000: episode: 74, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15000/100000: episode: 75, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15200/100000: episode: 76, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15400/100000: episode: 77, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15600/100000: episode: 78, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15800/100000: episode: 79, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16000/100000: episode: 80, duration: 1.714s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16200/100000: episode: 81, duration: 1.635s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16400/100000: episode: 82, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16600/100000: episode: 83, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16800/100000: episode: 84, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17000/100000: episode: 85, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17200/100000: episode: 86, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17400/100000: episode: 87, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17600/100000: episode: 88, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17800/100000: episode: 89, duration: 1.626s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18000/100000: episode: 90, duration: 1.669s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18200/100000: episode: 91, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18400/100000: episode: 92, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18600/100000: episode: 93, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18800/100000: episode: 94, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19000/100000: episode: 95, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19200/100000: episode: 96, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19400/100000: episode: 97, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19600/100000: episode: 98, duration: 1.498s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19800/100000: episode: 99, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20000/100000: episode: 100, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20200/100000: episode: 101, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20400/100000: episode: 102, duration: 1.423s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20600/100000: episode: 103, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20800/100000: episode: 104, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21000/100000: episode: 105, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21200/100000: episode: 106, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21400/100000: episode: 107, duration: 1.662s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21600/100000: episode: 108, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21800/100000: episode: 109, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22000/100000: episode: 110, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22200/100000: episode: 111, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22400/100000: episode: 112, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22600/100000: episode: 113, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22800/100000: episode: 114, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23000/100000: episode: 115, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23200/100000: episode: 116, duration: 1.774s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23400/100000: episode: 117, duration: 1.604s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23600/100000: episode: 118, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23800/100000: episode: 119, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24000/100000: episode: 120, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24200/100000: episode: 121, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24400/100000: episode: 122, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24600/100000: episode: 123, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24800/100000: episode: 124, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25000/100000: episode: 125, duration: 1.763s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25200/100000: episode: 126, duration: 1.650s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25400/100000: episode: 127, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25600/100000: episode: 128, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25800/100000: episode: 129, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26000/100000: episode: 130, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26200/100000: episode: 131, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26400/100000: episode: 132, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26600/100000: episode: 133, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26800/100000: episode: 134, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27000/100000: episode: 135, duration: 1.517s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27200/100000: episode: 136, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27400/100000: episode: 137, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27600/100000: episode: 138, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27800/100000: episode: 139, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28000/100000: episode: 140, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28200/100000: episode: 141, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28400/100000: episode: 142, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28600/100000: episode: 143, duration: 1.964s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28800/100000: episode: 144, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29000/100000: episode: 145, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29200/100000: episode: 146, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29400/100000: episode: 147, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29600/100000: episode: 148, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29800/100000: episode: 149, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30000/100000: episode: 150, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30200/100000: episode: 151, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30400/100000: episode: 152, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30600/100000: episode: 153, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30800/100000: episode: 154, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31000/100000: episode: 155, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31200/100000: episode: 156, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31400/100000: episode: 157, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31600/100000: episode: 158, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31800/100000: episode: 159, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32000/100000: episode: 160, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32200/100000: episode: 161, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32400/100000: episode: 162, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32600/100000: episode: 163, duration: 1.286s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32800/100000: episode: 164, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33000/100000: episode: 165, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33200/100000: episode: 166, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33400/100000: episode: 167, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33600/100000: episode: 168, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33800/100000: episode: 169, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34000/100000: episode: 170, duration: 1.942s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34200/100000: episode: 171, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34400/100000: episode: 172, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34600/100000: episode: 173, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34800/100000: episode: 174, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35000/100000: episode: 175, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35200/100000: episode: 176, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35400/100000: episode: 177, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35600/100000: episode: 178, duration: 1.493s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35800/100000: episode: 179, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36000/100000: episode: 180, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36200/100000: episode: 181, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36400/100000: episode: 182, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36600/100000: episode: 183, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36800/100000: episode: 184, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37000/100000: episode: 185, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37200/100000: episode: 186, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37400/100000: episode: 187, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37600/100000: episode: 188, duration: 1.946s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37800/100000: episode: 189, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38000/100000: episode: 190, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38200/100000: episode: 191, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38400/100000: episode: 192, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38600/100000: episode: 193, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38800/100000: episode: 194, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39000/100000: episode: 195, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39200/100000: episode: 196, duration: 1.481s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39400/100000: episode: 197, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39600/100000: episode: 198, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39800/100000: episode: 199, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40000/100000: episode: 200, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40200/100000: episode: 201, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40400/100000: episode: 202, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40600/100000: episode: 203, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40800/100000: episode: 204, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41000/100000: episode: 205, duration: 1.556s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41200/100000: episode: 206, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41400/100000: episode: 207, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41600/100000: episode: 208, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41800/100000: episode: 209, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42000/100000: episode: 210, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42200/100000: episode: 211, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42400/100000: episode: 212, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42600/100000: episode: 213, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42800/100000: episode: 214, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43000/100000: episode: 215, duration: 1.756s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43200/100000: episode: 216, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43400/100000: episode: 217, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43600/100000: episode: 218, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43800/100000: episode: 219, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44000/100000: episode: 220, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44200/100000: episode: 221, duration: 1.365s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44400/100000: episode: 222, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44600/100000: episode: 223, duration: 1.680s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44800/100000: episode: 224, duration: 1.732s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45000/100000: episode: 225, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45200/100000: episode: 226, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45400/100000: episode: 227, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45600/100000: episode: 228, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45800/100000: episode: 229, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46000/100000: episode: 230, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46200/100000: episode: 231, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46400/100000: episode: 232, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46600/100000: episode: 233, duration: 1.614s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46800/100000: episode: 234, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47000/100000: episode: 235, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47200/100000: episode: 236, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47400/100000: episode: 237, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47600/100000: episode: 238, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47800/100000: episode: 239, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48000/100000: episode: 240, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48200/100000: episode: 241, duration: 2.018s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48400/100000: episode: 242, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48600/100000: episode: 243, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48800/100000: episode: 244, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49000/100000: episode: 245, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49200/100000: episode: 246, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49400/100000: episode: 247, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49600/100000: episode: 248, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49800/100000: episode: 249, duration: 1.636s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50000/100000: episode: 250, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50200/100000: episode: 251, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50400/100000: episode: 252, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50600/100000: episode: 253, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50800/100000: episode: 254, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51000/100000: episode: 255, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51200/100000: episode: 256, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51400/100000: episode: 257, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51600/100000: episode: 258, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51800/100000: episode: 259, duration: 1.610s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52000/100000: episode: 260, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52200/100000: episode: 261, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52400/100000: episode: 262, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52600/100000: episode: 263, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52800/100000: episode: 264, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53000/100000: episode: 265, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53200/100000: episode: 266, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53400/100000: episode: 267, duration: 1.896s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53600/100000: episode: 268, duration: 1.508s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53800/100000: episode: 269, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54000/100000: episode: 270, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54200/100000: episode: 271, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54400/100000: episode: 272, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54600/100000: episode: 273, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54800/100000: episode: 274, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55000/100000: episode: 275, duration: 1.465s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55200/100000: episode: 276, duration: 1.975s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55400/100000: episode: 277, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55600/100000: episode: 278, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55800/100000: episode: 279, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56000/100000: episode: 280, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56200/100000: episode: 281, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56400/100000: episode: 282, duration: 1.365s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56600/100000: episode: 283, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56800/100000: episode: 284, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57000/100000: episode: 285, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57200/100000: episode: 286, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57400/100000: episode: 287, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57600/100000: episode: 288, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57800/100000: episode: 289, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58000/100000: episode: 290, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58200/100000: episode: 291, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58400/100000: episode: 292, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58600/100000: episode: 293, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58800/100000: episode: 294, duration: 1.622s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59000/100000: episode: 295, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59200/100000: episode: 296, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59400/100000: episode: 297, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59600/100000: episode: 298, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59800/100000: episode: 299, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60000/100000: episode: 300, duration: 1.347s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60200/100000: episode: 301, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60400/100000: episode: 302, duration: 1.926s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60600/100000: episode: 303, duration: 1.590s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60800/100000: episode: 304, duration: 1.441s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61000/100000: episode: 305, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61200/100000: episode: 306, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61400/100000: episode: 307, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61600/100000: episode: 308, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61800/100000: episode: 309, duration: 1.388s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62000/100000: episode: 310, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62200/100000: episode: 311, duration: 1.562s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62400/100000: episode: 312, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62600/100000: episode: 313, duration: 1.510s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62800/100000: episode: 314, duration: 1.448s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63000/100000: episode: 315, duration: 1.807s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63200/100000: episode: 316, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63400/100000: episode: 317, duration: 1.545s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63600/100000: episode: 318, duration: 2.055s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63800/100000: episode: 319, duration: 1.365s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64000/100000: episode: 320, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64200/100000: episode: 321, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64400/100000: episode: 322, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64600/100000: episode: 323, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64800/100000: episode: 324, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65000/100000: episode: 325, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65200/100000: episode: 326, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65400/100000: episode: 327, duration: 1.729s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65600/100000: episode: 328, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65800/100000: episode: 329, duration: 1.494s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66000/100000: episode: 330, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66200/100000: episode: 331, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66400/100000: episode: 332, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66600/100000: episode: 333, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66800/100000: episode: 334, duration: 1.519s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67000/100000: episode: 335, duration: 1.999s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67200/100000: episode: 336, duration: 1.523s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67400/100000: episode: 337, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67600/100000: episode: 338, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67800/100000: episode: 339, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68000/100000: episode: 340, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68200/100000: episode: 341, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68400/100000: episode: 342, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68600/100000: episode: 343, duration: 1.996s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68800/100000: episode: 344, duration: 1.580s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69000/100000: episode: 345, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69200/100000: episode: 346, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69400/100000: episode: 347, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69600/100000: episode: 348, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69800/100000: episode: 349, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70000/100000: episode: 350, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70200/100000: episode: 351, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70400/100000: episode: 352, duration: 2.020s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70600/100000: episode: 353, duration: 1.398s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70800/100000: episode: 354, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71000/100000: episode: 355, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71200/100000: episode: 356, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71400/100000: episode: 357, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71600/100000: episode: 358, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71800/100000: episode: 359, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72000/100000: episode: 360, duration: 1.775s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72200/100000: episode: 361, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72400/100000: episode: 362, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72600/100000: episode: 363, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72800/100000: episode: 364, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73000/100000: episode: 365, duration: 1.477s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73200/100000: episode: 366, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73400/100000: episode: 367, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73600/100000: episode: 368, duration: 1.507s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73800/100000: episode: 369, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74000/100000: episode: 370, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74200/100000: episode: 371, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74400/100000: episode: 372, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74600/100000: episode: 373, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74800/100000: episode: 374, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75000/100000: episode: 375, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75200/100000: episode: 376, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75400/100000: episode: 377, duration: 1.470s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75600/100000: episode: 378, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75800/100000: episode: 379, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76000/100000: episode: 380, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76200/100000: episode: 381, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76400/100000: episode: 382, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76600/100000: episode: 383, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76800/100000: episode: 384, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77000/100000: episode: 385, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77200/100000: episode: 386, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77400/100000: episode: 387, duration: 1.952s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77600/100000: episode: 388, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77800/100000: episode: 389, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78000/100000: episode: 390, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78200/100000: episode: 391, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78400/100000: episode: 392, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78600/100000: episode: 393, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78800/100000: episode: 394, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79000/100000: episode: 395, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79200/100000: episode: 396, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79400/100000: episode: 397, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79600/100000: episode: 398, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79800/100000: episode: 399, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80000/100000: episode: 400, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80200/100000: episode: 401, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80400/100000: episode: 402, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80600/100000: episode: 403, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80800/100000: episode: 404, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81000/100000: episode: 405, duration: 1.715s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81200/100000: episode: 406, duration: 1.538s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81400/100000: episode: 407, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81600/100000: episode: 408, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81800/100000: episode: 409, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82000/100000: episode: 410, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82200/100000: episode: 411, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82400/100000: episode: 412, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82600/100000: episode: 413, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82800/100000: episode: 414, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83000/100000: episode: 415, duration: 1.692s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83200/100000: episode: 416, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83400/100000: episode: 417, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83600/100000: episode: 418, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83800/100000: episode: 419, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84000/100000: episode: 420, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84200/100000: episode: 421, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84400/100000: episode: 422, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84600/100000: episode: 423, duration: 1.574s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84800/100000: episode: 424, duration: 1.790s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85000/100000: episode: 425, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85200/100000: episode: 426, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85400/100000: episode: 427, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85600/100000: episode: 428, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85800/100000: episode: 429, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86000/100000: episode: 430, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86200/100000: episode: 431, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86400/100000: episode: 432, duration: 1.465s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86600/100000: episode: 433, duration: 1.921s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86800/100000: episode: 434, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87000/100000: episode: 435, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87200/100000: episode: 436, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87400/100000: episode: 437, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87600/100000: episode: 438, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87800/100000: episode: 439, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88000/100000: episode: 440, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88200/100000: episode: 441, duration: 1.481s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88400/100000: episode: 442, duration: 1.921s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88600/100000: episode: 443, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88800/100000: episode: 444, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89000/100000: episode: 445, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89200/100000: episode: 446, duration: 1.394s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89400/100000: episode: 447, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89600/100000: episode: 448, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89800/100000: episode: 449, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90000/100000: episode: 450, duration: 1.539s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90200/100000: episode: 451, duration: 1.785s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90400/100000: episode: 452, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90600/100000: episode: 453, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90800/100000: episode: 454, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91000/100000: episode: 455, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91200/100000: episode: 456, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91400/100000: episode: 457, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91600/100000: episode: 458, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91800/100000: episode: 459, duration: 1.493s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92000/100000: episode: 460, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92200/100000: episode: 461, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92400/100000: episode: 462, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92600/100000: episode: 463, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92800/100000: episode: 464, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93000/100000: episode: 465, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93200/100000: episode: 466, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93400/100000: episode: 467, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93600/100000: episode: 468, duration: 1.702s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93800/100000: episode: 469, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94000/100000: episode: 470, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94200/100000: episode: 471, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94400/100000: episode: 472, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94600/100000: episode: 473, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94800/100000: episode: 474, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95000/100000: episode: 475, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95200/100000: episode: 476, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95400/100000: episode: 477, duration: 1.781s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95600/100000: episode: 478, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95800/100000: episode: 479, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96000/100000: episode: 480, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96200/100000: episode: 481, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96400/100000: episode: 482, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96600/100000: episode: 483, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96800/100000: episode: 484, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97000/100000: episode: 485, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97200/100000: episode: 486, duration: 2.167s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97400/100000: episode: 487, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97600/100000: episode: 488, duration: 1.510s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97800/100000: episode: 489, duration: 1.509s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98000/100000: episode: 490, duration: 1.459s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98200/100000: episode: 491, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98400/100000: episode: 492, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98600/100000: episode: 493, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98800/100000: episode: 494, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99000/100000: episode: 495, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99200/100000: episode: 496, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99400/100000: episode: 497, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99600/100000: episode: 498, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99800/100000: episode: 499, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100000/100000: episode: 500, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "done, took 703.994 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de construcción de una red neuronal y entrenamiento con SARSA\n",
        "\n",
        "########################## Creación del Ambiente ##########################\n",
        "env = gym.make('MountainCar-v0') #crea el ambiente utilizando gym.\n",
        "np.random.seed(613) #semilla aleatoria\n",
        "env.seed(613)\n",
        "nb_actions = env.action_space.n #obtiene la cantidad de acciones (3)\n",
        "\n",
        "###################### Creación de la Red Neuronal ########################\n",
        "# Modelo secuencial, una capa después de otra\n",
        "model = Sequential()\n",
        "\n",
        "#Capa de Entrada\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "#Capas \"Escondidas\". Aquí se pueden añadir varias capas con distinta cantidad de neuronas\n",
        "model.add(Dense(16,activation='relu'))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "\n",
        "#Capa de Salida\n",
        "model.add(Dense(nb_actions,activation='relu'))\n",
        "\n",
        "print(model.summary()) #muestra las neuronas y parámetros de la red\n",
        "\n",
        "###################### Configuración del Modelo ##########################\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1) #límite de la memoria\n",
        "policy = BoltzmannQPolicy() #política inicial\n",
        "\n",
        "#se crea el agente de SARSA\n",
        "sarsa_solucion_2 = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=100, policy=policy)\n",
        "sarsa_solucion_2.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "\n",
        "model.save_weights('model_sarsa_solucion_2_valoresiniciales.h5') #guarda los valores iniciales\n",
        "\n",
        "############################ Entrenamiento ###############################\n",
        "#Se entrena el agente por 10,000 pasos de tiempo y se guardan los resultados\n",
        "\n",
        "#carga los pesos iniciales para entrenar desde 0 (buena practica)\n",
        "sarsa_solucion_2.load_weights('model_sarsa_solucion_2_valoresiniciales.h5')\n",
        "\n",
        "#Comienza a entrenar\n",
        "#Guarda el historial y muestra progreso en pantalla\n",
        "historial_solucion_sarsa_ejemplo_2 = sarsa_solucion_2.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
        "\n",
        "#guarda los pesos finales (buena practica)\n",
        "sarsa_solucion_2.save_weights('model_sarsa_solucion_2_valoresfinales.h5f', overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpv7jA87s0dz",
        "outputId": "4e956d2b-735f-485d-b384-922836259bae"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_7 (Flatten)         (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 16)                48        \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 3)                 27        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 211 (844.00 Byte)\n",
            "Trainable params: 211 (844.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100000 steps ...\n",
            "   200/100000: episode: 1, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   400/100000: episode: 2, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   600/100000: episode: 3, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   800/100000: episode: 4, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1000/100000: episode: 5, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1200/100000: episode: 6, duration: 1.569s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1400/100000: episode: 7, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1600/100000: episode: 8, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  1800/100000: episode: 9, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2000/100000: episode: 10, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2200/100000: episode: 11, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2400/100000: episode: 12, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2600/100000: episode: 13, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  2800/100000: episode: 14, duration: 1.753s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3000/100000: episode: 15, duration: 1.606s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3200/100000: episode: 16, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3400/100000: episode: 17, duration: 1.613s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3600/100000: episode: 18, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  3800/100000: episode: 19, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4000/100000: episode: 20, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4200/100000: episode: 21, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4400/100000: episode: 22, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4600/100000: episode: 23, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  4800/100000: episode: 24, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5000/100000: episode: 25, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5200/100000: episode: 26, duration: 1.677s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5400/100000: episode: 27, duration: 1.654s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5600/100000: episode: 28, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  5800/100000: episode: 29, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6000/100000: episode: 30, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6200/100000: episode: 31, duration: 1.759s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6400/100000: episode: 32, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6600/100000: episode: 33, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  6800/100000: episode: 34, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7000/100000: episode: 35, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7200/100000: episode: 36, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7400/100000: episode: 37, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7600/100000: episode: 38, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  7800/100000: episode: 39, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8000/100000: episode: 40, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8200/100000: episode: 41, duration: 1.665s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8400/100000: episode: 42, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8600/100000: episode: 43, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  8800/100000: episode: 44, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9000/100000: episode: 45, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9200/100000: episode: 46, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9400/100000: episode: 47, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9600/100000: episode: 48, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  9800/100000: episode: 49, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10000/100000: episode: 50, duration: 1.645s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10200/100000: episode: 51, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10400/100000: episode: 52, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10600/100000: episode: 53, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 10800/100000: episode: 54, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11000/100000: episode: 55, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11200/100000: episode: 56, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11400/100000: episode: 57, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11600/100000: episode: 58, duration: 1.698s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 11800/100000: episode: 59, duration: 1.649s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12000/100000: episode: 60, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12200/100000: episode: 61, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12400/100000: episode: 62, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12600/100000: episode: 63, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 12800/100000: episode: 64, duration: 1.638s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13000/100000: episode: 65, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13200/100000: episode: 66, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13400/100000: episode: 67, duration: 1.921s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13600/100000: episode: 68, duration: 1.463s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 13800/100000: episode: 69, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14000/100000: episode: 70, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14200/100000: episode: 71, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14400/100000: episode: 72, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14600/100000: episode: 73, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 14800/100000: episode: 74, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15000/100000: episode: 75, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15200/100000: episode: 76, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15400/100000: episode: 77, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15600/100000: episode: 78, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 15800/100000: episode: 79, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16000/100000: episode: 80, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16200/100000: episode: 81, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16400/100000: episode: 82, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16600/100000: episode: 83, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 16800/100000: episode: 84, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17000/100000: episode: 85, duration: 1.722s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17200/100000: episode: 86, duration: 1.502s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17400/100000: episode: 87, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17600/100000: episode: 88, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 17800/100000: episode: 89, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18000/100000: episode: 90, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18200/100000: episode: 91, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18400/100000: episode: 92, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18600/100000: episode: 93, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 18800/100000: episode: 94, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19000/100000: episode: 95, duration: 1.643s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19200/100000: episode: 96, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19400/100000: episode: 97, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19600/100000: episode: 98, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 19800/100000: episode: 99, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20000/100000: episode: 100, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20200/100000: episode: 101, duration: 1.191s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20400/100000: episode: 102, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20600/100000: episode: 103, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 20800/100000: episode: 104, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21000/100000: episode: 105, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21200/100000: episode: 106, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21400/100000: episode: 107, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21600/100000: episode: 108, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 21800/100000: episode: 109, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22000/100000: episode: 110, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22200/100000: episode: 111, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22400/100000: episode: 112, duration: 1.505s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22600/100000: episode: 113, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 22800/100000: episode: 114, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23000/100000: episode: 115, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23200/100000: episode: 116, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23400/100000: episode: 117, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23600/100000: episode: 118, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 23800/100000: episode: 119, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24000/100000: episode: 120, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24200/100000: episode: 121, duration: 1.636s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24400/100000: episode: 122, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24600/100000: episode: 123, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 24800/100000: episode: 124, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25000/100000: episode: 125, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25200/100000: episode: 126, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25400/100000: episode: 127, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25600/100000: episode: 128, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 25800/100000: episode: 129, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26000/100000: episode: 130, duration: 1.567s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26200/100000: episode: 131, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26400/100000: episode: 132, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26600/100000: episode: 133, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 26800/100000: episode: 134, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27000/100000: episode: 135, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27200/100000: episode: 136, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27400/100000: episode: 137, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27600/100000: episode: 138, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 27800/100000: episode: 139, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28000/100000: episode: 140, duration: 1.733s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28200/100000: episode: 141, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28400/100000: episode: 142, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28600/100000: episode: 143, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 28800/100000: episode: 144, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29000/100000: episode: 145, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29200/100000: episode: 146, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29400/100000: episode: 147, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29600/100000: episode: 148, duration: 1.610s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 29800/100000: episode: 149, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30000/100000: episode: 150, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30200/100000: episode: 151, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30400/100000: episode: 152, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30600/100000: episode: 153, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 30800/100000: episode: 154, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31000/100000: episode: 155, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31200/100000: episode: 156, duration: 1.193s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31400/100000: episode: 157, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31600/100000: episode: 158, duration: 2.496s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 31800/100000: episode: 159, duration: 1.587s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32000/100000: episode: 160, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32200/100000: episode: 161, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32400/100000: episode: 162, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32600/100000: episode: 163, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 32800/100000: episode: 164, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33000/100000: episode: 165, duration: 1.692s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33200/100000: episode: 166, duration: 2.023s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33400/100000: episode: 167, duration: 1.533s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33600/100000: episode: 168, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 33800/100000: episode: 169, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34000/100000: episode: 170, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34200/100000: episode: 171, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34400/100000: episode: 172, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34600/100000: episode: 173, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 34800/100000: episode: 174, duration: 1.698s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35000/100000: episode: 175, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35200/100000: episode: 176, duration: 1.197s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35400/100000: episode: 177, duration: 1.210s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35600/100000: episode: 178, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 35800/100000: episode: 179, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36000/100000: episode: 180, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36200/100000: episode: 181, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36400/100000: episode: 182, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36600/100000: episode: 183, duration: 1.505s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 36800/100000: episode: 184, duration: 1.745s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37000/100000: episode: 185, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37200/100000: episode: 186, duration: 1.203s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37400/100000: episode: 187, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37600/100000: episode: 188, duration: 1.202s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 37800/100000: episode: 189, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38000/100000: episode: 190, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38200/100000: episode: 191, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38400/100000: episode: 192, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38600/100000: episode: 193, duration: 1.757s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 38800/100000: episode: 194, duration: 1.423s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39000/100000: episode: 195, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39200/100000: episode: 196, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39400/100000: episode: 197, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39600/100000: episode: 198, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 39800/100000: episode: 199, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40000/100000: episode: 200, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40200/100000: episode: 201, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40400/100000: episode: 202, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40600/100000: episode: 203, duration: 1.672s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 40800/100000: episode: 204, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41000/100000: episode: 205, duration: 1.201s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41200/100000: episode: 206, duration: 1.214s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41400/100000: episode: 207, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41600/100000: episode: 208, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 41800/100000: episode: 209, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42000/100000: episode: 210, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42200/100000: episode: 211, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42400/100000: episode: 212, duration: 1.812s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42600/100000: episode: 213, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 42800/100000: episode: 214, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43000/100000: episode: 215, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43200/100000: episode: 216, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43400/100000: episode: 217, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43600/100000: episode: 218, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 43800/100000: episode: 219, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44000/100000: episode: 220, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44200/100000: episode: 221, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44400/100000: episode: 222, duration: 1.536s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44600/100000: episode: 223, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 44800/100000: episode: 224, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45000/100000: episode: 225, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45200/100000: episode: 226, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45400/100000: episode: 227, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45600/100000: episode: 228, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 45800/100000: episode: 229, duration: 1.210s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46000/100000: episode: 230, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46200/100000: episode: 231, duration: 1.761s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46400/100000: episode: 232, duration: 1.211s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46600/100000: episode: 233, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 46800/100000: episode: 234, duration: 1.199s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47000/100000: episode: 235, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47200/100000: episode: 236, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47400/100000: episode: 237, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47600/100000: episode: 238, duration: 1.191s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 47800/100000: episode: 239, duration: 1.196s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48000/100000: episode: 240, duration: 1.682s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48200/100000: episode: 241, duration: 1.510s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48400/100000: episode: 242, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48600/100000: episode: 243, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 48800/100000: episode: 244, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49000/100000: episode: 245, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49200/100000: episode: 246, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49400/100000: episode: 247, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49600/100000: episode: 248, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 49800/100000: episode: 249, duration: 1.579s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50000/100000: episode: 250, duration: 1.722s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50200/100000: episode: 251, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50400/100000: episode: 252, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50600/100000: episode: 253, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 50800/100000: episode: 254, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51000/100000: episode: 255, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51200/100000: episode: 256, duration: 1.200s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51400/100000: episode: 257, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51600/100000: episode: 258, duration: 1.278s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 51800/100000: episode: 259, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52000/100000: episode: 260, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52200/100000: episode: 261, duration: 1.194s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52400/100000: episode: 262, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52600/100000: episode: 263, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 52800/100000: episode: 264, duration: 1.196s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53000/100000: episode: 265, duration: 1.195s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53200/100000: episode: 266, duration: 1.201s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53400/100000: episode: 267, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53600/100000: episode: 268, duration: 1.475s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 53800/100000: episode: 269, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54000/100000: episode: 270, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54200/100000: episode: 271, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54400/100000: episode: 272, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54600/100000: episode: 273, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 54800/100000: episode: 274, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55000/100000: episode: 275, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55200/100000: episode: 276, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55400/100000: episode: 277, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55600/100000: episode: 278, duration: 1.816s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 55800/100000: episode: 279, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56000/100000: episode: 280, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56200/100000: episode: 281, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56400/100000: episode: 282, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56600/100000: episode: 283, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 56800/100000: episode: 284, duration: 1.216s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57000/100000: episode: 285, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57200/100000: episode: 286, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57400/100000: episode: 287, duration: 1.694s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57600/100000: episode: 288, duration: 1.559s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 57800/100000: episode: 289, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58000/100000: episode: 290, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58200/100000: episode: 291, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58400/100000: episode: 292, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58600/100000: episode: 293, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 58800/100000: episode: 294, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59000/100000: episode: 295, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59200/100000: episode: 296, duration: 1.548s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59400/100000: episode: 297, duration: 1.649s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59600/100000: episode: 298, duration: 1.196s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 59800/100000: episode: 299, duration: 1.201s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60000/100000: episode: 300, duration: 1.199s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60200/100000: episode: 301, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60400/100000: episode: 302, duration: 1.209s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60600/100000: episode: 303, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 60800/100000: episode: 304, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61000/100000: episode: 305, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61200/100000: episode: 306, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61400/100000: episode: 307, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61600/100000: episode: 308, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 61800/100000: episode: 309, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62000/100000: episode: 310, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62200/100000: episode: 311, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62400/100000: episode: 312, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62600/100000: episode: 313, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 62800/100000: episode: 314, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63000/100000: episode: 315, duration: 1.695s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63200/100000: episode: 316, duration: 1.646s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63400/100000: episode: 317, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63600/100000: episode: 318, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 63800/100000: episode: 319, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64000/100000: episode: 320, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64200/100000: episode: 321, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64400/100000: episode: 322, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64600/100000: episode: 323, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 64800/100000: episode: 324, duration: 1.554s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65000/100000: episode: 325, duration: 1.633s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65200/100000: episode: 326, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65400/100000: episode: 327, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65600/100000: episode: 328, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 65800/100000: episode: 329, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66000/100000: episode: 330, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66200/100000: episode: 331, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66400/100000: episode: 332, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66600/100000: episode: 333, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 66800/100000: episode: 334, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67000/100000: episode: 335, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67200/100000: episode: 336, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67400/100000: episode: 337, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67600/100000: episode: 338, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 67800/100000: episode: 339, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68000/100000: episode: 340, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68200/100000: episode: 341, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68400/100000: episode: 342, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68600/100000: episode: 343, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 68800/100000: episode: 344, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69000/100000: episode: 345, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69200/100000: episode: 346, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69400/100000: episode: 347, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69600/100000: episode: 348, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 69800/100000: episode: 349, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70000/100000: episode: 350, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70200/100000: episode: 351, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70400/100000: episode: 352, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70600/100000: episode: 353, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 70800/100000: episode: 354, duration: 1.203s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71000/100000: episode: 355, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71200/100000: episode: 356, duration: 1.214s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71400/100000: episode: 357, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71600/100000: episode: 358, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 71800/100000: episode: 359, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72000/100000: episode: 360, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72200/100000: episode: 361, duration: 1.449s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72400/100000: episode: 362, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72600/100000: episode: 363, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 72800/100000: episode: 364, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73000/100000: episode: 365, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73200/100000: episode: 366, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73400/100000: episode: 367, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73600/100000: episode: 368, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 73800/100000: episode: 369, duration: 1.211s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74000/100000: episode: 370, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74200/100000: episode: 371, duration: 1.805s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74400/100000: episode: 372, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74600/100000: episode: 373, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 74800/100000: episode: 374, duration: 1.197s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75000/100000: episode: 375, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75200/100000: episode: 376, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75400/100000: episode: 377, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75600/100000: episode: 378, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 75800/100000: episode: 379, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76000/100000: episode: 380, duration: 1.581s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76200/100000: episode: 381, duration: 1.699s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76400/100000: episode: 382, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76600/100000: episode: 383, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 76800/100000: episode: 384, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77000/100000: episode: 385, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77200/100000: episode: 386, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77400/100000: episode: 387, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77600/100000: episode: 388, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 77800/100000: episode: 389, duration: 1.506s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78000/100000: episode: 390, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78200/100000: episode: 391, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78400/100000: episode: 392, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78600/100000: episode: 393, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 78800/100000: episode: 394, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79000/100000: episode: 395, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79200/100000: episode: 396, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79400/100000: episode: 397, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79600/100000: episode: 398, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 79800/100000: episode: 399, duration: 1.754s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80000/100000: episode: 400, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80200/100000: episode: 401, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80400/100000: episode: 402, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80600/100000: episode: 403, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 80800/100000: episode: 404, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81000/100000: episode: 405, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81200/100000: episode: 406, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81400/100000: episode: 407, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81600/100000: episode: 408, duration: 1.672s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 81800/100000: episode: 409, duration: 1.558s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82000/100000: episode: 410, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82200/100000: episode: 411, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82400/100000: episode: 412, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82600/100000: episode: 413, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 82800/100000: episode: 414, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83000/100000: episode: 415, duration: 1.210s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83200/100000: episode: 416, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83400/100000: episode: 417, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83600/100000: episode: 418, duration: 1.727s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 83800/100000: episode: 419, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84000/100000: episode: 420, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84200/100000: episode: 421, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84400/100000: episode: 422, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84600/100000: episode: 423, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 84800/100000: episode: 424, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85000/100000: episode: 425, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85200/100000: episode: 426, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85400/100000: episode: 427, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85600/100000: episode: 428, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 85800/100000: episode: 429, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86000/100000: episode: 430, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86200/100000: episode: 431, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86400/100000: episode: 432, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86600/100000: episode: 433, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 86800/100000: episode: 434, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87000/100000: episode: 435, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87200/100000: episode: 436, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87400/100000: episode: 437, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87600/100000: episode: 438, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 87800/100000: episode: 439, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88000/100000: episode: 440, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88200/100000: episode: 441, duration: 1.193s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88400/100000: episode: 442, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88600/100000: episode: 443, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 88800/100000: episode: 444, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89000/100000: episode: 445, duration: 1.572s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89200/100000: episode: 446, duration: 1.613s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89400/100000: episode: 447, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89600/100000: episode: 448, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 89800/100000: episode: 449, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90000/100000: episode: 450, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90200/100000: episode: 451, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90400/100000: episode: 452, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90600/100000: episode: 453, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 90800/100000: episode: 454, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91000/100000: episode: 455, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91200/100000: episode: 456, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91400/100000: episode: 457, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91600/100000: episode: 458, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 91800/100000: episode: 459, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92000/100000: episode: 460, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92200/100000: episode: 461, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92400/100000: episode: 462, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92600/100000: episode: 463, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 92800/100000: episode: 464, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93000/100000: episode: 465, duration: 1.488s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93200/100000: episode: 466, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93400/100000: episode: 467, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93600/100000: episode: 468, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 93800/100000: episode: 469, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94000/100000: episode: 470, duration: 1.195s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94200/100000: episode: 471, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94400/100000: episode: 472, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94600/100000: episode: 473, duration: 1.607s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 94800/100000: episode: 474, duration: 1.682s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95000/100000: episode: 475, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95200/100000: episode: 476, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95400/100000: episode: 477, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95600/100000: episode: 478, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 95800/100000: episode: 479, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96000/100000: episode: 480, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96200/100000: episode: 481, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96400/100000: episode: 482, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96600/100000: episode: 483, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 96800/100000: episode: 484, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97000/100000: episode: 485, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97200/100000: episode: 486, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97400/100000: episode: 487, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97600/100000: episode: 488, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 97800/100000: episode: 489, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98000/100000: episode: 490, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98200/100000: episode: 491, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98400/100000: episode: 492, duration: 1.696s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98600/100000: episode: 493, duration: 1.525s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 98800/100000: episode: 494, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99000/100000: episode: 495, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99200/100000: episode: 496, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99400/100000: episode: 497, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99600/100000: episode: 498, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 99800/100000: episode: 499, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100000/100000: episode: 500, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "done, took 675.410 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de construcción de una red neuronal y entrenamiento con SARSA\n",
        "\n",
        "########################## Creación del Ambiente ##########################\n",
        "env = gym.make('MountainCar-v0') #crea el ambiente utilizando gym.\n",
        "np.random.seed(613) #semilla aleatoria\n",
        "env.seed(613)\n",
        "nb_actions = env.action_space.n #obtiene la cantidad de acciones (3)\n",
        "\n",
        "###################### Creación de la Red Neuronal ########################\n",
        "# Modelo secuencial, una capa después de otra\n",
        "model = Sequential()\n",
        "\n",
        "#Capa de Entrada\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "#Capas \"Escondidas\". Aquí se pueden añadir varias capas con distinta cantidad de neuronas\n",
        "model.add(Dense(16,activation='relu'))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "\n",
        "#Capa de Salida\n",
        "model.add(Dense(nb_actions,activation='relu'))\n",
        "\n",
        "print(model.summary()) #muestra las neuronas y parámetros de la red\n",
        "\n",
        "###################### Configuración del Modelo ##########################\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1) #límite de la memoria\n",
        "policy = BoltzmannQPolicy() #política inicial\n",
        "\n",
        "#se crea el agente de SARSA\n",
        "sarsa_solucion_3 = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=100, policy=policy)\n",
        "sarsa_solucion_3.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "\n",
        "model.save_weights('model_sarsa_solucion_3_valoresiniciales.h5') #guarda los valores iniciales\n",
        "\n",
        "############################ Entrenamiento ###############################\n",
        "#Se entrena el agente por 10,000 pasos de tiempo y se guardan los resultados\n",
        "\n",
        "#carga los pesos iniciales para entrenar desde 0 (buena practica)\n",
        "sarsa_solucion_3.load_weights('model_sarsa_solucion_3_valoresiniciales.h5')\n",
        "\n",
        "#Comienza a entrenar\n",
        "#Guarda el historial y muestra progreso en pantalla\n",
        "historial_solucion_sarsa_ejemplo_3 = sarsa_solucion_3.fit(env, nb_steps=600000, visualize=False, verbose=2)\n",
        "\n",
        "#guarda los pesos finales (buena practica)\n",
        "sarsa_solucion_3.save_weights('model_sarsa_solucion_3_valoresfinales.h5f', overwrite=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACZlZaWI6JOx",
        "outputId": "9851c486-e68f-49dd-8df6-bcb4f7fb637f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_9 (Flatten)         (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 16)                48        \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 3)                 27        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 211 (844.00 Byte)\n",
            "Trainable params: 211 (844.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 600000 steps ...\n",
            "    200/600000: episode: 1, duration: 2.422s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500237, mae: 0.333496, mean_q: 0.000693\n",
            "    400/600000: episode: 2, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "    600/600000: episode: 3, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "    800/600000: episode: 4, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1000/600000: episode: 5, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1200/600000: episode: 6, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1400/600000: episode: 7, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1600/600000: episode: 8, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1800/600000: episode: 9, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2000/600000: episode: 10, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2200/600000: episode: 11, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2400/600000: episode: 12, duration: 3.069s, episode steps: 200, steps per second:  65, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2600/600000: episode: 13, duration: 2.305s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2800/600000: episode: 14, duration: 2.773s, episode steps: 200, steps per second:  72, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3000/600000: episode: 15, duration: 2.315s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3200/600000: episode: 16, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3400/600000: episode: 17, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3600/600000: episode: 18, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3800/600000: episode: 19, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4000/600000: episode: 20, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4200/600000: episode: 21, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4400/600000: episode: 22, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4600/600000: episode: 23, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4800/600000: episode: 24, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5000/600000: episode: 25, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5200/600000: episode: 26, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5400/600000: episode: 27, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5600/600000: episode: 28, duration: 1.476s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5800/600000: episode: 29, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6000/600000: episode: 30, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6200/600000: episode: 31, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6400/600000: episode: 32, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6600/600000: episode: 33, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6800/600000: episode: 34, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7000/600000: episode: 35, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7200/600000: episode: 36, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7400/600000: episode: 37, duration: 1.529s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7600/600000: episode: 38, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7800/600000: episode: 39, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8000/600000: episode: 40, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8200/600000: episode: 41, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8400/600000: episode: 42, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8600/600000: episode: 43, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8800/600000: episode: 44, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9000/600000: episode: 45, duration: 1.737s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9200/600000: episode: 46, duration: 1.527s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9400/600000: episode: 47, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9600/600000: episode: 48, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9800/600000: episode: 49, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10000/600000: episode: 50, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10200/600000: episode: 51, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10400/600000: episode: 52, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10600/600000: episode: 53, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10800/600000: episode: 54, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11000/600000: episode: 55, duration: 1.638s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11200/600000: episode: 56, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11400/600000: episode: 57, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11600/600000: episode: 58, duration: 1.208s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11800/600000: episode: 59, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12000/600000: episode: 60, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12200/600000: episode: 61, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12400/600000: episode: 62, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12600/600000: episode: 63, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12800/600000: episode: 64, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13000/600000: episode: 65, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13200/600000: episode: 66, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13400/600000: episode: 67, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13600/600000: episode: 68, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13800/600000: episode: 69, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14000/600000: episode: 70, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14200/600000: episode: 71, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14400/600000: episode: 72, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14600/600000: episode: 73, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14800/600000: episode: 74, duration: 1.465s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15000/600000: episode: 75, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15200/600000: episode: 76, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15400/600000: episode: 77, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15600/600000: episode: 78, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15800/600000: episode: 79, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16000/600000: episode: 80, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16200/600000: episode: 81, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16400/600000: episode: 82, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16600/600000: episode: 83, duration: 1.763s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16800/600000: episode: 84, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17000/600000: episode: 85, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17200/600000: episode: 86, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17400/600000: episode: 87, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17600/600000: episode: 88, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17800/600000: episode: 89, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18000/600000: episode: 90, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18200/600000: episode: 91, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18400/600000: episode: 92, duration: 1.717s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18600/600000: episode: 93, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18800/600000: episode: 94, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19000/600000: episode: 95, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19200/600000: episode: 96, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19400/600000: episode: 97, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19600/600000: episode: 98, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19800/600000: episode: 99, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20000/600000: episode: 100, duration: 1.480s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20200/600000: episode: 101, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20400/600000: episode: 102, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20600/600000: episode: 103, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20800/600000: episode: 104, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21000/600000: episode: 105, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21200/600000: episode: 106, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21400/600000: episode: 107, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21600/600000: episode: 108, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21800/600000: episode: 109, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22000/600000: episode: 110, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22200/600000: episode: 111, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22400/600000: episode: 112, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22600/600000: episode: 113, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22800/600000: episode: 114, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23000/600000: episode: 115, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23200/600000: episode: 116, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23400/600000: episode: 117, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23600/600000: episode: 118, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23800/600000: episode: 119, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24000/600000: episode: 120, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24200/600000: episode: 121, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24400/600000: episode: 122, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24600/600000: episode: 123, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24800/600000: episode: 124, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25000/600000: episode: 125, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25200/600000: episode: 126, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25400/600000: episode: 127, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25600/600000: episode: 128, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25800/600000: episode: 129, duration: 1.607s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26000/600000: episode: 130, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26200/600000: episode: 131, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26400/600000: episode: 132, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26600/600000: episode: 133, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26800/600000: episode: 134, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27000/600000: episode: 135, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27200/600000: episode: 136, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27400/600000: episode: 137, duration: 1.559s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27600/600000: episode: 138, duration: 1.738s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27800/600000: episode: 139, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28000/600000: episode: 140, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28200/600000: episode: 141, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28400/600000: episode: 142, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28600/600000: episode: 143, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28800/600000: episode: 144, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29000/600000: episode: 145, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29200/600000: episode: 146, duration: 1.384s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29400/600000: episode: 147, duration: 1.793s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29600/600000: episode: 148, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29800/600000: episode: 149, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30000/600000: episode: 150, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30200/600000: episode: 151, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30400/600000: episode: 152, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30600/600000: episode: 153, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30800/600000: episode: 154, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31000/600000: episode: 155, duration: 1.432s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31200/600000: episode: 156, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31400/600000: episode: 157, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31600/600000: episode: 158, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31800/600000: episode: 159, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32000/600000: episode: 160, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32200/600000: episode: 161, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32400/600000: episode: 162, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32600/600000: episode: 163, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32800/600000: episode: 164, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33000/600000: episode: 165, duration: 1.802s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33200/600000: episode: 166, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33400/600000: episode: 167, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33600/600000: episode: 168, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33800/600000: episode: 169, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34000/600000: episode: 170, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34200/600000: episode: 171, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34400/600000: episode: 172, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34600/600000: episode: 173, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34800/600000: episode: 174, duration: 1.797s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35000/600000: episode: 175, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35200/600000: episode: 176, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35400/600000: episode: 177, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35600/600000: episode: 178, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35800/600000: episode: 179, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36000/600000: episode: 180, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36200/600000: episode: 181, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36400/600000: episode: 182, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36600/600000: episode: 183, duration: 1.766s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36800/600000: episode: 184, duration: 1.568s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37000/600000: episode: 185, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37200/600000: episode: 186, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37400/600000: episode: 187, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37600/600000: episode: 188, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37800/600000: episode: 189, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38000/600000: episode: 190, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38200/600000: episode: 191, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38400/600000: episode: 192, duration: 1.649s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38600/600000: episode: 193, duration: 1.673s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38800/600000: episode: 194, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39000/600000: episode: 195, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39200/600000: episode: 196, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39400/600000: episode: 197, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39600/600000: episode: 198, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39800/600000: episode: 199, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40000/600000: episode: 200, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40200/600000: episode: 201, duration: 1.631s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40400/600000: episode: 202, duration: 1.683s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40600/600000: episode: 203, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40800/600000: episode: 204, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41000/600000: episode: 205, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41200/600000: episode: 206, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41400/600000: episode: 207, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41600/600000: episode: 208, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41800/600000: episode: 209, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42000/600000: episode: 210, duration: 1.483s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42200/600000: episode: 211, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42400/600000: episode: 212, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42600/600000: episode: 213, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42800/600000: episode: 214, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43000/600000: episode: 215, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43200/600000: episode: 216, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43400/600000: episode: 217, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43600/600000: episode: 218, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43800/600000: episode: 219, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44000/600000: episode: 220, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44200/600000: episode: 221, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44400/600000: episode: 222, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44600/600000: episode: 223, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44800/600000: episode: 224, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45000/600000: episode: 225, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45200/600000: episode: 226, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45400/600000: episode: 227, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45600/600000: episode: 228, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45800/600000: episode: 229, duration: 1.750s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46000/600000: episode: 230, duration: 1.560s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46200/600000: episode: 231, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46400/600000: episode: 232, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46600/600000: episode: 233, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46800/600000: episode: 234, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47000/600000: episode: 235, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47200/600000: episode: 236, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47400/600000: episode: 237, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47600/600000: episode: 238, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47800/600000: episode: 239, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48000/600000: episode: 240, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48200/600000: episode: 241, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48400/600000: episode: 242, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48600/600000: episode: 243, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48800/600000: episode: 244, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49000/600000: episode: 245, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49200/600000: episode: 246, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49400/600000: episode: 247, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49600/600000: episode: 248, duration: 1.782s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49800/600000: episode: 249, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50000/600000: episode: 250, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50200/600000: episode: 251, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50400/600000: episode: 252, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50600/600000: episode: 253, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50800/600000: episode: 254, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51000/600000: episode: 255, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51200/600000: episode: 256, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51400/600000: episode: 257, duration: 1.705s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51600/600000: episode: 258, duration: 1.555s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51800/600000: episode: 259, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52000/600000: episode: 260, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52200/600000: episode: 261, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52400/600000: episode: 262, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52600/600000: episode: 263, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52800/600000: episode: 264, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53000/600000: episode: 265, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53200/600000: episode: 266, duration: 1.590s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53400/600000: episode: 267, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53600/600000: episode: 268, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53800/600000: episode: 269, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54000/600000: episode: 270, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54200/600000: episode: 271, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54400/600000: episode: 272, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54600/600000: episode: 273, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54800/600000: episode: 274, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55000/600000: episode: 275, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55200/600000: episode: 276, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55400/600000: episode: 277, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55600/600000: episode: 278, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55800/600000: episode: 279, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56000/600000: episode: 280, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56200/600000: episode: 281, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56400/600000: episode: 282, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56600/600000: episode: 283, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56800/600000: episode: 284, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57000/600000: episode: 285, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57200/600000: episode: 286, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57400/600000: episode: 287, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57600/600000: episode: 288, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57800/600000: episode: 289, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58000/600000: episode: 290, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58200/600000: episode: 291, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58400/600000: episode: 292, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58600/600000: episode: 293, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58800/600000: episode: 294, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59000/600000: episode: 295, duration: 1.519s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59200/600000: episode: 296, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59400/600000: episode: 297, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59600/600000: episode: 298, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59800/600000: episode: 299, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60000/600000: episode: 300, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60200/600000: episode: 301, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60400/600000: episode: 302, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60600/600000: episode: 303, duration: 1.613s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60800/600000: episode: 304, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61000/600000: episode: 305, duration: 1.972s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61200/600000: episode: 306, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61400/600000: episode: 307, duration: 1.480s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61600/600000: episode: 308, duration: 1.442s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61800/600000: episode: 309, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62000/600000: episode: 310, duration: 1.626s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62200/600000: episode: 311, duration: 1.936s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62400/600000: episode: 312, duration: 1.329s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62600/600000: episode: 313, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62800/600000: episode: 314, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63000/600000: episode: 315, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63200/600000: episode: 316, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63400/600000: episode: 317, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63600/600000: episode: 318, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63800/600000: episode: 319, duration: 1.579s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64000/600000: episode: 320, duration: 1.736s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64200/600000: episode: 321, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64400/600000: episode: 322, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64600/600000: episode: 323, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64800/600000: episode: 324, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65000/600000: episode: 325, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65200/600000: episode: 326, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65400/600000: episode: 327, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65600/600000: episode: 328, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65800/600000: episode: 329, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66000/600000: episode: 330, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66200/600000: episode: 331, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66400/600000: episode: 332, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66600/600000: episode: 333, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66800/600000: episode: 334, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67000/600000: episode: 335, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67200/600000: episode: 336, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67400/600000: episode: 337, duration: 1.538s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67600/600000: episode: 338, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67800/600000: episode: 339, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68000/600000: episode: 340, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68200/600000: episode: 341, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68400/600000: episode: 342, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68600/600000: episode: 343, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68800/600000: episode: 344, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69000/600000: episode: 345, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69200/600000: episode: 346, duration: 1.365s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69400/600000: episode: 347, duration: 1.810s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69600/600000: episode: 348, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69800/600000: episode: 349, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70000/600000: episode: 350, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70200/600000: episode: 351, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70400/600000: episode: 352, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70600/600000: episode: 353, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70800/600000: episode: 354, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71000/600000: episode: 355, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71200/600000: episode: 356, duration: 1.728s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71400/600000: episode: 357, duration: 1.553s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71600/600000: episode: 358, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71800/600000: episode: 359, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72000/600000: episode: 360, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72200/600000: episode: 361, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72400/600000: episode: 362, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72600/600000: episode: 363, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72800/600000: episode: 364, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73000/600000: episode: 365, duration: 1.584s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73200/600000: episode: 366, duration: 1.668s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73400/600000: episode: 367, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73600/600000: episode: 368, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73800/600000: episode: 369, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74000/600000: episode: 370, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74200/600000: episode: 371, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74400/600000: episode: 372, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74600/600000: episode: 373, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74800/600000: episode: 374, duration: 1.444s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75000/600000: episode: 375, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75200/600000: episode: 376, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75400/600000: episode: 377, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75600/600000: episode: 378, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75800/600000: episode: 379, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76000/600000: episode: 380, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76200/600000: episode: 381, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76400/600000: episode: 382, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76600/600000: episode: 383, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76800/600000: episode: 384, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77000/600000: episode: 385, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77200/600000: episode: 386, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77400/600000: episode: 387, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77600/600000: episode: 388, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77800/600000: episode: 389, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78000/600000: episode: 390, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78200/600000: episode: 391, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78400/600000: episode: 392, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78600/600000: episode: 393, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78800/600000: episode: 394, duration: 1.482s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79000/600000: episode: 395, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79200/600000: episode: 396, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79400/600000: episode: 397, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79600/600000: episode: 398, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79800/600000: episode: 399, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80000/600000: episode: 400, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80200/600000: episode: 401, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80400/600000: episode: 402, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80600/600000: episode: 403, duration: 1.672s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80800/600000: episode: 404, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81000/600000: episode: 405, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81200/600000: episode: 406, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81400/600000: episode: 407, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81600/600000: episode: 408, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81800/600000: episode: 409, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82000/600000: episode: 410, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82200/600000: episode: 411, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82400/600000: episode: 412, duration: 1.839s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82600/600000: episode: 413, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82800/600000: episode: 414, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83000/600000: episode: 415, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83200/600000: episode: 416, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83400/600000: episode: 417, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83600/600000: episode: 418, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83800/600000: episode: 419, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84000/600000: episode: 420, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84200/600000: episode: 421, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84400/600000: episode: 422, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84600/600000: episode: 423, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84800/600000: episode: 424, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85000/600000: episode: 425, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85200/600000: episode: 426, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85400/600000: episode: 427, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85600/600000: episode: 428, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85800/600000: episode: 429, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86000/600000: episode: 430, duration: 1.677s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86200/600000: episode: 431, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86400/600000: episode: 432, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86600/600000: episode: 433, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86800/600000: episode: 434, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87000/600000: episode: 435, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87200/600000: episode: 436, duration: 1.238s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87400/600000: episode: 437, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87600/600000: episode: 438, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87800/600000: episode: 439, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88000/600000: episode: 440, duration: 1.730s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88200/600000: episode: 441, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88400/600000: episode: 442, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88600/600000: episode: 443, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88800/600000: episode: 444, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89000/600000: episode: 445, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89200/600000: episode: 446, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89400/600000: episode: 447, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89600/600000: episode: 448, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89800/600000: episode: 449, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90000/600000: episode: 450, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90200/600000: episode: 451, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90400/600000: episode: 452, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90600/600000: episode: 453, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90800/600000: episode: 454, duration: 1.207s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91000/600000: episode: 455, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91200/600000: episode: 456, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91400/600000: episode: 457, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91600/600000: episode: 458, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91800/600000: episode: 459, duration: 1.584s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92000/600000: episode: 460, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92200/600000: episode: 461, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92400/600000: episode: 462, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92600/600000: episode: 463, duration: 1.206s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92800/600000: episode: 464, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93000/600000: episode: 465, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93200/600000: episode: 466, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93400/600000: episode: 467, duration: 1.547s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93600/600000: episode: 468, duration: 1.712s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93800/600000: episode: 469, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94000/600000: episode: 470, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94200/600000: episode: 471, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94400/600000: episode: 472, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94600/600000: episode: 473, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94800/600000: episode: 474, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95000/600000: episode: 475, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95200/600000: episode: 476, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95400/600000: episode: 477, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95600/600000: episode: 478, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95800/600000: episode: 479, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96000/600000: episode: 480, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96200/600000: episode: 481, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96400/600000: episode: 482, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96600/600000: episode: 483, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96800/600000: episode: 484, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97000/600000: episode: 485, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97200/600000: episode: 486, duration: 1.732s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97400/600000: episode: 487, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97600/600000: episode: 488, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97800/600000: episode: 489, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98000/600000: episode: 490, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98200/600000: episode: 491, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98400/600000: episode: 492, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98600/600000: episode: 493, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98800/600000: episode: 494, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99000/600000: episode: 495, duration: 1.639s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99200/600000: episode: 496, duration: 1.648s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99400/600000: episode: 497, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99600/600000: episode: 498, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99800/600000: episode: 499, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100000/600000: episode: 500, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100200/600000: episode: 501, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100400/600000: episode: 502, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100600/600000: episode: 503, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100800/600000: episode: 504, duration: 1.413s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101000/600000: episode: 505, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101200/600000: episode: 506, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101400/600000: episode: 507, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101600/600000: episode: 508, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101800/600000: episode: 509, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102000/600000: episode: 510, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102200/600000: episode: 511, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102400/600000: episode: 512, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102600/600000: episode: 513, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102800/600000: episode: 514, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103000/600000: episode: 515, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103200/600000: episode: 516, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103400/600000: episode: 517, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103600/600000: episode: 518, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103800/600000: episode: 519, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104000/600000: episode: 520, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104200/600000: episode: 521, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104400/600000: episode: 522, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104600/600000: episode: 523, duration: 1.617s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104800/600000: episode: 524, duration: 1.623s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105000/600000: episode: 525, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105200/600000: episode: 526, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105400/600000: episode: 527, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105600/600000: episode: 528, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105800/600000: episode: 529, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106000/600000: episode: 530, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106200/600000: episode: 531, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106400/600000: episode: 532, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106600/600000: episode: 533, duration: 1.764s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106800/600000: episode: 534, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107000/600000: episode: 535, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107200/600000: episode: 536, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107400/600000: episode: 537, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107600/600000: episode: 538, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107800/600000: episode: 539, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108000/600000: episode: 540, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108200/600000: episode: 541, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108400/600000: episode: 542, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108600/600000: episode: 543, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108800/600000: episode: 544, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109000/600000: episode: 545, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109200/600000: episode: 546, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109400/600000: episode: 547, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109600/600000: episode: 548, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109800/600000: episode: 549, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110000/600000: episode: 550, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110200/600000: episode: 551, duration: 1.723s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110400/600000: episode: 552, duration: 1.612s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110600/600000: episode: 553, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110800/600000: episode: 554, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111000/600000: episode: 555, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111200/600000: episode: 556, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111400/600000: episode: 557, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111600/600000: episode: 558, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111800/600000: episode: 559, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112000/600000: episode: 560, duration: 1.592s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112200/600000: episode: 561, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112400/600000: episode: 562, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112600/600000: episode: 563, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112800/600000: episode: 564, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113000/600000: episode: 565, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113200/600000: episode: 566, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113400/600000: episode: 567, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113600/600000: episode: 568, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113800/600000: episode: 569, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114000/600000: episode: 570, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114200/600000: episode: 571, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114400/600000: episode: 572, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114600/600000: episode: 573, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114800/600000: episode: 574, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115000/600000: episode: 575, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115200/600000: episode: 576, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115400/600000: episode: 577, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115600/600000: episode: 578, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115800/600000: episode: 579, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116000/600000: episode: 580, duration: 1.448s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116200/600000: episode: 581, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116400/600000: episode: 582, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116600/600000: episode: 583, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116800/600000: episode: 584, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117000/600000: episode: 585, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117200/600000: episode: 586, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117400/600000: episode: 587, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117600/600000: episode: 588, duration: 1.569s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117800/600000: episode: 589, duration: 1.633s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118000/600000: episode: 590, duration: 1.190s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118200/600000: episode: 591, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118400/600000: episode: 592, duration: 1.195s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118600/600000: episode: 593, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118800/600000: episode: 594, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119000/600000: episode: 595, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119200/600000: episode: 596, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119400/600000: episode: 597, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119600/600000: episode: 598, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119800/600000: episode: 599, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120000/600000: episode: 600, duration: 1.208s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120200/600000: episode: 601, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120400/600000: episode: 602, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120600/600000: episode: 603, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120800/600000: episode: 604, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121000/600000: episode: 605, duration: 1.210s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121200/600000: episode: 606, duration: 1.202s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121400/600000: episode: 607, duration: 1.641s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121600/600000: episode: 608, duration: 1.560s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121800/600000: episode: 609, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122000/600000: episode: 610, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122200/600000: episode: 611, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122400/600000: episode: 612, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122600/600000: episode: 613, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122800/600000: episode: 614, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123000/600000: episode: 615, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123200/600000: episode: 616, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123400/600000: episode: 617, duration: 1.843s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123600/600000: episode: 618, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123800/600000: episode: 619, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124000/600000: episode: 620, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124200/600000: episode: 621, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124400/600000: episode: 622, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124600/600000: episode: 623, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124800/600000: episode: 624, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125000/600000: episode: 625, duration: 1.486s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125200/600000: episode: 626, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125400/600000: episode: 627, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125600/600000: episode: 628, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125800/600000: episode: 629, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126000/600000: episode: 630, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126200/600000: episode: 631, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126400/600000: episode: 632, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126600/600000: episode: 633, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126800/600000: episode: 634, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127000/600000: episode: 635, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127200/600000: episode: 636, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127400/600000: episode: 637, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127600/600000: episode: 638, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127800/600000: episode: 639, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128000/600000: episode: 640, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128200/600000: episode: 641, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128400/600000: episode: 642, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128600/600000: episode: 643, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128800/600000: episode: 644, duration: 1.724s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129000/600000: episode: 645, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129200/600000: episode: 646, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129400/600000: episode: 647, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129600/600000: episode: 648, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129800/600000: episode: 649, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130000/600000: episode: 650, duration: 1.286s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130200/600000: episode: 651, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130400/600000: episode: 652, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130600/600000: episode: 653, duration: 1.612s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130800/600000: episode: 654, duration: 1.617s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131000/600000: episode: 655, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131200/600000: episode: 656, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131400/600000: episode: 657, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131600/600000: episode: 658, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131800/600000: episode: 659, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132000/600000: episode: 660, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132200/600000: episode: 661, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132400/600000: episode: 662, duration: 1.418s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132600/600000: episode: 663, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132800/600000: episode: 664, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133000/600000: episode: 665, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133200/600000: episode: 666, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133400/600000: episode: 667, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133600/600000: episode: 668, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133800/600000: episode: 669, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134000/600000: episode: 670, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134200/600000: episode: 671, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134400/600000: episode: 672, duration: 1.810s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134600/600000: episode: 673, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134800/600000: episode: 674, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135000/600000: episode: 675, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135200/600000: episode: 676, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135400/600000: episode: 677, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135600/600000: episode: 678, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135800/600000: episode: 679, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136000/600000: episode: 680, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136200/600000: episode: 681, duration: 1.709s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136400/600000: episode: 682, duration: 1.544s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136600/600000: episode: 683, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136800/600000: episode: 684, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137000/600000: episode: 685, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137200/600000: episode: 686, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137400/600000: episode: 687, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137600/600000: episode: 688, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137800/600000: episode: 689, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138000/600000: episode: 690, duration: 1.555s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138200/600000: episode: 691, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138400/600000: episode: 692, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138600/600000: episode: 693, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138800/600000: episode: 694, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139000/600000: episode: 695, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139200/600000: episode: 696, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139400/600000: episode: 697, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139600/600000: episode: 698, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139800/600000: episode: 699, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140000/600000: episode: 700, duration: 1.773s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140200/600000: episode: 701, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140400/600000: episode: 702, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140600/600000: episode: 703, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140800/600000: episode: 704, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141000/600000: episode: 705, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141200/600000: episode: 706, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141400/600000: episode: 707, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141600/600000: episode: 708, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141800/600000: episode: 709, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142000/600000: episode: 710, duration: 1.588s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142200/600000: episode: 711, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142400/600000: episode: 712, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142600/600000: episode: 713, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142800/600000: episode: 714, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143000/600000: episode: 715, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143200/600000: episode: 716, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143400/600000: episode: 717, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143600/600000: episode: 718, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143800/600000: episode: 719, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144000/600000: episode: 720, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144200/600000: episode: 721, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144400/600000: episode: 722, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144600/600000: episode: 723, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144800/600000: episode: 724, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145000/600000: episode: 725, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145200/600000: episode: 726, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145400/600000: episode: 727, duration: 1.648s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145600/600000: episode: 728, duration: 1.684s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145800/600000: episode: 729, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146000/600000: episode: 730, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146200/600000: episode: 731, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146400/600000: episode: 732, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146600/600000: episode: 733, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146800/600000: episode: 734, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147000/600000: episode: 735, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147200/600000: episode: 736, duration: 1.423s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147400/600000: episode: 737, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147600/600000: episode: 738, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147800/600000: episode: 739, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148000/600000: episode: 740, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148200/600000: episode: 741, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148400/600000: episode: 742, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148600/600000: episode: 743, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148800/600000: episode: 744, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149000/600000: episode: 745, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149200/600000: episode: 746, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149400/600000: episode: 747, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149600/600000: episode: 748, duration: 1.201s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149800/600000: episode: 749, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150000/600000: episode: 750, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150200/600000: episode: 751, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150400/600000: episode: 752, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150600/600000: episode: 753, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150800/600000: episode: 754, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151000/600000: episode: 755, duration: 1.705s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151200/600000: episode: 756, duration: 1.526s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151400/600000: episode: 757, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151600/600000: episode: 758, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151800/600000: episode: 759, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152000/600000: episode: 760, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152200/600000: episode: 761, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152400/600000: episode: 762, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152600/600000: episode: 763, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152800/600000: episode: 764, duration: 1.565s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153000/600000: episode: 765, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153200/600000: episode: 766, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153400/600000: episode: 767, duration: 1.214s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153600/600000: episode: 768, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153800/600000: episode: 769, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154000/600000: episode: 770, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154200/600000: episode: 771, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154400/600000: episode: 772, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154600/600000: episode: 773, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154800/600000: episode: 774, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155000/600000: episode: 775, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155200/600000: episode: 776, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155400/600000: episode: 777, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155600/600000: episode: 778, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155800/600000: episode: 779, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156000/600000: episode: 780, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156200/600000: episode: 781, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156400/600000: episode: 782, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156600/600000: episode: 783, duration: 1.769s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156800/600000: episode: 784, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157000/600000: episode: 785, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157200/600000: episode: 786, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157400/600000: episode: 787, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157600/600000: episode: 788, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157800/600000: episode: 789, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158000/600000: episode: 790, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158200/600000: episode: 791, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158400/600000: episode: 792, duration: 1.680s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158600/600000: episode: 793, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158800/600000: episode: 794, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159000/600000: episode: 795, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159200/600000: episode: 796, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159400/600000: episode: 797, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159600/600000: episode: 798, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159800/600000: episode: 799, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160000/600000: episode: 800, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160200/600000: episode: 801, duration: 1.561s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160400/600000: episode: 802, duration: 1.679s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160600/600000: episode: 803, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160800/600000: episode: 804, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161000/600000: episode: 805, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161200/600000: episode: 806, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161400/600000: episode: 807, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161600/600000: episode: 808, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161800/600000: episode: 809, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162000/600000: episode: 810, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162200/600000: episode: 811, duration: 1.790s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162400/600000: episode: 812, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162600/600000: episode: 813, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162800/600000: episode: 814, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163000/600000: episode: 815, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163200/600000: episode: 816, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163400/600000: episode: 817, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163600/600000: episode: 818, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163800/600000: episode: 819, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164000/600000: episode: 820, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164200/600000: episode: 821, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164400/600000: episode: 822, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164600/600000: episode: 823, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164800/600000: episode: 824, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165000/600000: episode: 825, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165200/600000: episode: 826, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165400/600000: episode: 827, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165600/600000: episode: 828, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165800/600000: episode: 829, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166000/600000: episode: 830, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166200/600000: episode: 831, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166400/600000: episode: 832, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166600/600000: episode: 833, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166800/600000: episode: 834, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167000/600000: episode: 835, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167200/600000: episode: 836, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167400/600000: episode: 837, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167600/600000: episode: 838, duration: 1.665s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167800/600000: episode: 839, duration: 1.627s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168000/600000: episode: 840, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168200/600000: episode: 841, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168400/600000: episode: 842, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168600/600000: episode: 843, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168800/600000: episode: 844, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169000/600000: episode: 845, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169200/600000: episode: 846, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169400/600000: episode: 847, duration: 1.556s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169600/600000: episode: 848, duration: 1.717s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169800/600000: episode: 849, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170000/600000: episode: 850, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170200/600000: episode: 851, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170400/600000: episode: 852, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170600/600000: episode: 853, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170800/600000: episode: 854, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171000/600000: episode: 855, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171200/600000: episode: 856, duration: 1.513s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171400/600000: episode: 857, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171600/600000: episode: 858, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171800/600000: episode: 859, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172000/600000: episode: 860, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172200/600000: episode: 861, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172400/600000: episode: 862, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172600/600000: episode: 863, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172800/600000: episode: 864, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173000/600000: episode: 865, duration: 1.453s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173200/600000: episode: 866, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173400/600000: episode: 867, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173600/600000: episode: 868, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173800/600000: episode: 869, duration: 1.206s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174000/600000: episode: 870, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174200/600000: episode: 871, duration: 1.211s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174400/600000: episode: 872, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174600/600000: episode: 873, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174800/600000: episode: 874, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175000/600000: episode: 875, duration: 1.725s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175200/600000: episode: 876, duration: 1.543s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175400/600000: episode: 877, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175600/600000: episode: 878, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175800/600000: episode: 879, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176000/600000: episode: 880, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176200/600000: episode: 881, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176400/600000: episode: 882, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176600/600000: episode: 883, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176800/600000: episode: 884, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177000/600000: episode: 885, duration: 1.637s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177200/600000: episode: 886, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177400/600000: episode: 887, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177600/600000: episode: 888, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177800/600000: episode: 889, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178000/600000: episode: 890, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178200/600000: episode: 891, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178400/600000: episode: 892, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178600/600000: episode: 893, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178800/600000: episode: 894, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179000/600000: episode: 895, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179200/600000: episode: 896, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179400/600000: episode: 897, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179600/600000: episode: 898, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179800/600000: episode: 899, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180000/600000: episode: 900, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180200/600000: episode: 901, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180400/600000: episode: 902, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180600/600000: episode: 903, duration: 1.798s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180800/600000: episode: 904, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181000/600000: episode: 905, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181200/600000: episode: 906, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181400/600000: episode: 907, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181600/600000: episode: 908, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181800/600000: episode: 909, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182000/600000: episode: 910, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182200/600000: episode: 911, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182400/600000: episode: 912, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182600/600000: episode: 913, duration: 1.572s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182800/600000: episode: 914, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183000/600000: episode: 915, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183200/600000: episode: 916, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183400/600000: episode: 917, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183600/600000: episode: 918, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183800/600000: episode: 919, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184000/600000: episode: 920, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184200/600000: episode: 921, duration: 1.585s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184400/600000: episode: 922, duration: 1.665s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184600/600000: episode: 923, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184800/600000: episode: 924, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185000/600000: episode: 925, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185200/600000: episode: 926, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185400/600000: episode: 927, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185600/600000: episode: 928, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185800/600000: episode: 929, duration: 1.205s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186000/600000: episode: 930, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186200/600000: episode: 931, duration: 1.768s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186400/600000: episode: 932, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186600/600000: episode: 933, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186800/600000: episode: 934, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187000/600000: episode: 935, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187200/600000: episode: 936, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187400/600000: episode: 937, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187600/600000: episode: 938, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187800/600000: episode: 939, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188000/600000: episode: 940, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188200/600000: episode: 941, duration: 1.531s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188400/600000: episode: 942, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188600/600000: episode: 943, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188800/600000: episode: 944, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189000/600000: episode: 945, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189200/600000: episode: 946, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189400/600000: episode: 947, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189600/600000: episode: 948, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189800/600000: episode: 949, duration: 1.520s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190000/600000: episode: 950, duration: 1.688s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190200/600000: episode: 951, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190400/600000: episode: 952, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190600/600000: episode: 953, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190800/600000: episode: 954, duration: 1.199s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191000/600000: episode: 955, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191200/600000: episode: 956, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191400/600000: episode: 957, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191600/600000: episode: 958, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191800/600000: episode: 959, duration: 1.890s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192000/600000: episode: 960, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192200/600000: episode: 961, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192400/600000: episode: 962, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192600/600000: episode: 963, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192800/600000: episode: 964, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193000/600000: episode: 965, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193200/600000: episode: 966, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193400/600000: episode: 967, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193600/600000: episode: 968, duration: 1.778s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193800/600000: episode: 969, duration: 1.454s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194000/600000: episode: 970, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194200/600000: episode: 971, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194400/600000: episode: 972, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194600/600000: episode: 973, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194800/600000: episode: 974, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195000/600000: episode: 975, duration: 1.204s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195200/600000: episode: 976, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195400/600000: episode: 977, duration: 1.607s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195600/600000: episode: 978, duration: 1.736s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195800/600000: episode: 979, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196000/600000: episode: 980, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196200/600000: episode: 981, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196400/600000: episode: 982, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196600/600000: episode: 983, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196800/600000: episode: 984, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197000/600000: episode: 985, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197200/600000: episode: 986, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197400/600000: episode: 987, duration: 1.721s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197600/600000: episode: 988, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197800/600000: episode: 989, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198000/600000: episode: 990, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198200/600000: episode: 991, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198400/600000: episode: 992, duration: 1.209s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198600/600000: episode: 993, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198800/600000: episode: 994, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199000/600000: episode: 995, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199200/600000: episode: 996, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199400/600000: episode: 997, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199600/600000: episode: 998, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199800/600000: episode: 999, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200000/600000: episode: 1000, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200200/600000: episode: 1001, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200400/600000: episode: 1002, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200600/600000: episode: 1003, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200800/600000: episode: 1004, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201000/600000: episode: 1005, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201200/600000: episode: 1006, duration: 1.580s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201400/600000: episode: 1007, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201600/600000: episode: 1008, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201800/600000: episode: 1009, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202000/600000: episode: 1010, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202200/600000: episode: 1011, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202400/600000: episode: 1012, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202600/600000: episode: 1013, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202800/600000: episode: 1014, duration: 1.555s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203000/600000: episode: 1015, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203200/600000: episode: 1016, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203400/600000: episode: 1017, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203600/600000: episode: 1018, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203800/600000: episode: 1019, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204000/600000: episode: 1020, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204200/600000: episode: 1021, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204400/600000: episode: 1022, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204600/600000: episode: 1023, duration: 1.438s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204800/600000: episode: 1024, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205000/600000: episode: 1025, duration: 1.202s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205200/600000: episode: 1026, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205400/600000: episode: 1027, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205600/600000: episode: 1028, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205800/600000: episode: 1029, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206000/600000: episode: 1030, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206200/600000: episode: 1031, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206400/600000: episode: 1032, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206600/600000: episode: 1033, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206800/600000: episode: 1034, duration: 1.476s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207000/600000: episode: 1035, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207200/600000: episode: 1036, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207400/600000: episode: 1037, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207600/600000: episode: 1038, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207800/600000: episode: 1039, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208000/600000: episode: 1040, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208200/600000: episode: 1041, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208400/600000: episode: 1042, duration: 1.571s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208600/600000: episode: 1043, duration: 1.630s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208800/600000: episode: 1044, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209000/600000: episode: 1045, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209200/600000: episode: 1046, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209400/600000: episode: 1047, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209600/600000: episode: 1048, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209800/600000: episode: 1049, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210000/600000: episode: 1050, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210200/600000: episode: 1051, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210400/600000: episode: 1052, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210600/600000: episode: 1053, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210800/600000: episode: 1054, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211000/600000: episode: 1055, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211200/600000: episode: 1056, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211400/600000: episode: 1057, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211600/600000: episode: 1058, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211800/600000: episode: 1059, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212000/600000: episode: 1060, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212200/600000: episode: 1061, duration: 1.727s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212400/600000: episode: 1062, duration: 1.469s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212600/600000: episode: 1063, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212800/600000: episode: 1064, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213000/600000: episode: 1065, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213200/600000: episode: 1066, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213400/600000: episode: 1067, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213600/600000: episode: 1068, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213800/600000: episode: 1069, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214000/600000: episode: 1070, duration: 1.580s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214200/600000: episode: 1071, duration: 1.635s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214400/600000: episode: 1072, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214600/600000: episode: 1073, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214800/600000: episode: 1074, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215000/600000: episode: 1075, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215200/600000: episode: 1076, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215400/600000: episode: 1077, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215600/600000: episode: 1078, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215800/600000: episode: 1079, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216000/600000: episode: 1080, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216200/600000: episode: 1081, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216400/600000: episode: 1082, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216600/600000: episode: 1083, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216800/600000: episode: 1084, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217000/600000: episode: 1085, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217200/600000: episode: 1086, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217400/600000: episode: 1087, duration: 1.210s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217600/600000: episode: 1088, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217800/600000: episode: 1089, duration: 1.761s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218000/600000: episode: 1090, duration: 1.501s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218200/600000: episode: 1091, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218400/600000: episode: 1092, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218600/600000: episode: 1093, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218800/600000: episode: 1094, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219000/600000: episode: 1095, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219200/600000: episode: 1096, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219400/600000: episode: 1097, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219600/600000: episode: 1098, duration: 1.708s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219800/600000: episode: 1099, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220000/600000: episode: 1100, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220200/600000: episode: 1101, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220400/600000: episode: 1102, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220600/600000: episode: 1103, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220800/600000: episode: 1104, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221000/600000: episode: 1105, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221200/600000: episode: 1106, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221400/600000: episode: 1107, duration: 1.510s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221600/600000: episode: 1108, duration: 1.764s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221800/600000: episode: 1109, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222000/600000: episode: 1110, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222200/600000: episode: 1111, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222400/600000: episode: 1112, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222600/600000: episode: 1113, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222800/600000: episode: 1114, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223000/600000: episode: 1115, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223200/600000: episode: 1116, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223400/600000: episode: 1117, duration: 1.781s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223600/600000: episode: 1118, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223800/600000: episode: 1119, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224000/600000: episode: 1120, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224200/600000: episode: 1121, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224400/600000: episode: 1122, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224600/600000: episode: 1123, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224800/600000: episode: 1124, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225000/600000: episode: 1125, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225200/600000: episode: 1126, duration: 1.727s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225400/600000: episode: 1127, duration: 1.518s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225600/600000: episode: 1128, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225800/600000: episode: 1129, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226000/600000: episode: 1130, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226200/600000: episode: 1131, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226400/600000: episode: 1132, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226600/600000: episode: 1133, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226800/600000: episode: 1134, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227000/600000: episode: 1135, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227200/600000: episode: 1136, duration: 1.744s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227400/600000: episode: 1137, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227600/600000: episode: 1138, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227800/600000: episode: 1139, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228000/600000: episode: 1140, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228200/600000: episode: 1141, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228400/600000: episode: 1142, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228600/600000: episode: 1143, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228800/600000: episode: 1144, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229000/600000: episode: 1145, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229200/600000: episode: 1146, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229400/600000: episode: 1147, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229600/600000: episode: 1148, duration: 1.216s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229800/600000: episode: 1149, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230000/600000: episode: 1150, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230200/600000: episode: 1151, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230400/600000: episode: 1152, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230600/600000: episode: 1153, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230800/600000: episode: 1154, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231000/600000: episode: 1155, duration: 1.549s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231200/600000: episode: 1156, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231400/600000: episode: 1157, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231600/600000: episode: 1158, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231800/600000: episode: 1159, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232000/600000: episode: 1160, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232200/600000: episode: 1161, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232400/600000: episode: 1162, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232600/600000: episode: 1163, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232800/600000: episode: 1164, duration: 1.726s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233000/600000: episode: 1165, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233200/600000: episode: 1166, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233400/600000: episode: 1167, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233600/600000: episode: 1168, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233800/600000: episode: 1169, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234000/600000: episode: 1170, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234200/600000: episode: 1171, duration: 1.209s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234400/600000: episode: 1172, duration: 1.416s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234600/600000: episode: 1173, duration: 1.804s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234800/600000: episode: 1174, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235000/600000: episode: 1175, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235200/600000: episode: 1176, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235400/600000: episode: 1177, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235600/600000: episode: 1178, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235800/600000: episode: 1179, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236000/600000: episode: 1180, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236200/600000: episode: 1181, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236400/600000: episode: 1182, duration: 1.824s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236600/600000: episode: 1183, duration: 1.472s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236800/600000: episode: 1184, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237000/600000: episode: 1185, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237200/600000: episode: 1186, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237400/600000: episode: 1187, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237600/600000: episode: 1188, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237800/600000: episode: 1189, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238000/600000: episode: 1190, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238200/600000: episode: 1191, duration: 1.654s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238400/600000: episode: 1192, duration: 1.630s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238600/600000: episode: 1193, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238800/600000: episode: 1194, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239000/600000: episode: 1195, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239200/600000: episode: 1196, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239400/600000: episode: 1197, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239600/600000: episode: 1198, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239800/600000: episode: 1199, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240000/600000: episode: 1200, duration: 1.563s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240200/600000: episode: 1201, duration: 1.820s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240400/600000: episode: 1202, duration: 1.640s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240600/600000: episode: 1203, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240800/600000: episode: 1204, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241000/600000: episode: 1205, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241200/600000: episode: 1206, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241400/600000: episode: 1207, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241600/600000: episode: 1208, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241800/600000: episode: 1209, duration: 1.746s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242000/600000: episode: 1210, duration: 1.614s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242200/600000: episode: 1211, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242400/600000: episode: 1212, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242600/600000: episode: 1213, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242800/600000: episode: 1214, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243000/600000: episode: 1215, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243200/600000: episode: 1216, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243400/600000: episode: 1217, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243600/600000: episode: 1218, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243800/600000: episode: 1219, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244000/600000: episode: 1220, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244200/600000: episode: 1221, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244400/600000: episode: 1222, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244600/600000: episode: 1223, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244800/600000: episode: 1224, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245000/600000: episode: 1225, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245200/600000: episode: 1226, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245400/600000: episode: 1227, duration: 1.755s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245600/600000: episode: 1228, duration: 1.647s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245800/600000: episode: 1229, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246000/600000: episode: 1230, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246200/600000: episode: 1231, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246400/600000: episode: 1232, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246600/600000: episode: 1233, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246800/600000: episode: 1234, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247000/600000: episode: 1235, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247200/600000: episode: 1236, duration: 1.605s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247400/600000: episode: 1237, duration: 1.676s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247600/600000: episode: 1238, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247800/600000: episode: 1239, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248000/600000: episode: 1240, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248200/600000: episode: 1241, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248400/600000: episode: 1242, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248600/600000: episode: 1243, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248800/600000: episode: 1244, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249000/600000: episode: 1245, duration: 1.404s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249200/600000: episode: 1246, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249400/600000: episode: 1247, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249600/600000: episode: 1248, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249800/600000: episode: 1249, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250000/600000: episode: 1250, duration: 1.201s, episode steps: 200, steps per second: 167, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250200/600000: episode: 1251, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250400/600000: episode: 1252, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250600/600000: episode: 1253, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250800/600000: episode: 1254, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251000/600000: episode: 1255, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251200/600000: episode: 1256, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251400/600000: episode: 1257, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251600/600000: episode: 1258, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251800/600000: episode: 1259, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252000/600000: episode: 1260, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252200/600000: episode: 1261, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252400/600000: episode: 1262, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252600/600000: episode: 1263, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252800/600000: episode: 1264, duration: 1.729s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253000/600000: episode: 1265, duration: 1.525s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253200/600000: episode: 1266, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253400/600000: episode: 1267, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253600/600000: episode: 1268, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253800/600000: episode: 1269, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254000/600000: episode: 1270, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254200/600000: episode: 1271, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254400/600000: episode: 1272, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254600/600000: episode: 1273, duration: 1.688s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254800/600000: episode: 1274, duration: 1.626s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255000/600000: episode: 1275, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255200/600000: episode: 1276, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255400/600000: episode: 1277, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255600/600000: episode: 1278, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255800/600000: episode: 1279, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256000/600000: episode: 1280, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256200/600000: episode: 1281, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256400/600000: episode: 1282, duration: 1.611s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256600/600000: episode: 1283, duration: 1.643s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256800/600000: episode: 1284, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257000/600000: episode: 1285, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257200/600000: episode: 1286, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257400/600000: episode: 1287, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257600/600000: episode: 1288, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257800/600000: episode: 1289, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258000/600000: episode: 1290, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258200/600000: episode: 1291, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258400/600000: episode: 1292, duration: 1.839s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258600/600000: episode: 1293, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258800/600000: episode: 1294, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259000/600000: episode: 1295, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259200/600000: episode: 1296, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259400/600000: episode: 1297, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259600/600000: episode: 1298, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259800/600000: episode: 1299, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260000/600000: episode: 1300, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260200/600000: episode: 1301, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260400/600000: episode: 1302, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260600/600000: episode: 1303, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260800/600000: episode: 1304, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261000/600000: episode: 1305, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261200/600000: episode: 1306, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261400/600000: episode: 1307, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261600/600000: episode: 1308, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261800/600000: episode: 1309, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262000/600000: episode: 1310, duration: 1.732s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262200/600000: episode: 1311, duration: 1.517s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262400/600000: episode: 1312, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262600/600000: episode: 1313, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262800/600000: episode: 1314, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263000/600000: episode: 1315, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263200/600000: episode: 1316, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263400/600000: episode: 1317, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263600/600000: episode: 1318, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263800/600000: episode: 1319, duration: 1.642s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264000/600000: episode: 1320, duration: 1.642s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264200/600000: episode: 1321, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264400/600000: episode: 1322, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264600/600000: episode: 1323, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264800/600000: episode: 1324, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265000/600000: episode: 1325, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265200/600000: episode: 1326, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265400/600000: episode: 1327, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265600/600000: episode: 1328, duration: 1.663s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265800/600000: episode: 1329, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266000/600000: episode: 1330, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266200/600000: episode: 1331, duration: 1.208s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266400/600000: episode: 1332, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266600/600000: episode: 1333, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266800/600000: episode: 1334, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267000/600000: episode: 1335, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267200/600000: episode: 1336, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267400/600000: episode: 1337, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267600/600000: episode: 1338, duration: 1.757s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267800/600000: episode: 1339, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268000/600000: episode: 1340, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268200/600000: episode: 1341, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268400/600000: episode: 1342, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268600/600000: episode: 1343, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268800/600000: episode: 1344, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269000/600000: episode: 1345, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269200/600000: episode: 1346, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269400/600000: episode: 1347, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269600/600000: episode: 1348, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269800/600000: episode: 1349, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270000/600000: episode: 1350, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270200/600000: episode: 1351, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270400/600000: episode: 1352, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270600/600000: episode: 1353, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270800/600000: episode: 1354, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271000/600000: episode: 1355, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271200/600000: episode: 1356, duration: 1.810s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271400/600000: episode: 1357, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271600/600000: episode: 1358, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271800/600000: episode: 1359, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272000/600000: episode: 1360, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272200/600000: episode: 1361, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272400/600000: episode: 1362, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272600/600000: episode: 1363, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272800/600000: episode: 1364, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273000/600000: episode: 1365, duration: 1.665s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273200/600000: episode: 1366, duration: 1.604s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273400/600000: episode: 1367, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273600/600000: episode: 1368, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273800/600000: episode: 1369, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274000/600000: episode: 1370, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274200/600000: episode: 1371, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274400/600000: episode: 1372, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274600/600000: episode: 1373, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274800/600000: episode: 1374, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275000/600000: episode: 1375, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275200/600000: episode: 1376, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275400/600000: episode: 1377, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275600/600000: episode: 1378, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275800/600000: episode: 1379, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276000/600000: episode: 1380, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276200/600000: episode: 1381, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276400/600000: episode: 1382, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276600/600000: episode: 1383, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276800/600000: episode: 1384, duration: 1.812s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277000/600000: episode: 1385, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277200/600000: episode: 1386, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277400/600000: episode: 1387, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277600/600000: episode: 1388, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277800/600000: episode: 1389, duration: 1.238s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278000/600000: episode: 1390, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278200/600000: episode: 1391, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278400/600000: episode: 1392, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278600/600000: episode: 1393, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278800/600000: episode: 1394, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279000/600000: episode: 1395, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279200/600000: episode: 1396, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279400/600000: episode: 1397, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279600/600000: episode: 1398, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279800/600000: episode: 1399, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280000/600000: episode: 1400, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280200/600000: episode: 1401, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280400/600000: episode: 1402, duration: 1.543s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280600/600000: episode: 1403, duration: 1.671s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280800/600000: episode: 1404, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281000/600000: episode: 1405, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281200/600000: episode: 1406, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281400/600000: episode: 1407, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281600/600000: episode: 1408, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281800/600000: episode: 1409, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282000/600000: episode: 1410, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282200/600000: episode: 1411, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282400/600000: episode: 1412, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282600/600000: episode: 1413, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282800/600000: episode: 1414, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283000/600000: episode: 1415, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283200/600000: episode: 1416, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283400/600000: episode: 1417, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283600/600000: episode: 1418, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283800/600000: episode: 1419, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284000/600000: episode: 1420, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284200/600000: episode: 1421, duration: 1.726s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284400/600000: episode: 1422, duration: 1.506s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284600/600000: episode: 1423, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284800/600000: episode: 1424, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285000/600000: episode: 1425, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285200/600000: episode: 1426, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285400/600000: episode: 1427, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285600/600000: episode: 1428, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285800/600000: episode: 1429, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286000/600000: episode: 1430, duration: 1.584s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286200/600000: episode: 1431, duration: 1.638s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286400/600000: episode: 1432, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286600/600000: episode: 1433, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286800/600000: episode: 1434, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287000/600000: episode: 1435, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287200/600000: episode: 1436, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287400/600000: episode: 1437, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287600/600000: episode: 1438, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287800/600000: episode: 1439, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288000/600000: episode: 1440, duration: 1.681s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288200/600000: episode: 1441, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288400/600000: episode: 1442, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288600/600000: episode: 1443, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288800/600000: episode: 1444, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289000/600000: episode: 1445, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289200/600000: episode: 1446, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289400/600000: episode: 1447, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289600/600000: episode: 1448, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289800/600000: episode: 1449, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290000/600000: episode: 1450, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290200/600000: episode: 1451, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290400/600000: episode: 1452, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290600/600000: episode: 1453, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290800/600000: episode: 1454, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291000/600000: episode: 1455, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291200/600000: episode: 1456, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291400/600000: episode: 1457, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291600/600000: episode: 1458, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291800/600000: episode: 1459, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292000/600000: episode: 1460, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292200/600000: episode: 1461, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292400/600000: episode: 1462, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292600/600000: episode: 1463, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292800/600000: episode: 1464, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293000/600000: episode: 1465, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293200/600000: episode: 1466, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293400/600000: episode: 1467, duration: 1.807s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293600/600000: episode: 1468, duration: 1.471s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293800/600000: episode: 1469, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294000/600000: episode: 1470, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294200/600000: episode: 1471, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294400/600000: episode: 1472, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294600/600000: episode: 1473, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294800/600000: episode: 1474, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295000/600000: episode: 1475, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295200/600000: episode: 1476, duration: 1.658s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295400/600000: episode: 1477, duration: 1.626s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295600/600000: episode: 1478, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295800/600000: episode: 1479, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296000/600000: episode: 1480, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296200/600000: episode: 1481, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296400/600000: episode: 1482, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296600/600000: episode: 1483, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296800/600000: episode: 1484, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297000/600000: episode: 1485, duration: 1.496s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297200/600000: episode: 1486, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297400/600000: episode: 1487, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297600/600000: episode: 1488, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297800/600000: episode: 1489, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298000/600000: episode: 1490, duration: 1.206s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298200/600000: episode: 1491, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298400/600000: episode: 1492, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298600/600000: episode: 1493, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298800/600000: episode: 1494, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299000/600000: episode: 1495, duration: 1.834s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299200/600000: episode: 1496, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299400/600000: episode: 1497, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299600/600000: episode: 1498, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299800/600000: episode: 1499, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300000/600000: episode: 1500, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300200/600000: episode: 1501, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300400/600000: episode: 1502, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300600/600000: episode: 1503, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300800/600000: episode: 1504, duration: 1.585s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301000/600000: episode: 1505, duration: 1.663s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301200/600000: episode: 1506, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301400/600000: episode: 1507, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301600/600000: episode: 1508, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301800/600000: episode: 1509, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302000/600000: episode: 1510, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302200/600000: episode: 1511, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302400/600000: episode: 1512, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302600/600000: episode: 1513, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302800/600000: episode: 1514, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303000/600000: episode: 1515, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303200/600000: episode: 1516, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303400/600000: episode: 1517, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303600/600000: episode: 1518, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303800/600000: episode: 1519, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304000/600000: episode: 1520, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304200/600000: episode: 1521, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304400/600000: episode: 1522, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304600/600000: episode: 1523, duration: 1.672s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304800/600000: episode: 1524, duration: 1.537s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305000/600000: episode: 1525, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305200/600000: episode: 1526, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305400/600000: episode: 1527, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305600/600000: episode: 1528, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305800/600000: episode: 1529, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306000/600000: episode: 1530, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306200/600000: episode: 1531, duration: 1.190s, episode steps: 200, steps per second: 168, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306400/600000: episode: 1532, duration: 1.511s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306600/600000: episode: 1533, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306800/600000: episode: 1534, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307000/600000: episode: 1535, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307200/600000: episode: 1536, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307400/600000: episode: 1537, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307600/600000: episode: 1538, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307800/600000: episode: 1539, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308000/600000: episode: 1540, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308200/600000: episode: 1541, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308400/600000: episode: 1542, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308600/600000: episode: 1543, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308800/600000: episode: 1544, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309000/600000: episode: 1545, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309200/600000: episode: 1546, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309400/600000: episode: 1547, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309600/600000: episode: 1548, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309800/600000: episode: 1549, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310000/600000: episode: 1550, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310200/600000: episode: 1551, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310400/600000: episode: 1552, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310600/600000: episode: 1553, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310800/600000: episode: 1554, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311000/600000: episode: 1555, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311200/600000: episode: 1556, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311400/600000: episode: 1557, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311600/600000: episode: 1558, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311800/600000: episode: 1559, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312000/600000: episode: 1560, duration: 1.779s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312200/600000: episode: 1561, duration: 1.559s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312400/600000: episode: 1562, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312600/600000: episode: 1563, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312800/600000: episode: 1564, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313000/600000: episode: 1565, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313200/600000: episode: 1566, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313400/600000: episode: 1567, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313600/600000: episode: 1568, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313800/600000: episode: 1569, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314000/600000: episode: 1570, duration: 1.537s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314200/600000: episode: 1571, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314400/600000: episode: 1572, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314600/600000: episode: 1573, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314800/600000: episode: 1574, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315000/600000: episode: 1575, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315200/600000: episode: 1576, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315400/600000: episode: 1577, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315600/600000: episode: 1578, duration: 1.592s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315800/600000: episode: 1579, duration: 1.631s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316000/600000: episode: 1580, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316200/600000: episode: 1581, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316400/600000: episode: 1582, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316600/600000: episode: 1583, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316800/600000: episode: 1584, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317000/600000: episode: 1585, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317200/600000: episode: 1586, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317400/600000: episode: 1587, duration: 1.508s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317600/600000: episode: 1588, duration: 1.779s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317800/600000: episode: 1589, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318000/600000: episode: 1590, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318200/600000: episode: 1591, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318400/600000: episode: 1592, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318600/600000: episode: 1593, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318800/600000: episode: 1594, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319000/600000: episode: 1595, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319200/600000: episode: 1596, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319400/600000: episode: 1597, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319600/600000: episode: 1598, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319800/600000: episode: 1599, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320000/600000: episode: 1600, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320200/600000: episode: 1601, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320400/600000: episode: 1602, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320600/600000: episode: 1603, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320800/600000: episode: 1604, duration: 1.211s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321000/600000: episode: 1605, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321200/600000: episode: 1606, duration: 1.688s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321400/600000: episode: 1607, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321600/600000: episode: 1608, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321800/600000: episode: 1609, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322000/600000: episode: 1610, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322200/600000: episode: 1611, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322400/600000: episode: 1612, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322600/600000: episode: 1613, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322800/600000: episode: 1614, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323000/600000: episode: 1615, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323200/600000: episode: 1616, duration: 1.747s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323400/600000: episode: 1617, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323600/600000: episode: 1618, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323800/600000: episode: 1619, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324000/600000: episode: 1620, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324200/600000: episode: 1621, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324400/600000: episode: 1622, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324600/600000: episode: 1623, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324800/600000: episode: 1624, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325000/600000: episode: 1625, duration: 1.816s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325200/600000: episode: 1626, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325400/600000: episode: 1627, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325600/600000: episode: 1628, duration: 1.204s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325800/600000: episode: 1629, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326000/600000: episode: 1630, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326200/600000: episode: 1631, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326400/600000: episode: 1632, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326600/600000: episode: 1633, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326800/600000: episode: 1634, duration: 1.761s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327000/600000: episode: 1635, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327200/600000: episode: 1636, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327400/600000: episode: 1637, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327600/600000: episode: 1638, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327800/600000: episode: 1639, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328000/600000: episode: 1640, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328200/600000: episode: 1641, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328400/600000: episode: 1642, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328600/600000: episode: 1643, duration: 1.526s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328800/600000: episode: 1644, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329000/600000: episode: 1645, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329200/600000: episode: 1646, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329400/600000: episode: 1647, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329600/600000: episode: 1648, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329800/600000: episode: 1649, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330000/600000: episode: 1650, duration: 1.688s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330200/600000: episode: 1651, duration: 1.776s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330400/600000: episode: 1652, duration: 2.095s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330600/600000: episode: 1653, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330800/600000: episode: 1654, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331000/600000: episode: 1655, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331200/600000: episode: 1656, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331400/600000: episode: 1657, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331600/600000: episode: 1658, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331800/600000: episode: 1659, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332000/600000: episode: 1660, duration: 1.465s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332200/600000: episode: 1661, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332400/600000: episode: 1662, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332600/600000: episode: 1663, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332800/600000: episode: 1664, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333000/600000: episode: 1665, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333200/600000: episode: 1666, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333400/600000: episode: 1667, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333600/600000: episode: 1668, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333800/600000: episode: 1669, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334000/600000: episode: 1670, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334200/600000: episode: 1671, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334400/600000: episode: 1672, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334600/600000: episode: 1673, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334800/600000: episode: 1674, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335000/600000: episode: 1675, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335200/600000: episode: 1676, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335400/600000: episode: 1677, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335600/600000: episode: 1678, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335800/600000: episode: 1679, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336000/600000: episode: 1680, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336200/600000: episode: 1681, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336400/600000: episode: 1682, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336600/600000: episode: 1683, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336800/600000: episode: 1684, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337000/600000: episode: 1685, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337200/600000: episode: 1686, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337400/600000: episode: 1687, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337600/600000: episode: 1688, duration: 1.636s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337800/600000: episode: 1689, duration: 1.645s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338000/600000: episode: 1690, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338200/600000: episode: 1691, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338400/600000: episode: 1692, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338600/600000: episode: 1693, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338800/600000: episode: 1694, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339000/600000: episode: 1695, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339200/600000: episode: 1696, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339400/600000: episode: 1697, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339600/600000: episode: 1698, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339800/600000: episode: 1699, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340000/600000: episode: 1700, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340200/600000: episode: 1701, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340400/600000: episode: 1702, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340600/600000: episode: 1703, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340800/600000: episode: 1704, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341000/600000: episode: 1705, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341200/600000: episode: 1706, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341400/600000: episode: 1707, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341600/600000: episode: 1708, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341800/600000: episode: 1709, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342000/600000: episode: 1710, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342200/600000: episode: 1711, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342400/600000: episode: 1712, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342600/600000: episode: 1713, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342800/600000: episode: 1714, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343000/600000: episode: 1715, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343200/600000: episode: 1716, duration: 1.689s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343400/600000: episode: 1717, duration: 1.545s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343600/600000: episode: 1718, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343800/600000: episode: 1719, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344000/600000: episode: 1720, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344200/600000: episode: 1721, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344400/600000: episode: 1722, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344600/600000: episode: 1723, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344800/600000: episode: 1724, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345000/600000: episode: 1725, duration: 1.602s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345200/600000: episode: 1726, duration: 1.647s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345400/600000: episode: 1727, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345600/600000: episode: 1728, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345800/600000: episode: 1729, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346000/600000: episode: 1730, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346200/600000: episode: 1731, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346400/600000: episode: 1732, duration: 1.201s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346600/600000: episode: 1733, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346800/600000: episode: 1734, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347000/600000: episode: 1735, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347200/600000: episode: 1736, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347400/600000: episode: 1737, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347600/600000: episode: 1738, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347800/600000: episode: 1739, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348000/600000: episode: 1740, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348200/600000: episode: 1741, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348400/600000: episode: 1742, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348600/600000: episode: 1743, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348800/600000: episode: 1744, duration: 1.724s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349000/600000: episode: 1745, duration: 1.498s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349200/600000: episode: 1746, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349400/600000: episode: 1747, duration: 1.216s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349600/600000: episode: 1748, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349800/600000: episode: 1749, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350000/600000: episode: 1750, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350200/600000: episode: 1751, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350400/600000: episode: 1752, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350600/600000: episode: 1753, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350800/600000: episode: 1754, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351000/600000: episode: 1755, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351200/600000: episode: 1756, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351400/600000: episode: 1757, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351600/600000: episode: 1758, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351800/600000: episode: 1759, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352000/600000: episode: 1760, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352200/600000: episode: 1761, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352400/600000: episode: 1762, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352600/600000: episode: 1763, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352800/600000: episode: 1764, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353000/600000: episode: 1765, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353200/600000: episode: 1766, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353400/600000: episode: 1767, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353600/600000: episode: 1768, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353800/600000: episode: 1769, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354000/600000: episode: 1770, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354200/600000: episode: 1771, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354400/600000: episode: 1772, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354600/600000: episode: 1773, duration: 1.555s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354800/600000: episode: 1774, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355000/600000: episode: 1775, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355200/600000: episode: 1776, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355400/600000: episode: 1777, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355600/600000: episode: 1778, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355800/600000: episode: 1779, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356000/600000: episode: 1780, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356200/600000: episode: 1781, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356400/600000: episode: 1782, duration: 1.753s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356600/600000: episode: 1783, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356800/600000: episode: 1784, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357000/600000: episode: 1785, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357200/600000: episode: 1786, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357400/600000: episode: 1787, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357600/600000: episode: 1788, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357800/600000: episode: 1789, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358000/600000: episode: 1790, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358200/600000: episode: 1791, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358400/600000: episode: 1792, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358600/600000: episode: 1793, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358800/600000: episode: 1794, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359000/600000: episode: 1795, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359200/600000: episode: 1796, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359400/600000: episode: 1797, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359600/600000: episode: 1798, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359800/600000: episode: 1799, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360000/600000: episode: 1800, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360200/600000: episode: 1801, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360400/600000: episode: 1802, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360600/600000: episode: 1803, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360800/600000: episode: 1804, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361000/600000: episode: 1805, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361200/600000: episode: 1806, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361400/600000: episode: 1807, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361600/600000: episode: 1808, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361800/600000: episode: 1809, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362000/600000: episode: 1810, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362200/600000: episode: 1811, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362400/600000: episode: 1812, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362600/600000: episode: 1813, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362800/600000: episode: 1814, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363000/600000: episode: 1815, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363200/600000: episode: 1816, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363400/600000: episode: 1817, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363600/600000: episode: 1818, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363800/600000: episode: 1819, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364000/600000: episode: 1820, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364200/600000: episode: 1821, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364400/600000: episode: 1822, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364600/600000: episode: 1823, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364800/600000: episode: 1824, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365000/600000: episode: 1825, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365200/600000: episode: 1826, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365400/600000: episode: 1827, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365600/600000: episode: 1828, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365800/600000: episode: 1829, duration: 1.477s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366000/600000: episode: 1830, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366200/600000: episode: 1831, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366400/600000: episode: 1832, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366600/600000: episode: 1833, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366800/600000: episode: 1834, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367000/600000: episode: 1835, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367200/600000: episode: 1836, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367400/600000: episode: 1837, duration: 1.696s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367600/600000: episode: 1838, duration: 1.569s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367800/600000: episode: 1839, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368000/600000: episode: 1840, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368200/600000: episode: 1841, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368400/600000: episode: 1842, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368600/600000: episode: 1843, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368800/600000: episode: 1844, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369000/600000: episode: 1845, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369200/600000: episode: 1846, duration: 1.531s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369400/600000: episode: 1847, duration: 1.679s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369600/600000: episode: 1848, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369800/600000: episode: 1849, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370000/600000: episode: 1850, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370200/600000: episode: 1851, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370400/600000: episode: 1852, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370600/600000: episode: 1853, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370800/600000: episode: 1854, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371000/600000: episode: 1855, duration: 1.365s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371200/600000: episode: 1856, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371400/600000: episode: 1857, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371600/600000: episode: 1858, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371800/600000: episode: 1859, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372000/600000: episode: 1860, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372200/600000: episode: 1861, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372400/600000: episode: 1862, duration: 1.210s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372600/600000: episode: 1863, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372800/600000: episode: 1864, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373000/600000: episode: 1865, duration: 1.770s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373200/600000: episode: 1866, duration: 1.490s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373400/600000: episode: 1867, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373600/600000: episode: 1868, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373800/600000: episode: 1869, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374000/600000: episode: 1870, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374200/600000: episode: 1871, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374400/600000: episode: 1872, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374600/600000: episode: 1873, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374800/600000: episode: 1874, duration: 1.534s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375000/600000: episode: 1875, duration: 1.742s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375200/600000: episode: 1876, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375400/600000: episode: 1877, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375600/600000: episode: 1878, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375800/600000: episode: 1879, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376000/600000: episode: 1880, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376200/600000: episode: 1881, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376400/600000: episode: 1882, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376600/600000: episode: 1883, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376800/600000: episode: 1884, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377000/600000: episode: 1885, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377200/600000: episode: 1886, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.795 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377400/600000: episode: 1887, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377600/600000: episode: 1888, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377800/600000: episode: 1889, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378000/600000: episode: 1890, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378200/600000: episode: 1891, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378400/600000: episode: 1892, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378600/600000: episode: 1893, duration: 1.797s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378800/600000: episode: 1894, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379000/600000: episode: 1895, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379200/600000: episode: 1896, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379400/600000: episode: 1897, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379600/600000: episode: 1898, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379800/600000: episode: 1899, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380000/600000: episode: 1900, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380200/600000: episode: 1901, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380400/600000: episode: 1902, duration: 1.737s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380600/600000: episode: 1903, duration: 1.565s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380800/600000: episode: 1904, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381000/600000: episode: 1905, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381200/600000: episode: 1906, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381400/600000: episode: 1907, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381600/600000: episode: 1908, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381800/600000: episode: 1909, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382000/600000: episode: 1910, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382200/600000: episode: 1911, duration: 1.626s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382400/600000: episode: 1912, duration: 1.668s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382600/600000: episode: 1913, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382800/600000: episode: 1914, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383000/600000: episode: 1915, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383200/600000: episode: 1916, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383400/600000: episode: 1917, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383600/600000: episode: 1918, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383800/600000: episode: 1919, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384000/600000: episode: 1920, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384200/600000: episode: 1921, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384400/600000: episode: 1922, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384600/600000: episode: 1923, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384800/600000: episode: 1924, duration: 1.212s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385000/600000: episode: 1925, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385200/600000: episode: 1926, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385400/600000: episode: 1927, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385600/600000: episode: 1928, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385800/600000: episode: 1929, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386000/600000: episode: 1930, duration: 1.834s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386200/600000: episode: 1931, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386400/600000: episode: 1932, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386600/600000: episode: 1933, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386800/600000: episode: 1934, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387000/600000: episode: 1935, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387200/600000: episode: 1936, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387400/600000: episode: 1937, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387600/600000: episode: 1938, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387800/600000: episode: 1939, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388000/600000: episode: 1940, duration: 1.547s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388200/600000: episode: 1941, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388400/600000: episode: 1942, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388600/600000: episode: 1943, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388800/600000: episode: 1944, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389000/600000: episode: 1945, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389200/600000: episode: 1946, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389400/600000: episode: 1947, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389600/600000: episode: 1948, duration: 1.539s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389800/600000: episode: 1949, duration: 1.745s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390000/600000: episode: 1950, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390200/600000: episode: 1951, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390400/600000: episode: 1952, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390600/600000: episode: 1953, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390800/600000: episode: 1954, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391000/600000: episode: 1955, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391200/600000: episode: 1956, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391400/600000: episode: 1957, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391600/600000: episode: 1958, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391800/600000: episode: 1959, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392000/600000: episode: 1960, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392200/600000: episode: 1961, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392400/600000: episode: 1962, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392600/600000: episode: 1963, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392800/600000: episode: 1964, duration: 1.214s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393000/600000: episode: 1965, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393200/600000: episode: 1966, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393400/600000: episode: 1967, duration: 1.815s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393600/600000: episode: 1968, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393800/600000: episode: 1969, duration: 1.286s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394000/600000: episode: 1970, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394200/600000: episode: 1971, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394400/600000: episode: 1972, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394600/600000: episode: 1973, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394800/600000: episode: 1974, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395000/600000: episode: 1975, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395200/600000: episode: 1976, duration: 1.650s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395400/600000: episode: 1977, duration: 1.608s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395600/600000: episode: 1978, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395800/600000: episode: 1979, duration: 1.203s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396000/600000: episode: 1980, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396200/600000: episode: 1981, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396400/600000: episode: 1982, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396600/600000: episode: 1983, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396800/600000: episode: 1984, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397000/600000: episode: 1985, duration: 1.470s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397200/600000: episode: 1986, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397400/600000: episode: 1987, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397600/600000: episode: 1988, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397800/600000: episode: 1989, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398000/600000: episode: 1990, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398200/600000: episode: 1991, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398400/600000: episode: 1992, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398600/600000: episode: 1993, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398800/600000: episode: 1994, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399000/600000: episode: 1995, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399200/600000: episode: 1996, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399400/600000: episode: 1997, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399600/600000: episode: 1998, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399800/600000: episode: 1999, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400000/600000: episode: 2000, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400200/600000: episode: 2001, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400400/600000: episode: 2002, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400600/600000: episode: 2003, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400800/600000: episode: 2004, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401000/600000: episode: 2005, duration: 1.628s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401200/600000: episode: 2006, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401400/600000: episode: 2007, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401600/600000: episode: 2008, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401800/600000: episode: 2009, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402000/600000: episode: 2010, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402200/600000: episode: 2011, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402400/600000: episode: 2012, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402600/600000: episode: 2013, duration: 1.459s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402800/600000: episode: 2014, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403000/600000: episode: 2015, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403200/600000: episode: 2016, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403400/600000: episode: 2017, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403600/600000: episode: 2018, duration: 1.216s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403800/600000: episode: 2019, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404000/600000: episode: 2020, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404200/600000: episode: 2021, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404400/600000: episode: 2022, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404600/600000: episode: 2023, duration: 1.826s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404800/600000: episode: 2024, duration: 1.404s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405000/600000: episode: 2025, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405200/600000: episode: 2026, duration: 1.223s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405400/600000: episode: 2027, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405600/600000: episode: 2028, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405800/600000: episode: 2029, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406000/600000: episode: 2030, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406200/600000: episode: 2031, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406400/600000: episode: 2032, duration: 1.598s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406600/600000: episode: 2033, duration: 1.726s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406800/600000: episode: 2034, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407000/600000: episode: 2035, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407200/600000: episode: 2036, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407400/600000: episode: 2037, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407600/600000: episode: 2038, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407800/600000: episode: 2039, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408000/600000: episode: 2040, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408200/600000: episode: 2041, duration: 1.527s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408400/600000: episode: 2042, duration: 1.814s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408600/600000: episode: 2043, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408800/600000: episode: 2044, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409000/600000: episode: 2045, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409200/600000: episode: 2046, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409400/600000: episode: 2047, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409600/600000: episode: 2048, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409800/600000: episode: 2049, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410000/600000: episode: 2050, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410200/600000: episode: 2051, duration: 1.830s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410400/600000: episode: 2052, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410600/600000: episode: 2053, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410800/600000: episode: 2054, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411000/600000: episode: 2055, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411200/600000: episode: 2056, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411400/600000: episode: 2057, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411600/600000: episode: 2058, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411800/600000: episode: 2059, duration: 1.213s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412000/600000: episode: 2060, duration: 1.747s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412200/600000: episode: 2061, duration: 1.488s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412400/600000: episode: 2062, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412600/600000: episode: 2063, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412800/600000: episode: 2064, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413000/600000: episode: 2065, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413200/600000: episode: 2066, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413400/600000: episode: 2067, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413600/600000: episode: 2068, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413800/600000: episode: 2069, duration: 1.635s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414000/600000: episode: 2070, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414200/600000: episode: 2071, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414400/600000: episode: 2072, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414600/600000: episode: 2073, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414800/600000: episode: 2074, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415000/600000: episode: 2075, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415200/600000: episode: 2076, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415400/600000: episode: 2077, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415600/600000: episode: 2078, duration: 1.550s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415800/600000: episode: 2079, duration: 1.807s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416000/600000: episode: 2080, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416200/600000: episode: 2081, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416400/600000: episode: 2082, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416600/600000: episode: 2083, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416800/600000: episode: 2084, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417000/600000: episode: 2085, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417200/600000: episode: 2086, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417400/600000: episode: 2087, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417600/600000: episode: 2088, duration: 1.797s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417800/600000: episode: 2089, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418000/600000: episode: 2090, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418200/600000: episode: 2091, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418400/600000: episode: 2092, duration: 1.206s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418600/600000: episode: 2093, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418800/600000: episode: 2094, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419000/600000: episode: 2095, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419200/600000: episode: 2096, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419400/600000: episode: 2097, duration: 1.699s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419600/600000: episode: 2098, duration: 1.571s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419800/600000: episode: 2099, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420000/600000: episode: 2100, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420200/600000: episode: 2101, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420400/600000: episode: 2102, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420600/600000: episode: 2103, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420800/600000: episode: 2104, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421000/600000: episode: 2105, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421200/600000: episode: 2106, duration: 1.634s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421400/600000: episode: 2107, duration: 1.656s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421600/600000: episode: 2108, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421800/600000: episode: 2109, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422000/600000: episode: 2110, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422200/600000: episode: 2111, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422400/600000: episode: 2112, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422600/600000: episode: 2113, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422800/600000: episode: 2114, duration: 1.222s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423000/600000: episode: 2115, duration: 1.505s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423200/600000: episode: 2116, duration: 1.727s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423400/600000: episode: 2117, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423600/600000: episode: 2118, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423800/600000: episode: 2119, duration: 1.203s, episode steps: 200, steps per second: 166, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424000/600000: episode: 2120, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424200/600000: episode: 2121, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424400/600000: episode: 2122, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424600/600000: episode: 2123, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424800/600000: episode: 2124, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425000/600000: episode: 2125, duration: 1.837s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425200/600000: episode: 2126, duration: 1.449s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425400/600000: episode: 2127, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425600/600000: episode: 2128, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425800/600000: episode: 2129, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426000/600000: episode: 2130, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426200/600000: episode: 2131, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426400/600000: episode: 2132, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426600/600000: episode: 2133, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426800/600000: episode: 2134, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427000/600000: episode: 2135, duration: 1.633s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427200/600000: episode: 2136, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427400/600000: episode: 2137, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427600/600000: episode: 2138, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427800/600000: episode: 2139, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428000/600000: episode: 2140, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428200/600000: episode: 2141, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428400/600000: episode: 2142, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428600/600000: episode: 2143, duration: 1.522s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428800/600000: episode: 2144, duration: 1.720s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429000/600000: episode: 2145, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429200/600000: episode: 2146, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429400/600000: episode: 2147, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429600/600000: episode: 2148, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429800/600000: episode: 2149, duration: 1.209s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430000/600000: episode: 2150, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430200/600000: episode: 2151, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430400/600000: episode: 2152, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430600/600000: episode: 2153, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430800/600000: episode: 2154, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431000/600000: episode: 2155, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431200/600000: episode: 2156, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431400/600000: episode: 2157, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431600/600000: episode: 2158, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431800/600000: episode: 2159, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432000/600000: episode: 2160, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432200/600000: episode: 2161, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432400/600000: episode: 2162, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432600/600000: episode: 2163, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432800/600000: episode: 2164, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433000/600000: episode: 2165, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433200/600000: episode: 2166, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433400/600000: episode: 2167, duration: 1.231s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433600/600000: episode: 2168, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433800/600000: episode: 2169, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434000/600000: episode: 2170, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434200/600000: episode: 2171, duration: 1.642s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434400/600000: episode: 2172, duration: 1.608s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434600/600000: episode: 2173, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434800/600000: episode: 2174, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435000/600000: episode: 2175, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435200/600000: episode: 2176, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435400/600000: episode: 2177, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435600/600000: episode: 2178, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435800/600000: episode: 2179, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436000/600000: episode: 2180, duration: 1.512s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436200/600000: episode: 2181, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436400/600000: episode: 2182, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436600/600000: episode: 2183, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436800/600000: episode: 2184, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437000/600000: episode: 2185, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437200/600000: episode: 2186, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437400/600000: episode: 2187, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437600/600000: episode: 2188, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437800/600000: episode: 2189, duration: 1.388s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438000/600000: episode: 2190, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438200/600000: episode: 2191, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438400/600000: episode: 2192, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438600/600000: episode: 2193, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438800/600000: episode: 2194, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439000/600000: episode: 2195, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439200/600000: episode: 2196, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439400/600000: episode: 2197, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439600/600000: episode: 2198, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439800/600000: episode: 2199, duration: 1.693s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440000/600000: episode: 2200, duration: 1.603s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440200/600000: episode: 2201, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440400/600000: episode: 2202, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440600/600000: episode: 2203, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440800/600000: episode: 2204, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441000/600000: episode: 2205, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441200/600000: episode: 2206, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441400/600000: episode: 2207, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441600/600000: episode: 2208, duration: 1.642s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441800/600000: episode: 2209, duration: 1.637s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442000/600000: episode: 2210, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442200/600000: episode: 2211, duration: 1.254s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442400/600000: episode: 2212, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442600/600000: episode: 2213, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442800/600000: episode: 2214, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443000/600000: episode: 2215, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443200/600000: episode: 2216, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443400/600000: episode: 2217, duration: 1.784s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443600/600000: episode: 2218, duration: 1.824s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443800/600000: episode: 2219, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444000/600000: episode: 2220, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444200/600000: episode: 2221, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444400/600000: episode: 2222, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444600/600000: episode: 2223, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444800/600000: episode: 2224, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445000/600000: episode: 2225, duration: 1.500s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445200/600000: episode: 2226, duration: 1.999s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445400/600000: episode: 2227, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445600/600000: episode: 2228, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445800/600000: episode: 2229, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446000/600000: episode: 2230, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446200/600000: episode: 2231, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446400/600000: episode: 2232, duration: 1.428s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446600/600000: episode: 2233, duration: 1.494s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446800/600000: episode: 2234, duration: 1.942s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447000/600000: episode: 2235, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447200/600000: episode: 2236, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447400/600000: episode: 2237, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447600/600000: episode: 2238, duration: 1.488s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447800/600000: episode: 2239, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448000/600000: episode: 2240, duration: 1.329s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448200/600000: episode: 2241, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448400/600000: episode: 2242, duration: 1.545s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448600/600000: episode: 2243, duration: 1.979s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448800/600000: episode: 2244, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449000/600000: episode: 2245, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449200/600000: episode: 2246, duration: 1.347s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449400/600000: episode: 2247, duration: 1.384s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449600/600000: episode: 2248, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449800/600000: episode: 2249, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450000/600000: episode: 2250, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450200/600000: episode: 2251, duration: 1.773s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450400/600000: episode: 2252, duration: 1.629s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450600/600000: episode: 2253, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450800/600000: episode: 2254, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451000/600000: episode: 2255, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451200/600000: episode: 2256, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451400/600000: episode: 2257, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451600/600000: episode: 2258, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451800/600000: episode: 2259, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452000/600000: episode: 2260, duration: 2.022s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452200/600000: episode: 2261, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452400/600000: episode: 2262, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452600/600000: episode: 2263, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452800/600000: episode: 2264, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453000/600000: episode: 2265, duration: 1.347s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453200/600000: episode: 2266, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453400/600000: episode: 2267, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453600/600000: episode: 2268, duration: 1.646s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453800/600000: episode: 2269, duration: 2.111s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454000/600000: episode: 2270, duration: 1.668s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454200/600000: episode: 2271, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454400/600000: episode: 2272, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454600/600000: episode: 2273, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454800/600000: episode: 2274, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455000/600000: episode: 2275, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455200/600000: episode: 2276, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455400/600000: episode: 2277, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455600/600000: episode: 2278, duration: 1.432s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455800/600000: episode: 2279, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456000/600000: episode: 2280, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456200/600000: episode: 2281, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456400/600000: episode: 2282, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456600/600000: episode: 2283, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456800/600000: episode: 2284, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457000/600000: episode: 2285, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457200/600000: episode: 2286, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457400/600000: episode: 2287, duration: 1.454s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457600/600000: episode: 2288, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457800/600000: episode: 2289, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458000/600000: episode: 2290, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458200/600000: episode: 2291, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458400/600000: episode: 2292, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458600/600000: episode: 2293, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458800/600000: episode: 2294, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459000/600000: episode: 2295, duration: 1.776s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459200/600000: episode: 2296, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459400/600000: episode: 2297, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459600/600000: episode: 2298, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459800/600000: episode: 2299, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460000/600000: episode: 2300, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460200/600000: episode: 2301, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460400/600000: episode: 2302, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460600/600000: episode: 2303, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460800/600000: episode: 2304, duration: 1.656s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461000/600000: episode: 2305, duration: 1.624s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461200/600000: episode: 2306, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461400/600000: episode: 2307, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461600/600000: episode: 2308, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461800/600000: episode: 2309, duration: 1.423s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462000/600000: episode: 2310, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462200/600000: episode: 2311, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462400/600000: episode: 2312, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462600/600000: episode: 2313, duration: 1.698s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462800/600000: episode: 2314, duration: 1.569s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463000/600000: episode: 2315, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463200/600000: episode: 2316, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463400/600000: episode: 2317, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463600/600000: episode: 2318, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463800/600000: episode: 2319, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464000/600000: episode: 2320, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464200/600000: episode: 2321, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464400/600000: episode: 2322, duration: 1.667s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464600/600000: episode: 2323, duration: 1.654s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464800/600000: episode: 2324, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465000/600000: episode: 2325, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465200/600000: episode: 2326, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465400/600000: episode: 2327, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465600/600000: episode: 2328, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465800/600000: episode: 2329, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466000/600000: episode: 2330, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466200/600000: episode: 2331, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466400/600000: episode: 2332, duration: 1.728s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466600/600000: episode: 2333, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466800/600000: episode: 2334, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467000/600000: episode: 2335, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467200/600000: episode: 2336, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467400/600000: episode: 2337, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467600/600000: episode: 2338, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467800/600000: episode: 2339, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468000/600000: episode: 2340, duration: 1.664s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468200/600000: episode: 2341, duration: 1.714s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468400/600000: episode: 2342, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468600/600000: episode: 2343, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468800/600000: episode: 2344, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469000/600000: episode: 2345, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469200/600000: episode: 2346, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469400/600000: episode: 2347, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469600/600000: episode: 2348, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469800/600000: episode: 2349, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470000/600000: episode: 2350, duration: 1.725s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470200/600000: episode: 2351, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470400/600000: episode: 2352, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470600/600000: episode: 2353, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470800/600000: episode: 2354, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471000/600000: episode: 2355, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471200/600000: episode: 2356, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471400/600000: episode: 2357, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471600/600000: episode: 2358, duration: 1.723s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471800/600000: episode: 2359, duration: 1.675s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472000/600000: episode: 2360, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472200/600000: episode: 2361, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472400/600000: episode: 2362, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472600/600000: episode: 2363, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472800/600000: episode: 2364, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473000/600000: episode: 2365, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473200/600000: episode: 2366, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473400/600000: episode: 2367, duration: 1.757s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473600/600000: episode: 2368, duration: 1.610s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473800/600000: episode: 2369, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474000/600000: episode: 2370, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474200/600000: episode: 2371, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474400/600000: episode: 2372, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474600/600000: episode: 2373, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474800/600000: episode: 2374, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475000/600000: episode: 2375, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475200/600000: episode: 2376, duration: 1.701s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475400/600000: episode: 2377, duration: 1.615s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475600/600000: episode: 2378, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475800/600000: episode: 2379, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476000/600000: episode: 2380, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476200/600000: episode: 2381, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476400/600000: episode: 2382, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476600/600000: episode: 2383, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476800/600000: episode: 2384, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477000/600000: episode: 2385, duration: 1.746s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477200/600000: episode: 2386, duration: 1.575s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477400/600000: episode: 2387, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477600/600000: episode: 2388, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477800/600000: episode: 2389, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478000/600000: episode: 2390, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478200/600000: episode: 2391, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478400/600000: episode: 2392, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478600/600000: episode: 2393, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478800/600000: episode: 2394, duration: 1.826s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479000/600000: episode: 2395, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479200/600000: episode: 2396, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479400/600000: episode: 2397, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479600/600000: episode: 2398, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479800/600000: episode: 2399, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480000/600000: episode: 2400, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480200/600000: episode: 2401, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480400/600000: episode: 2402, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480600/600000: episode: 2403, duration: 1.772s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480800/600000: episode: 2404, duration: 1.609s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481000/600000: episode: 2405, duration: 1.418s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481200/600000: episode: 2406, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481400/600000: episode: 2407, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481600/600000: episode: 2408, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481800/600000: episode: 2409, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482000/600000: episode: 2410, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482200/600000: episode: 2411, duration: 1.511s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482400/600000: episode: 2412, duration: 1.949s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482600/600000: episode: 2413, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482800/600000: episode: 2414, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483000/600000: episode: 2415, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483200/600000: episode: 2416, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483400/600000: episode: 2417, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483600/600000: episode: 2418, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483800/600000: episode: 2419, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484000/600000: episode: 2420, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484200/600000: episode: 2421, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484400/600000: episode: 2422, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484600/600000: episode: 2423, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484800/600000: episode: 2424, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485000/600000: episode: 2425, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485200/600000: episode: 2426, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485400/600000: episode: 2427, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485600/600000: episode: 2428, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485800/600000: episode: 2429, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486000/600000: episode: 2430, duration: 1.898s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486200/600000: episode: 2431, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486400/600000: episode: 2432, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486600/600000: episode: 2433, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486800/600000: episode: 2434, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487000/600000: episode: 2435, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487200/600000: episode: 2436, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487400/600000: episode: 2437, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487600/600000: episode: 2438, duration: 1.596s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487800/600000: episode: 2439, duration: 1.852s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488000/600000: episode: 2440, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488200/600000: episode: 2441, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488400/600000: episode: 2442, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488600/600000: episode: 2443, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488800/600000: episode: 2444, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489000/600000: episode: 2445, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489200/600000: episode: 2446, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489400/600000: episode: 2447, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489600/600000: episode: 2448, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489800/600000: episode: 2449, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490000/600000: episode: 2450, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490200/600000: episode: 2451, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490400/600000: episode: 2452, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490600/600000: episode: 2453, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490800/600000: episode: 2454, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491000/600000: episode: 2455, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491200/600000: episode: 2456, duration: 1.664s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491400/600000: episode: 2457, duration: 1.674s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491600/600000: episode: 2458, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491800/600000: episode: 2459, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492000/600000: episode: 2460, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492200/600000: episode: 2461, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492400/600000: episode: 2462, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492600/600000: episode: 2463, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492800/600000: episode: 2464, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493000/600000: episode: 2465, duration: 1.728s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493200/600000: episode: 2466, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493400/600000: episode: 2467, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493600/600000: episode: 2468, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493800/600000: episode: 2469, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494000/600000: episode: 2470, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494200/600000: episode: 2471, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494400/600000: episode: 2472, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494600/600000: episode: 2473, duration: 1.278s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494800/600000: episode: 2474, duration: 1.716s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495000/600000: episode: 2475, duration: 1.607s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495200/600000: episode: 2476, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495400/600000: episode: 2477, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495600/600000: episode: 2478, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495800/600000: episode: 2479, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496000/600000: episode: 2480, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496200/600000: episode: 2481, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496400/600000: episode: 2482, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496600/600000: episode: 2483, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496800/600000: episode: 2484, duration: 1.645s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497000/600000: episode: 2485, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497200/600000: episode: 2486, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497400/600000: episode: 2487, duration: 1.384s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497600/600000: episode: 2488, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497800/600000: episode: 2489, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498000/600000: episode: 2490, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498200/600000: episode: 2491, duration: 1.553s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498400/600000: episode: 2492, duration: 1.976s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498600/600000: episode: 2493, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498800/600000: episode: 2494, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499000/600000: episode: 2495, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499200/600000: episode: 2496, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499400/600000: episode: 2497, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499600/600000: episode: 2498, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499800/600000: episode: 2499, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500000/600000: episode: 2500, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500200/600000: episode: 2501, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500400/600000: episode: 2502, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500600/600000: episode: 2503, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500800/600000: episode: 2504, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501000/600000: episode: 2505, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501200/600000: episode: 2506, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501400/600000: episode: 2507, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501600/600000: episode: 2508, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501800/600000: episode: 2509, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502000/600000: episode: 2510, duration: 1.804s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502200/600000: episode: 2511, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502400/600000: episode: 2512, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502600/600000: episode: 2513, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502800/600000: episode: 2514, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503000/600000: episode: 2515, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503200/600000: episode: 2516, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503400/600000: episode: 2517, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503600/600000: episode: 2518, duration: 1.588s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503800/600000: episode: 2519, duration: 1.811s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504000/600000: episode: 2520, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504200/600000: episode: 2521, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504400/600000: episode: 2522, duration: 1.278s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504600/600000: episode: 2523, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504800/600000: episode: 2524, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505000/600000: episode: 2525, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505200/600000: episode: 2526, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505400/600000: episode: 2527, duration: 1.553s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505600/600000: episode: 2528, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505800/600000: episode: 2529, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506000/600000: episode: 2530, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506200/600000: episode: 2531, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506400/600000: episode: 2532, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506600/600000: episode: 2533, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506800/600000: episode: 2534, duration: 1.427s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507000/600000: episode: 2535, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507200/600000: episode: 2536, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507400/600000: episode: 2537, duration: 1.467s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507600/600000: episode: 2538, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507800/600000: episode: 2539, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508000/600000: episode: 2540, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508200/600000: episode: 2541, duration: 1.329s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508400/600000: episode: 2542, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508600/600000: episode: 2543, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508800/600000: episode: 2544, duration: 1.398s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509000/600000: episode: 2545, duration: 2.044s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509200/600000: episode: 2546, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509400/600000: episode: 2547, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509600/600000: episode: 2548, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509800/600000: episode: 2549, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510000/600000: episode: 2550, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510200/600000: episode: 2551, duration: 1.536s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510400/600000: episode: 2552, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510600/600000: episode: 2553, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510800/600000: episode: 2554, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511000/600000: episode: 2555, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511200/600000: episode: 2556, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511400/600000: episode: 2557, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511600/600000: episode: 2558, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511800/600000: episode: 2559, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512000/600000: episode: 2560, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512200/600000: episode: 2561, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512400/600000: episode: 2562, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512600/600000: episode: 2563, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512800/600000: episode: 2564, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513000/600000: episode: 2565, duration: 1.396s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513200/600000: episode: 2566, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513400/600000: episode: 2567, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513600/600000: episode: 2568, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513800/600000: episode: 2569, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514000/600000: episode: 2570, duration: 1.470s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514200/600000: episode: 2571, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514400/600000: episode: 2572, duration: 1.383s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514600/600000: episode: 2573, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514800/600000: episode: 2574, duration: 1.365s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515000/600000: episode: 2575, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515200/600000: episode: 2576, duration: 1.450s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515400/600000: episode: 2577, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515600/600000: episode: 2578, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515800/600000: episode: 2579, duration: 1.641s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516000/600000: episode: 2580, duration: 1.717s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516200/600000: episode: 2581, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516400/600000: episode: 2582, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516600/600000: episode: 2583, duration: 1.278s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516800/600000: episode: 2584, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517000/600000: episode: 2585, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517200/600000: episode: 2586, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517400/600000: episode: 2587, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517600/600000: episode: 2588, duration: 1.636s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517800/600000: episode: 2589, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518000/600000: episode: 2590, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518200/600000: episode: 2591, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518400/600000: episode: 2592, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518600/600000: episode: 2593, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518800/600000: episode: 2594, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519000/600000: episode: 2595, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519200/600000: episode: 2596, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519400/600000: episode: 2597, duration: 1.602s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519600/600000: episode: 2598, duration: 1.778s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519800/600000: episode: 2599, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520000/600000: episode: 2600, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520200/600000: episode: 2601, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520400/600000: episode: 2602, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520600/600000: episode: 2603, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520800/600000: episode: 2604, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521000/600000: episode: 2605, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521200/600000: episode: 2606, duration: 1.692s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521400/600000: episode: 2607, duration: 1.679s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521600/600000: episode: 2608, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521800/600000: episode: 2609, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522000/600000: episode: 2610, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522200/600000: episode: 2611, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522400/600000: episode: 2612, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522600/600000: episode: 2613, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522800/600000: episode: 2614, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523000/600000: episode: 2615, duration: 1.540s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523200/600000: episode: 2616, duration: 1.779s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523400/600000: episode: 2617, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523600/600000: episode: 2618, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523800/600000: episode: 2619, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524000/600000: episode: 2620, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524200/600000: episode: 2621, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524400/600000: episode: 2622, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524600/600000: episode: 2623, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524800/600000: episode: 2624, duration: 1.606s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525000/600000: episode: 2625, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525200/600000: episode: 2626, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525400/600000: episode: 2627, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525600/600000: episode: 2628, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525800/600000: episode: 2629, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526000/600000: episode: 2630, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526200/600000: episode: 2631, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526400/600000: episode: 2632, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526600/600000: episode: 2633, duration: 1.502s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526800/600000: episode: 2634, duration: 1.820s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527000/600000: episode: 2635, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527200/600000: episode: 2636, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527400/600000: episode: 2637, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527600/600000: episode: 2638, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527800/600000: episode: 2639, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528000/600000: episode: 2640, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528200/600000: episode: 2641, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528400/600000: episode: 2642, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528600/600000: episode: 2643, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528800/600000: episode: 2644, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529000/600000: episode: 2645, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529200/600000: episode: 2646, duration: 1.226s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529400/600000: episode: 2647, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529600/600000: episode: 2648, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529800/600000: episode: 2649, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530000/600000: episode: 2650, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530200/600000: episode: 2651, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530400/600000: episode: 2652, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530600/600000: episode: 2653, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530800/600000: episode: 2654, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531000/600000: episode: 2655, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531200/600000: episode: 2656, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531400/600000: episode: 2657, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531600/600000: episode: 2658, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531800/600000: episode: 2659, duration: 1.246s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532000/600000: episode: 2660, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532200/600000: episode: 2661, duration: 1.742s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532400/600000: episode: 2662, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532600/600000: episode: 2663, duration: 1.475s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532800/600000: episode: 2664, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533000/600000: episode: 2665, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533200/600000: episode: 2666, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533400/600000: episode: 2667, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533600/600000: episode: 2668, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533800/600000: episode: 2669, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534000/600000: episode: 2670, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534200/600000: episode: 2671, duration: 1.511s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534400/600000: episode: 2672, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534600/600000: episode: 2673, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534800/600000: episode: 2674, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535000/600000: episode: 2675, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535200/600000: episode: 2676, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535400/600000: episode: 2677, duration: 1.227s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535600/600000: episode: 2678, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535800/600000: episode: 2679, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536000/600000: episode: 2680, duration: 1.613s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536200/600000: episode: 2681, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536400/600000: episode: 2682, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536600/600000: episode: 2683, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536800/600000: episode: 2684, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537000/600000: episode: 2685, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537200/600000: episode: 2686, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537400/600000: episode: 2687, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537600/600000: episode: 2688, duration: 1.581s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537800/600000: episode: 2689, duration: 1.713s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538000/600000: episode: 2690, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538200/600000: episode: 2691, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538400/600000: episode: 2692, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538600/600000: episode: 2693, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538800/600000: episode: 2694, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539000/600000: episode: 2695, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539200/600000: episode: 2696, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539400/600000: episode: 2697, duration: 1.496s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539600/600000: episode: 2698, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539800/600000: episode: 2699, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540000/600000: episode: 2700, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540200/600000: episode: 2701, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540400/600000: episode: 2702, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540600/600000: episode: 2703, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540800/600000: episode: 2704, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541000/600000: episode: 2705, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541200/600000: episode: 2706, duration: 1.430s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541400/600000: episode: 2707, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541600/600000: episode: 2708, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541800/600000: episode: 2709, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542000/600000: episode: 2710, duration: 1.263s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542200/600000: episode: 2711, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542400/600000: episode: 2712, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542600/600000: episode: 2713, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542800/600000: episode: 2714, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543000/600000: episode: 2715, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543200/600000: episode: 2716, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543400/600000: episode: 2717, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543600/600000: episode: 2718, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543800/600000: episode: 2719, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544000/600000: episode: 2720, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544200/600000: episode: 2721, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544400/600000: episode: 2722, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544600/600000: episode: 2723, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544800/600000: episode: 2724, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545000/600000: episode: 2725, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545200/600000: episode: 2726, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545400/600000: episode: 2727, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545600/600000: episode: 2728, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545800/600000: episode: 2729, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546000/600000: episode: 2730, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546200/600000: episode: 2731, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546400/600000: episode: 2732, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546600/600000: episode: 2733, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546800/600000: episode: 2734, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547000/600000: episode: 2735, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547200/600000: episode: 2736, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547400/600000: episode: 2737, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547600/600000: episode: 2738, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547800/600000: episode: 2739, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548000/600000: episode: 2740, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548200/600000: episode: 2741, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548400/600000: episode: 2742, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548600/600000: episode: 2743, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548800/600000: episode: 2744, duration: 1.567s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549000/600000: episode: 2745, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549200/600000: episode: 2746, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549400/600000: episode: 2747, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549600/600000: episode: 2748, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549800/600000: episode: 2749, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550000/600000: episode: 2750, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550200/600000: episode: 2751, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550400/600000: episode: 2752, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550600/600000: episode: 2753, duration: 1.616s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550800/600000: episode: 2754, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551000/600000: episode: 2755, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551200/600000: episode: 2756, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551400/600000: episode: 2757, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551600/600000: episode: 2758, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551800/600000: episode: 2759, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552000/600000: episode: 2760, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552200/600000: episode: 2761, duration: 1.638s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552400/600000: episode: 2762, duration: 1.662s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552600/600000: episode: 2763, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552800/600000: episode: 2764, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553000/600000: episode: 2765, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553200/600000: episode: 2766, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553400/600000: episode: 2767, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553600/600000: episode: 2768, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553800/600000: episode: 2769, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554000/600000: episode: 2770, duration: 1.699s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554200/600000: episode: 2771, duration: 1.633s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554400/600000: episode: 2772, duration: 1.270s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554600/600000: episode: 2773, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554800/600000: episode: 2774, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555000/600000: episode: 2775, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555200/600000: episode: 2776, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555400/600000: episode: 2777, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555600/600000: episode: 2778, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555800/600000: episode: 2779, duration: 1.619s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556000/600000: episode: 2780, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556200/600000: episode: 2781, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556400/600000: episode: 2782, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556600/600000: episode: 2783, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556800/600000: episode: 2784, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557000/600000: episode: 2785, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557200/600000: episode: 2786, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557400/600000: episode: 2787, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557600/600000: episode: 2788, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557800/600000: episode: 2789, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558000/600000: episode: 2790, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558200/600000: episode: 2791, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558400/600000: episode: 2792, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558600/600000: episode: 2793, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558800/600000: episode: 2794, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559000/600000: episode: 2795, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559200/600000: episode: 2796, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559400/600000: episode: 2797, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559600/600000: episode: 2798, duration: 1.852s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559800/600000: episode: 2799, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560000/600000: episode: 2800, duration: 1.262s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560200/600000: episode: 2801, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560400/600000: episode: 2802, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560600/600000: episode: 2803, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560800/600000: episode: 2804, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561000/600000: episode: 2805, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561200/600000: episode: 2806, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561400/600000: episode: 2807, duration: 1.705s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561600/600000: episode: 2808, duration: 1.566s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561800/600000: episode: 2809, duration: 1.269s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562000/600000: episode: 2810, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562200/600000: episode: 2811, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562400/600000: episode: 2812, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562600/600000: episode: 2813, duration: 1.236s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562800/600000: episode: 2814, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563000/600000: episode: 2815, duration: 1.216s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563200/600000: episode: 2816, duration: 1.530s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563400/600000: episode: 2817, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563600/600000: episode: 2818, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563800/600000: episode: 2819, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564000/600000: episode: 2820, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564200/600000: episode: 2821, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564400/600000: episode: 2822, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564600/600000: episode: 2823, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564800/600000: episode: 2824, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565000/600000: episode: 2825, duration: 1.487s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565200/600000: episode: 2826, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565400/600000: episode: 2827, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565600/600000: episode: 2828, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565800/600000: episode: 2829, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566000/600000: episode: 2830, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566200/600000: episode: 2831, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566400/600000: episode: 2832, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566600/600000: episode: 2833, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566800/600000: episode: 2834, duration: 1.401s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567000/600000: episode: 2835, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567200/600000: episode: 2836, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567400/600000: episode: 2837, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567600/600000: episode: 2838, duration: 1.214s, episode steps: 200, steps per second: 165, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567800/600000: episode: 2839, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568000/600000: episode: 2840, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568200/600000: episode: 2841, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568400/600000: episode: 2842, duration: 1.238s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568600/600000: episode: 2843, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568800/600000: episode: 2844, duration: 1.730s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569000/600000: episode: 2845, duration: 1.495s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569200/600000: episode: 2846, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569400/600000: episode: 2847, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569600/600000: episode: 2848, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569800/600000: episode: 2849, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570000/600000: episode: 2850, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570200/600000: episode: 2851, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570400/600000: episode: 2852, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570600/600000: episode: 2853, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570800/600000: episode: 2854, duration: 1.630s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571000/600000: episode: 2855, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571200/600000: episode: 2856, duration: 1.262s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571400/600000: episode: 2857, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571600/600000: episode: 2858, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571800/600000: episode: 2859, duration: 1.248s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572000/600000: episode: 2860, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572200/600000: episode: 2861, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572400/600000: episode: 2862, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572600/600000: episode: 2863, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572800/600000: episode: 2864, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573000/600000: episode: 2865, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573200/600000: episode: 2866, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573400/600000: episode: 2867, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573600/600000: episode: 2868, duration: 1.252s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573800/600000: episode: 2869, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574000/600000: episode: 2870, duration: 1.228s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574200/600000: episode: 2871, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574400/600000: episode: 2872, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574600/600000: episode: 2873, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574800/600000: episode: 2874, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575000/600000: episode: 2875, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575200/600000: episode: 2876, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575400/600000: episode: 2877, duration: 1.216s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575600/600000: episode: 2878, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575800/600000: episode: 2879, duration: 1.217s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576000/600000: episode: 2880, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576200/600000: episode: 2881, duration: 1.663s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576400/600000: episode: 2882, duration: 1.571s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576600/600000: episode: 2883, duration: 1.230s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576800/600000: episode: 2884, duration: 1.277s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577000/600000: episode: 2885, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577200/600000: episode: 2886, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577400/600000: episode: 2887, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577600/600000: episode: 2888, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577800/600000: episode: 2889, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578000/600000: episode: 2890, duration: 1.561s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578200/600000: episode: 2891, duration: 1.669s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578400/600000: episode: 2892, duration: 1.224s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578600/600000: episode: 2893, duration: 1.246s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578800/600000: episode: 2894, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579000/600000: episode: 2895, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579200/600000: episode: 2896, duration: 1.223s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579400/600000: episode: 2897, duration: 1.257s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579600/600000: episode: 2898, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579800/600000: episode: 2899, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580000/600000: episode: 2900, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580200/600000: episode: 2901, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580400/600000: episode: 2902, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580600/600000: episode: 2903, duration: 1.232s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580800/600000: episode: 2904, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581000/600000: episode: 2905, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581200/600000: episode: 2906, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581400/600000: episode: 2907, duration: 1.225s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581600/600000: episode: 2908, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581800/600000: episode: 2909, duration: 1.693s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582000/600000: episode: 2910, duration: 1.608s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582200/600000: episode: 2911, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582400/600000: episode: 2912, duration: 1.241s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582600/600000: episode: 2913, duration: 1.234s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582800/600000: episode: 2914, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583000/600000: episode: 2915, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583200/600000: episode: 2916, duration: 1.231s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583400/600000: episode: 2917, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583600/600000: episode: 2918, duration: 1.445s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583800/600000: episode: 2919, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584000/600000: episode: 2920, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584200/600000: episode: 2921, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584400/600000: episode: 2922, duration: 1.239s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584600/600000: episode: 2923, duration: 1.258s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584800/600000: episode: 2924, duration: 1.243s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585000/600000: episode: 2925, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585200/600000: episode: 2926, duration: 1.229s, episode steps: 200, steps per second: 163, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585400/600000: episode: 2927, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585600/600000: episode: 2928, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585800/600000: episode: 2929, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586000/600000: episode: 2930, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586200/600000: episode: 2931, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586400/600000: episode: 2932, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586600/600000: episode: 2933, duration: 1.251s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586800/600000: episode: 2934, duration: 1.267s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587000/600000: episode: 2935, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587200/600000: episode: 2936, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587400/600000: episode: 2937, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587600/600000: episode: 2938, duration: 1.465s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587800/600000: episode: 2939, duration: 1.237s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588000/600000: episode: 2940, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588200/600000: episode: 2941, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588400/600000: episode: 2942, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588600/600000: episode: 2943, duration: 1.244s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588800/600000: episode: 2944, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589000/600000: episode: 2945, duration: 1.233s, episode steps: 200, steps per second: 162, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589200/600000: episode: 2946, duration: 1.682s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589400/600000: episode: 2947, duration: 1.681s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589600/600000: episode: 2948, duration: 1.242s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589800/600000: episode: 2949, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590000/600000: episode: 2950, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590200/600000: episode: 2951, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590400/600000: episode: 2952, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590600/600000: episode: 2953, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590800/600000: episode: 2954, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591000/600000: episode: 2955, duration: 1.689s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591200/600000: episode: 2956, duration: 1.694s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591400/600000: episode: 2957, duration: 1.259s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591600/600000: episode: 2958, duration: 1.272s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591800/600000: episode: 2959, duration: 1.253s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592000/600000: episode: 2960, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592200/600000: episode: 2961, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592400/600000: episode: 2962, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592600/600000: episode: 2963, duration: 1.240s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592800/600000: episode: 2964, duration: 1.523s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593000/600000: episode: 2965, duration: 1.746s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593200/600000: episode: 2966, duration: 1.249s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593400/600000: episode: 2967, duration: 1.219s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593600/600000: episode: 2968, duration: 1.254s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593800/600000: episode: 2969, duration: 1.221s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594000/600000: episode: 2970, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594200/600000: episode: 2971, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594400/600000: episode: 2972, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594600/600000: episode: 2973, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594800/600000: episode: 2974, duration: 2.440s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595000/600000: episode: 2975, duration: 1.743s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595200/600000: episode: 2976, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595400/600000: episode: 2977, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595600/600000: episode: 2978, duration: 1.384s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595800/600000: episode: 2979, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596000/600000: episode: 2980, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596200/600000: episode: 2981, duration: 1.394s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596400/600000: episode: 2982, duration: 1.939s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596600/600000: episode: 2983, duration: 1.402s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596800/600000: episode: 2984, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597000/600000: episode: 2985, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597200/600000: episode: 2986, duration: 1.282s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597400/600000: episode: 2987, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597600/600000: episode: 2988, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597800/600000: episode: 2989, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598000/600000: episode: 2990, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598200/600000: episode: 2991, duration: 1.816s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598400/600000: episode: 2992, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598600/600000: episode: 2993, duration: 1.273s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598800/600000: episode: 2994, duration: 1.255s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599000/600000: episode: 2995, duration: 1.247s, episode steps: 200, steps per second: 160, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599200/600000: episode: 2996, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599400/600000: episode: 2997, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599600/600000: episode: 2998, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599800/600000: episode: 2999, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 600000/600000: episode: 3000, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "done, took 4054.172 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de construcción de una red neuronal y entrenamiento con SARSA\n",
        "\n",
        "########################## Creación del Ambiente ##########################\n",
        "env = gym.make('MountainCar-v0') #crea el ambiente utilizando gym.\n",
        "np.random.seed(613) #semilla aleatoria\n",
        "env.seed(613)\n",
        "nb_actions = env.action_space.n #obtiene la cantidad de acciones (3)\n",
        "\n",
        "###################### Creación de la Red Neuronal ########################\n",
        "# Modelo secuencial, una capa después de otra\n",
        "model = Sequential()\n",
        "\n",
        "#Capa de Entrada\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "#Capas \"Escondidas\". Aquí se pueden añadir varias capas con distinta cantidad de neuronas\n",
        "model.add(Dense(200,activation='relu'))\n",
        "\n",
        "#Capa de Salida\n",
        "model.add(Dense(nb_actions,activation='relu'))\n",
        "\n",
        "print(model.summary()) #muestra las neuronas y parámetros de la red\n",
        "\n",
        "###################### Configuración del Modelo ##########################\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1) #límite de la memoria\n",
        "policy = BoltzmannQPolicy() #política inicial\n",
        "\n",
        "#se crea el agente de SARSA\n",
        "sarsa_solucion_4 = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=100, policy=policy)\n",
        "sarsa_solucion_4.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "\n",
        "model.save_weights('model_sarsa_solucion_4_valoresiniciales.h5') #guarda los valores iniciales\n",
        "\n",
        "############################ Entrenamiento ###############################\n",
        "#Se entrena el agente por 10,000 pasos de tiempo y se guardan los resultados\n",
        "\n",
        "#carga los pesos iniciales para entrenar desde 0 (buena practica)\n",
        "sarsa_solucion_4.load_weights('model_sarsa_solucion_4_valoresiniciales.h5')\n",
        "\n",
        "#Comienza a entrenar\n",
        "#Guarda el historial y muestra progreso en pantalla\n",
        "historial_solucion_sarsa_ejemplo_4 = sarsa_solucion_4.fit(env, nb_steps=600000, visualize=False, verbose=2)\n",
        "\n",
        "#guarda los pesos finales (buena practica)\n",
        "sarsa_solucion_4.save_weights('model_sarsa_solucion_4_valoresfinales.h5f', overwrite=True)"
      ],
      "metadata": {
        "id": "4TIkzBj84Y41",
        "outputId": "c5ab3a5e-a9b0-491b-fd80-2fe2ea8c961f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_10 (Flatten)        (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 200)               600       \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 3)                 603       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1203 (4.70 KB)\n",
            "Trainable params: 1203 (4.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 600000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    200/600000: episode: 1, duration: 2.586s, episode steps: 200, steps per second:  77, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500277, mae: 0.334917, mean_q: 0.005207\n",
            "    400/600000: episode: 2, duration: 1.649s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "    600/600000: episode: 3, duration: 1.809s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "    800/600000: episode: 4, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1000/600000: episode: 5, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1200/600000: episode: 6, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1400/600000: episode: 7, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1600/600000: episode: 8, duration: 2.408s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1800/600000: episode: 9, duration: 1.669s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2000/600000: episode: 10, duration: 3.246s, episode steps: 200, steps per second:  62, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2200/600000: episode: 11, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2400/600000: episode: 12, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2600/600000: episode: 13, duration: 1.365s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2800/600000: episode: 14, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3000/600000: episode: 15, duration: 2.307s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3200/600000: episode: 16, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3400/600000: episode: 17, duration: 2.446s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3600/600000: episode: 18, duration: 1.495s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3800/600000: episode: 19, duration: 1.776s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4000/600000: episode: 20, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4200/600000: episode: 21, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4400/600000: episode: 22, duration: 1.744s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4600/600000: episode: 23, duration: 2.302s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4800/600000: episode: 24, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5000/600000: episode: 25, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5200/600000: episode: 26, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5400/600000: episode: 27, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5600/600000: episode: 28, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5800/600000: episode: 29, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6000/600000: episode: 30, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6200/600000: episode: 31, duration: 1.496s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6400/600000: episode: 32, duration: 2.117s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6600/600000: episode: 33, duration: 1.878s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6800/600000: episode: 34, duration: 1.481s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7000/600000: episode: 35, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7200/600000: episode: 36, duration: 1.428s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7400/600000: episode: 37, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7600/600000: episode: 38, duration: 1.477s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7800/600000: episode: 39, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8000/600000: episode: 40, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8200/600000: episode: 41, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8400/600000: episode: 42, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8600/600000: episode: 43, duration: 1.746s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8800/600000: episode: 44, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9000/600000: episode: 45, duration: 1.548s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9200/600000: episode: 46, duration: 1.495s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9400/600000: episode: 47, duration: 2.425s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9600/600000: episode: 48, duration: 2.253s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9800/600000: episode: 49, duration: 2.146s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10000/600000: episode: 50, duration: 1.986s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10200/600000: episode: 51, duration: 2.850s, episode steps: 200, steps per second:  70, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10400/600000: episode: 52, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10600/600000: episode: 53, duration: 3.377s, episode steps: 200, steps per second:  59, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10800/600000: episode: 54, duration: 1.941s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11000/600000: episode: 55, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11200/600000: episode: 56, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11400/600000: episode: 57, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11600/600000: episode: 58, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11800/600000: episode: 59, duration: 1.754s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12000/600000: episode: 60, duration: 3.693s, episode steps: 200, steps per second:  54, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12200/600000: episode: 61, duration: 2.654s, episode steps: 200, steps per second:  75, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12400/600000: episode: 62, duration: 1.621s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12600/600000: episode: 63, duration: 2.694s, episode steps: 200, steps per second:  74, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12800/600000: episode: 64, duration: 1.763s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13000/600000: episode: 65, duration: 2.440s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13200/600000: episode: 66, duration: 2.769s, episode steps: 200, steps per second:  72, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13400/600000: episode: 67, duration: 1.612s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13600/600000: episode: 68, duration: 2.301s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13800/600000: episode: 69, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14000/600000: episode: 70, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14200/600000: episode: 71, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14400/600000: episode: 72, duration: 1.931s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14600/600000: episode: 73, duration: 1.680s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14800/600000: episode: 74, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15000/600000: episode: 75, duration: 1.516s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15200/600000: episode: 76, duration: 2.120s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15400/600000: episode: 77, duration: 2.439s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15600/600000: episode: 78, duration: 2.224s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15800/600000: episode: 79, duration: 4.708s, episode steps: 200, steps per second:  42, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16000/600000: episode: 80, duration: 1.484s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16200/600000: episode: 81, duration: 1.514s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16400/600000: episode: 82, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16600/600000: episode: 83, duration: 1.459s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16800/600000: episode: 84, duration: 1.430s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17000/600000: episode: 85, duration: 1.907s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17200/600000: episode: 86, duration: 1.702s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17400/600000: episode: 87, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17600/600000: episode: 88, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17800/600000: episode: 89, duration: 1.442s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18000/600000: episode: 90, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18200/600000: episode: 91, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18400/600000: episode: 92, duration: 1.526s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18600/600000: episode: 93, duration: 1.833s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18800/600000: episode: 94, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19000/600000: episode: 95, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19200/600000: episode: 96, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19400/600000: episode: 97, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19600/600000: episode: 98, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19800/600000: episode: 99, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20000/600000: episode: 100, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20200/600000: episode: 101, duration: 1.542s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20400/600000: episode: 102, duration: 2.049s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20600/600000: episode: 103, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20800/600000: episode: 104, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21000/600000: episode: 105, duration: 1.421s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21200/600000: episode: 106, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21400/600000: episode: 107, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21600/600000: episode: 108, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21800/600000: episode: 109, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22000/600000: episode: 110, duration: 1.890s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22200/600000: episode: 111, duration: 1.678s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22400/600000: episode: 112, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22600/600000: episode: 113, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22800/600000: episode: 114, duration: 1.384s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23000/600000: episode: 115, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23200/600000: episode: 116, duration: 1.402s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23400/600000: episode: 117, duration: 1.455s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23600/600000: episode: 118, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23800/600000: episode: 119, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24000/600000: episode: 120, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24200/600000: episode: 121, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24400/600000: episode: 122, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24600/600000: episode: 123, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24800/600000: episode: 124, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25000/600000: episode: 125, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25200/600000: episode: 126, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25400/600000: episode: 127, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25600/600000: episode: 128, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25800/600000: episode: 129, duration: 1.993s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26000/600000: episode: 130, duration: 1.615s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26200/600000: episode: 131, duration: 1.442s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26400/600000: episode: 132, duration: 1.525s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26600/600000: episode: 133, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26800/600000: episode: 134, duration: 2.260s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27000/600000: episode: 135, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27200/600000: episode: 136, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27400/600000: episode: 137, duration: 1.472s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27600/600000: episode: 138, duration: 1.907s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27800/600000: episode: 139, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28000/600000: episode: 140, duration: 1.551s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28200/600000: episode: 141, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28400/600000: episode: 142, duration: 1.982s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28600/600000: episode: 143, duration: 1.724s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28800/600000: episode: 144, duration: 1.559s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29000/600000: episode: 145, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29200/600000: episode: 146, duration: 1.814s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29400/600000: episode: 147, duration: 1.481s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29600/600000: episode: 148, duration: 1.809s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29800/600000: episode: 149, duration: 2.758s, episode steps: 200, steps per second:  73, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30000/600000: episode: 150, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30200/600000: episode: 151, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30400/600000: episode: 152, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30600/600000: episode: 153, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30800/600000: episode: 154, duration: 1.810s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31000/600000: episode: 155, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31200/600000: episode: 156, duration: 2.989s, episode steps: 200, steps per second:  67, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31400/600000: episode: 157, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31600/600000: episode: 158, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31800/600000: episode: 159, duration: 1.648s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32000/600000: episode: 160, duration: 1.684s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32200/600000: episode: 161, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32400/600000: episode: 162, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32600/600000: episode: 163, duration: 1.790s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32800/600000: episode: 164, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33000/600000: episode: 165, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33200/600000: episode: 166, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33400/600000: episode: 167, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33600/600000: episode: 168, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33800/600000: episode: 169, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34000/600000: episode: 170, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34200/600000: episode: 171, duration: 1.388s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34400/600000: episode: 172, duration: 1.976s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34600/600000: episode: 173, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34800/600000: episode: 174, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35000/600000: episode: 175, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35200/600000: episode: 176, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35400/600000: episode: 177, duration: 1.602s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35600/600000: episode: 178, duration: 1.824s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35800/600000: episode: 179, duration: 1.681s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36000/600000: episode: 180, duration: 2.312s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36200/600000: episode: 181, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36400/600000: episode: 182, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36600/600000: episode: 183, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36800/600000: episode: 184, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37000/600000: episode: 185, duration: 1.621s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37200/600000: episode: 186, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37400/600000: episode: 187, duration: 1.725s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37600/600000: episode: 188, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37800/600000: episode: 189, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38000/600000: episode: 190, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38200/600000: episode: 191, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38400/600000: episode: 192, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38600/600000: episode: 193, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38800/600000: episode: 194, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39000/600000: episode: 195, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39200/600000: episode: 196, duration: 1.896s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39400/600000: episode: 197, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39600/600000: episode: 198, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39800/600000: episode: 199, duration: 1.747s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40000/600000: episode: 200, duration: 1.719s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40200/600000: episode: 201, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40400/600000: episode: 202, duration: 4.249s, episode steps: 200, steps per second:  47, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40600/600000: episode: 203, duration: 3.438s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40800/600000: episode: 204, duration: 1.942s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41000/600000: episode: 205, duration: 1.602s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41200/600000: episode: 206, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41400/600000: episode: 207, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41600/600000: episode: 208, duration: 2.102s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41800/600000: episode: 209, duration: 2.246s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42000/600000: episode: 210, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42200/600000: episode: 211, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42400/600000: episode: 212, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42600/600000: episode: 213, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42800/600000: episode: 214, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43000/600000: episode: 215, duration: 1.604s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43200/600000: episode: 216, duration: 1.781s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43400/600000: episode: 217, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43600/600000: episode: 218, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43800/600000: episode: 219, duration: 1.662s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44000/600000: episode: 220, duration: 1.501s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44200/600000: episode: 221, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44400/600000: episode: 222, duration: 1.539s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44600/600000: episode: 223, duration: 1.725s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44800/600000: episode: 224, duration: 2.991s, episode steps: 200, steps per second:  67, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45000/600000: episode: 225, duration: 1.444s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45200/600000: episode: 226, duration: 1.467s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45400/600000: episode: 227, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45600/600000: episode: 228, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45800/600000: episode: 229, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46000/600000: episode: 230, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46200/600000: episode: 231, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46400/600000: episode: 232, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46600/600000: episode: 233, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46800/600000: episode: 234, duration: 1.450s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47000/600000: episode: 235, duration: 2.134s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47200/600000: episode: 236, duration: 1.533s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47400/600000: episode: 237, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47600/600000: episode: 238, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47800/600000: episode: 239, duration: 2.658s, episode steps: 200, steps per second:  75, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48000/600000: episode: 240, duration: 2.004s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48200/600000: episode: 241, duration: 1.329s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48400/600000: episode: 242, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48600/600000: episode: 243, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48800/600000: episode: 244, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49000/600000: episode: 245, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49200/600000: episode: 246, duration: 1.662s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49400/600000: episode: 247, duration: 2.102s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49600/600000: episode: 248, duration: 1.734s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49800/600000: episode: 249, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50000/600000: episode: 250, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50200/600000: episode: 251, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50400/600000: episode: 252, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50600/600000: episode: 253, duration: 1.404s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50800/600000: episode: 254, duration: 1.457s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51000/600000: episode: 255, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51200/600000: episode: 256, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51400/600000: episode: 257, duration: 1.598s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51600/600000: episode: 258, duration: 1.416s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51800/600000: episode: 259, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52000/600000: episode: 260, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52200/600000: episode: 261, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52400/600000: episode: 262, duration: 1.402s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52600/600000: episode: 263, duration: 2.595s, episode steps: 200, steps per second:  77, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52800/600000: episode: 264, duration: 2.227s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53000/600000: episode: 265, duration: 1.494s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53200/600000: episode: 266, duration: 1.891s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53400/600000: episode: 267, duration: 2.020s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53600/600000: episode: 268, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53800/600000: episode: 269, duration: 1.443s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54000/600000: episode: 270, duration: 2.852s, episode steps: 200, steps per second:  70, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54200/600000: episode: 271, duration: 1.488s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54400/600000: episode: 272, duration: 1.467s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54600/600000: episode: 273, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54800/600000: episode: 274, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55000/600000: episode: 275, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55200/600000: episode: 276, duration: 1.534s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55400/600000: episode: 277, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55600/600000: episode: 278, duration: 1.977s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55800/600000: episode: 279, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56000/600000: episode: 280, duration: 1.485s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56200/600000: episode: 281, duration: 1.427s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56400/600000: episode: 282, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56600/600000: episode: 283, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  56800/600000: episode: 284, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57000/600000: episode: 285, duration: 2.509s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57200/600000: episode: 286, duration: 1.659s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57400/600000: episode: 287, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57600/600000: episode: 288, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  57800/600000: episode: 289, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58000/600000: episode: 290, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58200/600000: episode: 291, duration: 1.561s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58400/600000: episode: 292, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58600/600000: episode: 293, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  58800/600000: episode: 294, duration: 2.031s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59000/600000: episode: 295, duration: 1.693s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59200/600000: episode: 296, duration: 1.991s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59400/600000: episode: 297, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59600/600000: episode: 298, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  59800/600000: episode: 299, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60000/600000: episode: 300, duration: 1.778s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60200/600000: episode: 301, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60400/600000: episode: 302, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60600/600000: episode: 303, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  60800/600000: episode: 304, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61000/600000: episode: 305, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61200/600000: episode: 306, duration: 1.694s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61400/600000: episode: 307, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61600/600000: episode: 308, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  61800/600000: episode: 309, duration: 2.012s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62000/600000: episode: 310, duration: 1.593s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62200/600000: episode: 311, duration: 1.652s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62400/600000: episode: 312, duration: 1.955s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62600/600000: episode: 313, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  62800/600000: episode: 314, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63000/600000: episode: 315, duration: 1.416s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63200/600000: episode: 316, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63400/600000: episode: 317, duration: 2.461s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63600/600000: episode: 318, duration: 1.512s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  63800/600000: episode: 319, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64000/600000: episode: 320, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64200/600000: episode: 321, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64400/600000: episode: 322, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64600/600000: episode: 323, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  64800/600000: episode: 324, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65000/600000: episode: 325, duration: 1.909s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65200/600000: episode: 326, duration: 1.580s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65400/600000: episode: 327, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65600/600000: episode: 328, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  65800/600000: episode: 329, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66000/600000: episode: 330, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66200/600000: episode: 331, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66400/600000: episode: 332, duration: 1.694s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66600/600000: episode: 333, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  66800/600000: episode: 334, duration: 1.936s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67000/600000: episode: 335, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67200/600000: episode: 336, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67400/600000: episode: 337, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67600/600000: episode: 338, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  67800/600000: episode: 339, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68000/600000: episode: 340, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68200/600000: episode: 341, duration: 1.471s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68400/600000: episode: 342, duration: 1.926s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68600/600000: episode: 343, duration: 1.612s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  68800/600000: episode: 344, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69000/600000: episode: 345, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69200/600000: episode: 346, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69400/600000: episode: 347, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69600/600000: episode: 348, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  69800/600000: episode: 349, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70000/600000: episode: 350, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70200/600000: episode: 351, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70400/600000: episode: 352, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70600/600000: episode: 353, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  70800/600000: episode: 354, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71000/600000: episode: 355, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71200/600000: episode: 356, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71400/600000: episode: 357, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71600/600000: episode: 358, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  71800/600000: episode: 359, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72000/600000: episode: 360, duration: 1.825s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72200/600000: episode: 361, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72400/600000: episode: 362, duration: 1.728s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72600/600000: episode: 363, duration: 1.547s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  72800/600000: episode: 364, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73000/600000: episode: 365, duration: 2.829s, episode steps: 200, steps per second:  71, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73200/600000: episode: 366, duration: 1.628s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73400/600000: episode: 367, duration: 2.985s, episode steps: 200, steps per second:  67, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73600/600000: episode: 368, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  73800/600000: episode: 369, duration: 1.445s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74000/600000: episode: 370, duration: 1.596s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74200/600000: episode: 371, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74400/600000: episode: 372, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74600/600000: episode: 373, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  74800/600000: episode: 374, duration: 3.002s, episode steps: 200, steps per second:  67, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75000/600000: episode: 375, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75200/600000: episode: 376, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75400/600000: episode: 377, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75600/600000: episode: 378, duration: 1.736s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  75800/600000: episode: 379, duration: 1.481s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76000/600000: episode: 380, duration: 1.694s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76200/600000: episode: 381, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76400/600000: episode: 382, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76600/600000: episode: 383, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  76800/600000: episode: 384, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77000/600000: episode: 385, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77200/600000: episode: 386, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77400/600000: episode: 387, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77600/600000: episode: 388, duration: 1.421s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  77800/600000: episode: 389, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78000/600000: episode: 390, duration: 2.043s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78200/600000: episode: 391, duration: 1.788s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78400/600000: episode: 392, duration: 1.969s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78600/600000: episode: 393, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  78800/600000: episode: 394, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79000/600000: episode: 395, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79200/600000: episode: 396, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79400/600000: episode: 397, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79600/600000: episode: 398, duration: 1.956s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  79800/600000: episode: 399, duration: 1.708s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80000/600000: episode: 400, duration: 1.428s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80200/600000: episode: 401, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80400/600000: episode: 402, duration: 1.440s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80600/600000: episode: 403, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  80800/600000: episode: 404, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81000/600000: episode: 405, duration: 1.394s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81200/600000: episode: 406, duration: 2.205s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81400/600000: episode: 407, duration: 2.036s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81600/600000: episode: 408, duration: 1.942s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  81800/600000: episode: 409, duration: 1.696s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82000/600000: episode: 410, duration: 1.524s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82200/600000: episode: 411, duration: 2.597s, episode steps: 200, steps per second:  77, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82400/600000: episode: 412, duration: 1.629s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82600/600000: episode: 413, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  82800/600000: episode: 414, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83000/600000: episode: 415, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83200/600000: episode: 416, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83400/600000: episode: 417, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83600/600000: episode: 418, duration: 1.442s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  83800/600000: episode: 419, duration: 1.430s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84000/600000: episode: 420, duration: 1.545s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84200/600000: episode: 421, duration: 2.091s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84400/600000: episode: 422, duration: 1.394s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84600/600000: episode: 423, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  84800/600000: episode: 424, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85000/600000: episode: 425, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85200/600000: episode: 426, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85400/600000: episode: 427, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85600/600000: episode: 428, duration: 1.398s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  85800/600000: episode: 429, duration: 2.013s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86000/600000: episode: 430, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86200/600000: episode: 431, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86400/600000: episode: 432, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86600/600000: episode: 433, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  86800/600000: episode: 434, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87000/600000: episode: 435, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87200/600000: episode: 436, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87400/600000: episode: 437, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87600/600000: episode: 438, duration: 1.953s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  87800/600000: episode: 439, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88000/600000: episode: 440, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88200/600000: episode: 441, duration: 1.502s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88400/600000: episode: 442, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88600/600000: episode: 443, duration: 1.469s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  88800/600000: episode: 444, duration: 1.553s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89000/600000: episode: 445, duration: 1.541s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89200/600000: episode: 446, duration: 2.076s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89400/600000: episode: 447, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89600/600000: episode: 448, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  89800/600000: episode: 449, duration: 1.483s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90000/600000: episode: 450, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90200/600000: episode: 451, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90400/600000: episode: 452, duration: 1.953s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90600/600000: episode: 453, duration: 1.993s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  90800/600000: episode: 454, duration: 2.179s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91000/600000: episode: 455, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91200/600000: episode: 456, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91400/600000: episode: 457, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91600/600000: episode: 458, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  91800/600000: episode: 459, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92000/600000: episode: 460, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92200/600000: episode: 461, duration: 1.781s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92400/600000: episode: 462, duration: 1.642s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92600/600000: episode: 463, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  92800/600000: episode: 464, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93000/600000: episode: 465, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93200/600000: episode: 466, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93400/600000: episode: 467, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93600/600000: episode: 468, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  93800/600000: episode: 469, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94000/600000: episode: 470, duration: 1.955s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94200/600000: episode: 471, duration: 1.504s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94400/600000: episode: 472, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94600/600000: episode: 473, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  94800/600000: episode: 474, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95000/600000: episode: 475, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95200/600000: episode: 476, duration: 1.716s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95400/600000: episode: 477, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95600/600000: episode: 478, duration: 2.145s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  95800/600000: episode: 479, duration: 1.717s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96000/600000: episode: 480, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96200/600000: episode: 481, duration: 2.081s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96400/600000: episode: 482, duration: 2.181s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96600/600000: episode: 483, duration: 2.183s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  96800/600000: episode: 484, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97000/600000: episode: 485, duration: 1.940s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97200/600000: episode: 486, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97400/600000: episode: 487, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97600/600000: episode: 488, duration: 1.784s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  97800/600000: episode: 489, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98000/600000: episode: 490, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98200/600000: episode: 491, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98400/600000: episode: 492, duration: 1.624s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98600/600000: episode: 493, duration: 1.800s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  98800/600000: episode: 494, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99000/600000: episode: 495, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99200/600000: episode: 496, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99400/600000: episode: 497, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99600/600000: episode: 498, duration: 1.642s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  99800/600000: episode: 499, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100000/600000: episode: 500, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100200/600000: episode: 501, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100400/600000: episode: 502, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100600/600000: episode: 503, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 100800/600000: episode: 504, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101000/600000: episode: 505, duration: 1.738s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101200/600000: episode: 506, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101400/600000: episode: 507, duration: 1.443s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101600/600000: episode: 508, duration: 1.766s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 101800/600000: episode: 509, duration: 2.741s, episode steps: 200, steps per second:  73, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102000/600000: episode: 510, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102200/600000: episode: 511, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102400/600000: episode: 512, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102600/600000: episode: 513, duration: 1.534s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 102800/600000: episode: 514, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103000/600000: episode: 515, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103200/600000: episode: 516, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103400/600000: episode: 517, duration: 1.798s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103600/600000: episode: 518, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 103800/600000: episode: 519, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104000/600000: episode: 520, duration: 1.714s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104200/600000: episode: 521, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104400/600000: episode: 522, duration: 1.604s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104600/600000: episode: 523, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 104800/600000: episode: 524, duration: 2.259s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105000/600000: episode: 525, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105200/600000: episode: 526, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105400/600000: episode: 527, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105600/600000: episode: 528, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 105800/600000: episode: 529, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106000/600000: episode: 530, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106200/600000: episode: 531, duration: 1.476s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106400/600000: episode: 532, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106600/600000: episode: 533, duration: 1.404s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 106800/600000: episode: 534, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107000/600000: episode: 535, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107200/600000: episode: 536, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107400/600000: episode: 537, duration: 1.394s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107600/600000: episode: 538, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 107800/600000: episode: 539, duration: 2.287s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108000/600000: episode: 540, duration: 2.088s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108200/600000: episode: 541, duration: 1.388s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108400/600000: episode: 542, duration: 1.510s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108600/600000: episode: 543, duration: 1.598s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 108800/600000: episode: 544, duration: 1.452s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109000/600000: episode: 545, duration: 1.398s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109200/600000: episode: 546, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109400/600000: episode: 547, duration: 1.658s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109600/600000: episode: 548, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 109800/600000: episode: 549, duration: 1.416s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110000/600000: episode: 550, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110200/600000: episode: 551, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110400/600000: episode: 552, duration: 2.015s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110600/600000: episode: 553, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 110800/600000: episode: 554, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111000/600000: episode: 555, duration: 1.586s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111200/600000: episode: 556, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111400/600000: episode: 557, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111600/600000: episode: 558, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 111800/600000: episode: 559, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112000/600000: episode: 560, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112200/600000: episode: 561, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112400/600000: episode: 562, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112600/600000: episode: 563, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 112800/600000: episode: 564, duration: 1.702s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113000/600000: episode: 565, duration: 1.804s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113200/600000: episode: 566, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113400/600000: episode: 567, duration: 1.954s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113600/600000: episode: 568, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 113800/600000: episode: 569, duration: 1.450s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114000/600000: episode: 570, duration: 1.525s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114200/600000: episode: 571, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114400/600000: episode: 572, duration: 2.406s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114600/600000: episode: 573, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 114800/600000: episode: 574, duration: 1.569s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115000/600000: episode: 575, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115200/600000: episode: 576, duration: 1.752s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115400/600000: episode: 577, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115600/600000: episode: 578, duration: 1.430s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 115800/600000: episode: 579, duration: 1.536s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116000/600000: episode: 580, duration: 2.003s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116200/600000: episode: 581, duration: 1.432s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116400/600000: episode: 582, duration: 1.471s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116600/600000: episode: 583, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 116800/600000: episode: 584, duration: 1.957s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117000/600000: episode: 585, duration: 2.100s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117200/600000: episode: 586, duration: 2.257s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117400/600000: episode: 587, duration: 1.839s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117600/600000: episode: 588, duration: 1.383s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 117800/600000: episode: 589, duration: 1.560s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118000/600000: episode: 590, duration: 1.538s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118200/600000: episode: 591, duration: 1.722s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118400/600000: episode: 592, duration: 2.227s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118600/600000: episode: 593, duration: 2.072s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 118800/600000: episode: 594, duration: 2.032s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119000/600000: episode: 595, duration: 1.433s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119200/600000: episode: 596, duration: 1.519s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119400/600000: episode: 597, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119600/600000: episode: 598, duration: 1.547s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 119800/600000: episode: 599, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120000/600000: episode: 600, duration: 1.741s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120200/600000: episode: 601, duration: 2.095s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120400/600000: episode: 602, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120600/600000: episode: 603, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 120800/600000: episode: 604, duration: 2.075s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121000/600000: episode: 605, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121200/600000: episode: 606, duration: 1.683s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121400/600000: episode: 607, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121600/600000: episode: 608, duration: 1.775s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 121800/600000: episode: 609, duration: 2.010s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122000/600000: episode: 610, duration: 2.182s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122200/600000: episode: 611, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122400/600000: episode: 612, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122600/600000: episode: 613, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 122800/600000: episode: 614, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123000/600000: episode: 615, duration: 1.452s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123200/600000: episode: 616, duration: 2.118s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123400/600000: episode: 617, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123600/600000: episode: 618, duration: 1.998s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 123800/600000: episode: 619, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124000/600000: episode: 620, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124200/600000: episode: 621, duration: 1.421s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124400/600000: episode: 622, duration: 1.384s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124600/600000: episode: 623, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 124800/600000: episode: 624, duration: 2.721s, episode steps: 200, steps per second:  74, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125000/600000: episode: 625, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125200/600000: episode: 626, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125400/600000: episode: 627, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125600/600000: episode: 628, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 125800/600000: episode: 629, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126000/600000: episode: 630, duration: 2.259s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126200/600000: episode: 631, duration: 3.135s, episode steps: 200, steps per second:  64, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126400/600000: episode: 632, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126600/600000: episode: 633, duration: 1.396s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 126800/600000: episode: 634, duration: 2.041s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127000/600000: episode: 635, duration: 1.948s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127200/600000: episode: 636, duration: 1.906s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127400/600000: episode: 637, duration: 2.003s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127600/600000: episode: 638, duration: 2.713s, episode steps: 200, steps per second:  74, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 127800/600000: episode: 639, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128000/600000: episode: 640, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128200/600000: episode: 641, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128400/600000: episode: 642, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128600/600000: episode: 643, duration: 1.445s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 128800/600000: episode: 644, duration: 1.561s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129000/600000: episode: 645, duration: 2.495s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129200/600000: episode: 646, duration: 1.586s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129400/600000: episode: 647, duration: 1.587s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129600/600000: episode: 648, duration: 1.621s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 129800/600000: episode: 649, duration: 1.698s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130000/600000: episode: 650, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130200/600000: episode: 651, duration: 1.730s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130400/600000: episode: 652, duration: 2.182s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130600/600000: episode: 653, duration: 1.681s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 130800/600000: episode: 654, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131000/600000: episode: 655, duration: 1.581s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131200/600000: episode: 656, duration: 1.449s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131400/600000: episode: 657, duration: 1.481s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131600/600000: episode: 658, duration: 1.402s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 131800/600000: episode: 659, duration: 1.539s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132000/600000: episode: 660, duration: 3.174s, episode steps: 200, steps per second:  63, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132200/600000: episode: 661, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132400/600000: episode: 662, duration: 1.433s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132600/600000: episode: 663, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 132800/600000: episode: 664, duration: 1.401s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133000/600000: episode: 665, duration: 1.597s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133200/600000: episode: 666, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133400/600000: episode: 667, duration: 1.926s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133600/600000: episode: 668, duration: 1.833s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 133800/600000: episode: 669, duration: 1.493s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134000/600000: episode: 670, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134200/600000: episode: 671, duration: 1.448s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134400/600000: episode: 672, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134600/600000: episode: 673, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 134800/600000: episode: 674, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135000/600000: episode: 675, duration: 2.988s, episode steps: 200, steps per second:  67, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135200/600000: episode: 676, duration: 2.073s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135400/600000: episode: 677, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135600/600000: episode: 678, duration: 1.457s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 135800/600000: episode: 679, duration: 1.594s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136000/600000: episode: 680, duration: 2.050s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136200/600000: episode: 681, duration: 2.662s, episode steps: 200, steps per second:  75, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136400/600000: episode: 682, duration: 1.764s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136600/600000: episode: 683, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 136800/600000: episode: 684, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137000/600000: episode: 685, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137200/600000: episode: 686, duration: 1.505s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137400/600000: episode: 687, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137600/600000: episode: 688, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 137800/600000: episode: 689, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138000/600000: episode: 690, duration: 1.985s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138200/600000: episode: 691, duration: 1.399s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138400/600000: episode: 692, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138600/600000: episode: 693, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 138800/600000: episode: 694, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139000/600000: episode: 695, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139200/600000: episode: 696, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139400/600000: episode: 697, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139600/600000: episode: 698, duration: 2.011s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 139800/600000: episode: 699, duration: 1.554s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140000/600000: episode: 700, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140200/600000: episode: 701, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140400/600000: episode: 702, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140600/600000: episode: 703, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 140800/600000: episode: 704, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141000/600000: episode: 705, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141200/600000: episode: 706, duration: 1.712s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141400/600000: episode: 707, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141600/600000: episode: 708, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 141800/600000: episode: 709, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142000/600000: episode: 710, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142200/600000: episode: 711, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142400/600000: episode: 712, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142600/600000: episode: 713, duration: 1.375s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 142800/600000: episode: 714, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143000/600000: episode: 715, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143200/600000: episode: 716, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143400/600000: episode: 717, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143600/600000: episode: 718, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 143800/600000: episode: 719, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144000/600000: episode: 720, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144200/600000: episode: 721, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144400/600000: episode: 722, duration: 1.384s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144600/600000: episode: 723, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 144800/600000: episode: 724, duration: 1.755s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145000/600000: episode: 725, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145200/600000: episode: 726, duration: 2.649s, episode steps: 200, steps per second:  75, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145400/600000: episode: 727, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145600/600000: episode: 728, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 145800/600000: episode: 729, duration: 1.433s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146000/600000: episode: 730, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146200/600000: episode: 731, duration: 1.990s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146400/600000: episode: 732, duration: 1.916s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146600/600000: episode: 733, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 146800/600000: episode: 734, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147000/600000: episode: 735, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147200/600000: episode: 736, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147400/600000: episode: 737, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147600/600000: episode: 738, duration: 1.574s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 147800/600000: episode: 739, duration: 2.243s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148000/600000: episode: 740, duration: 2.233s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148200/600000: episode: 741, duration: 1.978s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148400/600000: episode: 742, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148600/600000: episode: 743, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 148800/600000: episode: 744, duration: 1.496s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149000/600000: episode: 745, duration: 1.560s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149200/600000: episode: 746, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149400/600000: episode: 747, duration: 1.616s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149600/600000: episode: 748, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 149800/600000: episode: 749, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150000/600000: episode: 750, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150200/600000: episode: 751, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150400/600000: episode: 752, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150600/600000: episode: 753, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 150800/600000: episode: 754, duration: 1.563s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151000/600000: episode: 755, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151200/600000: episode: 756, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151400/600000: episode: 757, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151600/600000: episode: 758, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 151800/600000: episode: 759, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152000/600000: episode: 760, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152200/600000: episode: 761, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152400/600000: episode: 762, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152600/600000: episode: 763, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 152800/600000: episode: 764, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153000/600000: episode: 765, duration: 1.477s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153200/600000: episode: 766, duration: 1.771s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153400/600000: episode: 767, duration: 1.538s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153600/600000: episode: 768, duration: 1.498s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 153800/600000: episode: 769, duration: 1.402s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154000/600000: episode: 770, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154200/600000: episode: 771, duration: 1.982s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154400/600000: episode: 772, duration: 1.574s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154600/600000: episode: 773, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 154800/600000: episode: 774, duration: 1.513s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155000/600000: episode: 775, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155200/600000: episode: 776, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155400/600000: episode: 777, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155600/600000: episode: 778, duration: 1.462s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 155800/600000: episode: 779, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156000/600000: episode: 780, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156200/600000: episode: 781, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156400/600000: episode: 782, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156600/600000: episode: 783, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 156800/600000: episode: 784, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157000/600000: episode: 785, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157200/600000: episode: 786, duration: 1.482s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157400/600000: episode: 787, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157600/600000: episode: 788, duration: 2.033s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 157800/600000: episode: 789, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158000/600000: episode: 790, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158200/600000: episode: 791, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158400/600000: episode: 792, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158600/600000: episode: 793, duration: 1.537s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 158800/600000: episode: 794, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159000/600000: episode: 795, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159200/600000: episode: 796, duration: 2.475s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159400/600000: episode: 797, duration: 1.772s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159600/600000: episode: 798, duration: 1.506s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 159800/600000: episode: 799, duration: 1.609s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160000/600000: episode: 800, duration: 1.715s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160200/600000: episode: 801, duration: 1.616s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160400/600000: episode: 802, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160600/600000: episode: 803, duration: 2.925s, episode steps: 200, steps per second:  68, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 160800/600000: episode: 804, duration: 2.219s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161000/600000: episode: 805, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161200/600000: episode: 806, duration: 2.070s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161400/600000: episode: 807, duration: 1.683s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161600/600000: episode: 808, duration: 1.566s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 161800/600000: episode: 809, duration: 2.188s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162000/600000: episode: 810, duration: 2.184s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162200/600000: episode: 811, duration: 1.964s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162400/600000: episode: 812, duration: 2.025s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162600/600000: episode: 813, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 162800/600000: episode: 814, duration: 1.820s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163000/600000: episode: 815, duration: 2.132s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163200/600000: episode: 816, duration: 2.294s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163400/600000: episode: 817, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163600/600000: episode: 818, duration: 1.978s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 163800/600000: episode: 819, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164000/600000: episode: 820, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164200/600000: episode: 821, duration: 1.927s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164400/600000: episode: 822, duration: 2.004s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164600/600000: episode: 823, duration: 1.737s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 164800/600000: episode: 824, duration: 1.683s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165000/600000: episode: 825, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165200/600000: episode: 826, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165400/600000: episode: 827, duration: 2.231s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165600/600000: episode: 828, duration: 1.684s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 165800/600000: episode: 829, duration: 2.040s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166000/600000: episode: 830, duration: 1.553s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166200/600000: episode: 831, duration: 1.445s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166400/600000: episode: 832, duration: 1.469s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166600/600000: episode: 833, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 166800/600000: episode: 834, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167000/600000: episode: 835, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167200/600000: episode: 836, duration: 1.991s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167400/600000: episode: 837, duration: 2.095s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167600/600000: episode: 838, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 167800/600000: episode: 839, duration: 1.564s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168000/600000: episode: 840, duration: 1.627s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168200/600000: episode: 841, duration: 1.505s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168400/600000: episode: 842, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168600/600000: episode: 843, duration: 1.559s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 168800/600000: episode: 844, duration: 1.992s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169000/600000: episode: 845, duration: 1.504s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169200/600000: episode: 846, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169400/600000: episode: 847, duration: 1.993s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169600/600000: episode: 848, duration: 1.535s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 169800/600000: episode: 849, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170000/600000: episode: 850, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170200/600000: episode: 851, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170400/600000: episode: 852, duration: 1.946s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170600/600000: episode: 853, duration: 1.432s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 170800/600000: episode: 854, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171000/600000: episode: 855, duration: 1.440s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171200/600000: episode: 856, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171400/600000: episode: 857, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171600/600000: episode: 858, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 171800/600000: episode: 859, duration: 1.606s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172000/600000: episode: 860, duration: 1.999s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172200/600000: episode: 861, duration: 1.401s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172400/600000: episode: 862, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172600/600000: episode: 863, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 172800/600000: episode: 864, duration: 1.384s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173000/600000: episode: 865, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173200/600000: episode: 866, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173400/600000: episode: 867, duration: 1.502s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173600/600000: episode: 868, duration: 2.232s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 173800/600000: episode: 869, duration: 1.401s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174000/600000: episode: 870, duration: 1.394s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174200/600000: episode: 871, duration: 1.421s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174400/600000: episode: 872, duration: 1.347s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174600/600000: episode: 873, duration: 2.163s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 174800/600000: episode: 874, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175000/600000: episode: 875, duration: 1.701s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175200/600000: episode: 876, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175400/600000: episode: 877, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175600/600000: episode: 878, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 175800/600000: episode: 879, duration: 1.401s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176000/600000: episode: 880, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176200/600000: episode: 881, duration: 2.017s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176400/600000: episode: 882, duration: 1.430s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176600/600000: episode: 883, duration: 2.351s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 176800/600000: episode: 884, duration: 1.458s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177000/600000: episode: 885, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177200/600000: episode: 886, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177400/600000: episode: 887, duration: 1.421s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177600/600000: episode: 888, duration: 1.814s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 177800/600000: episode: 889, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178000/600000: episode: 890, duration: 1.529s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178200/600000: episode: 891, duration: 2.600s, episode steps: 200, steps per second:  77, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178400/600000: episode: 892, duration: 1.779s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178600/600000: episode: 893, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 178800/600000: episode: 894, duration: 1.596s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179000/600000: episode: 895, duration: 1.457s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179200/600000: episode: 896, duration: 1.956s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179400/600000: episode: 897, duration: 2.832s, episode steps: 200, steps per second:  71, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179600/600000: episode: 898, duration: 1.896s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 179800/600000: episode: 899, duration: 2.000s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180000/600000: episode: 900, duration: 1.986s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180200/600000: episode: 901, duration: 1.721s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180400/600000: episode: 902, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180600/600000: episode: 903, duration: 1.752s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 180800/600000: episode: 904, duration: 2.141s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181000/600000: episode: 905, duration: 1.725s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181200/600000: episode: 906, duration: 1.594s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181400/600000: episode: 907, duration: 1.730s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181600/600000: episode: 908, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 181800/600000: episode: 909, duration: 1.523s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182000/600000: episode: 910, duration: 1.475s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182200/600000: episode: 911, duration: 1.681s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182400/600000: episode: 912, duration: 2.631s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182600/600000: episode: 913, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 182800/600000: episode: 914, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183000/600000: episode: 915, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183200/600000: episode: 916, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183400/600000: episode: 917, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183600/600000: episode: 918, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 183800/600000: episode: 919, duration: 1.515s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184000/600000: episode: 920, duration: 1.948s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184200/600000: episode: 921, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184400/600000: episode: 922, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184600/600000: episode: 923, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 184800/600000: episode: 924, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185000/600000: episode: 925, duration: 1.329s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185200/600000: episode: 926, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185400/600000: episode: 927, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185600/600000: episode: 928, duration: 1.788s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 185800/600000: episode: 929, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186000/600000: episode: 930, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186200/600000: episode: 931, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186400/600000: episode: 932, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186600/600000: episode: 933, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 186800/600000: episode: 934, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187000/600000: episode: 935, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187200/600000: episode: 936, duration: 1.441s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187400/600000: episode: 937, duration: 2.017s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187600/600000: episode: 938, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 187800/600000: episode: 939, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188000/600000: episode: 940, duration: 1.428s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188200/600000: episode: 941, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188400/600000: episode: 942, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188600/600000: episode: 943, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 188800/600000: episode: 944, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189000/600000: episode: 945, duration: 1.738s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189200/600000: episode: 946, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189400/600000: episode: 947, duration: 1.347s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189600/600000: episode: 948, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 189800/600000: episode: 949, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190000/600000: episode: 950, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190200/600000: episode: 951, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190400/600000: episode: 952, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190600/600000: episode: 953, duration: 1.467s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 190800/600000: episode: 954, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191000/600000: episode: 955, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191200/600000: episode: 956, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191400/600000: episode: 957, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191600/600000: episode: 958, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 191800/600000: episode: 959, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192000/600000: episode: 960, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192200/600000: episode: 961, duration: 2.004s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192400/600000: episode: 962, duration: 2.043s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192600/600000: episode: 963, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 192800/600000: episode: 964, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193000/600000: episode: 965, duration: 1.488s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193200/600000: episode: 966, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193400/600000: episode: 967, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193600/600000: episode: 968, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 193800/600000: episode: 969, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194000/600000: episode: 970, duration: 1.804s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194200/600000: episode: 971, duration: 2.165s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194400/600000: episode: 972, duration: 2.155s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194600/600000: episode: 973, duration: 1.642s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 194800/600000: episode: 974, duration: 1.566s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195000/600000: episode: 975, duration: 1.665s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195200/600000: episode: 976, duration: 1.706s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195400/600000: episode: 977, duration: 2.777s, episode steps: 200, steps per second:  72, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195600/600000: episode: 978, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 195800/600000: episode: 979, duration: 1.725s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196000/600000: episode: 980, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196200/600000: episode: 981, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196400/600000: episode: 982, duration: 1.802s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196600/600000: episode: 983, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 196800/600000: episode: 984, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197000/600000: episode: 985, duration: 2.302s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197200/600000: episode: 986, duration: 1.605s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197400/600000: episode: 987, duration: 1.583s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197600/600000: episode: 988, duration: 1.474s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 197800/600000: episode: 989, duration: 1.567s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198000/600000: episode: 990, duration: 1.634s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198200/600000: episode: 991, duration: 2.656s, episode steps: 200, steps per second:  75, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198400/600000: episode: 992, duration: 2.277s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198600/600000: episode: 993, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 198800/600000: episode: 994, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199000/600000: episode: 995, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199200/600000: episode: 996, duration: 1.762s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199400/600000: episode: 997, duration: 1.449s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199600/600000: episode: 998, duration: 1.448s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 199800/600000: episode: 999, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200000/600000: episode: 1000, duration: 1.629s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200200/600000: episode: 1001, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200400/600000: episode: 1002, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200600/600000: episode: 1003, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 200800/600000: episode: 1004, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201000/600000: episode: 1005, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201200/600000: episode: 1006, duration: 1.450s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201400/600000: episode: 1007, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201600/600000: episode: 1008, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 201800/600000: episode: 1009, duration: 1.388s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202000/600000: episode: 1010, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202200/600000: episode: 1011, duration: 1.384s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202400/600000: episode: 1012, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202600/600000: episode: 1013, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 202800/600000: episode: 1014, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203000/600000: episode: 1015, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203200/600000: episode: 1016, duration: 1.978s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203400/600000: episode: 1017, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203600/600000: episode: 1018, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 203800/600000: episode: 1019, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204000/600000: episode: 1020, duration: 1.656s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204200/600000: episode: 1021, duration: 1.994s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204400/600000: episode: 1022, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204600/600000: episode: 1023, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 204800/600000: episode: 1024, duration: 2.014s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205000/600000: episode: 1025, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205200/600000: episode: 1026, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205400/600000: episode: 1027, duration: 1.394s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205600/600000: episode: 1028, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 205800/600000: episode: 1029, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206000/600000: episode: 1030, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206200/600000: episode: 1031, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206400/600000: episode: 1032, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206600/600000: episode: 1033, duration: 1.710s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 206800/600000: episode: 1034, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207000/600000: episode: 1035, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207200/600000: episode: 1036, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207400/600000: episode: 1037, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207600/600000: episode: 1038, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 207800/600000: episode: 1039, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208000/600000: episode: 1040, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208200/600000: episode: 1041, duration: 1.825s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208400/600000: episode: 1042, duration: 1.638s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208600/600000: episode: 1043, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 208800/600000: episode: 1044, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209000/600000: episode: 1045, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209200/600000: episode: 1046, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209400/600000: episode: 1047, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209600/600000: episode: 1048, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 209800/600000: episode: 1049, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210000/600000: episode: 1050, duration: 1.966s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210200/600000: episode: 1051, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210400/600000: episode: 1052, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210600/600000: episode: 1053, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 210800/600000: episode: 1054, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211000/600000: episode: 1055, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211200/600000: episode: 1056, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211400/600000: episode: 1057, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211600/600000: episode: 1058, duration: 1.590s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 211800/600000: episode: 1059, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212000/600000: episode: 1060, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212200/600000: episode: 1061, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212400/600000: episode: 1062, duration: 1.418s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212600/600000: episode: 1063, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 212800/600000: episode: 1064, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213000/600000: episode: 1065, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213200/600000: episode: 1066, duration: 1.427s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213400/600000: episode: 1067, duration: 1.975s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213600/600000: episode: 1068, duration: 1.472s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 213800/600000: episode: 1069, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214000/600000: episode: 1070, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214200/600000: episode: 1071, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214400/600000: episode: 1072, duration: 1.347s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214600/600000: episode: 1073, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 214800/600000: episode: 1074, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215000/600000: episode: 1075, duration: 1.597s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215200/600000: episode: 1076, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215400/600000: episode: 1077, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215600/600000: episode: 1078, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 215800/600000: episode: 1079, duration: 1.365s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216000/600000: episode: 1080, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216200/600000: episode: 1081, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216400/600000: episode: 1082, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216600/600000: episode: 1083, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 216800/600000: episode: 1084, duration: 1.731s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217000/600000: episode: 1085, duration: 1.676s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217200/600000: episode: 1086, duration: 1.503s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217400/600000: episode: 1087, duration: 1.529s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217600/600000: episode: 1088, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 217800/600000: episode: 1089, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218000/600000: episode: 1090, duration: 1.396s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218200/600000: episode: 1091, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218400/600000: episode: 1092, duration: 1.639s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218600/600000: episode: 1093, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 218800/600000: episode: 1094, duration: 1.453s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219000/600000: episode: 1095, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219200/600000: episode: 1096, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219400/600000: episode: 1097, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219600/600000: episode: 1098, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 219800/600000: episode: 1099, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220000/600000: episode: 1100, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220200/600000: episode: 1101, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220400/600000: episode: 1102, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220600/600000: episode: 1103, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 220800/600000: episode: 1104, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221000/600000: episode: 1105, duration: 1.418s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221200/600000: episode: 1106, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221400/600000: episode: 1107, duration: 1.384s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221600/600000: episode: 1108, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 221800/600000: episode: 1109, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222000/600000: episode: 1110, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222200/600000: episode: 1111, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222400/600000: episode: 1112, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222600/600000: episode: 1113, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 222800/600000: episode: 1114, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223000/600000: episode: 1115, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223200/600000: episode: 1116, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223400/600000: episode: 1117, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223600/600000: episode: 1118, duration: 2.007s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 223800/600000: episode: 1119, duration: 1.570s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224000/600000: episode: 1120, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224200/600000: episode: 1121, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224400/600000: episode: 1122, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224600/600000: episode: 1123, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 224800/600000: episode: 1124, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225000/600000: episode: 1125, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225200/600000: episode: 1126, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225400/600000: episode: 1127, duration: 1.974s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225600/600000: episode: 1128, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 225800/600000: episode: 1129, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226000/600000: episode: 1130, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226200/600000: episode: 1131, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226400/600000: episode: 1132, duration: 1.562s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226600/600000: episode: 1133, duration: 2.050s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 226800/600000: episode: 1134, duration: 3.359s, episode steps: 200, steps per second:  60, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227000/600000: episode: 1135, duration: 2.194s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227200/600000: episode: 1136, duration: 1.738s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227400/600000: episode: 1137, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227600/600000: episode: 1138, duration: 1.692s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 227800/600000: episode: 1139, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228000/600000: episode: 1140, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228200/600000: episode: 1141, duration: 1.645s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228400/600000: episode: 1142, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228600/600000: episode: 1143, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 228800/600000: episode: 1144, duration: 2.018s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229000/600000: episode: 1145, duration: 1.973s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229200/600000: episode: 1146, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229400/600000: episode: 1147, duration: 1.554s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229600/600000: episode: 1148, duration: 1.961s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 229800/600000: episode: 1149, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230000/600000: episode: 1150, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230200/600000: episode: 1151, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230400/600000: episode: 1152, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230600/600000: episode: 1153, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 230800/600000: episode: 1154, duration: 1.492s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231000/600000: episode: 1155, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231200/600000: episode: 1156, duration: 1.960s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231400/600000: episode: 1157, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231600/600000: episode: 1158, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 231800/600000: episode: 1159, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232000/600000: episode: 1160, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232200/600000: episode: 1161, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232400/600000: episode: 1162, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232600/600000: episode: 1163, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 232800/600000: episode: 1164, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233000/600000: episode: 1165, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233200/600000: episode: 1166, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233400/600000: episode: 1167, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233600/600000: episode: 1168, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 233800/600000: episode: 1169, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234000/600000: episode: 1170, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234200/600000: episode: 1171, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234400/600000: episode: 1172, duration: 1.490s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234600/600000: episode: 1173, duration: 1.927s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 234800/600000: episode: 1174, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235000/600000: episode: 1175, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235200/600000: episode: 1176, duration: 1.438s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235400/600000: episode: 1177, duration: 1.557s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235600/600000: episode: 1178, duration: 1.416s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 235800/600000: episode: 1179, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236000/600000: episode: 1180, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236200/600000: episode: 1181, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236400/600000: episode: 1182, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236600/600000: episode: 1183, duration: 2.149s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 236800/600000: episode: 1184, duration: 2.162s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237000/600000: episode: 1185, duration: 1.698s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237200/600000: episode: 1186, duration: 1.454s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237400/600000: episode: 1187, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237600/600000: episode: 1188, duration: 2.900s, episode steps: 200, steps per second:  69, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 237800/600000: episode: 1189, duration: 1.566s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238000/600000: episode: 1190, duration: 1.776s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238200/600000: episode: 1191, duration: 1.521s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238400/600000: episode: 1192, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238600/600000: episode: 1193, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 238800/600000: episode: 1194, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239000/600000: episode: 1195, duration: 1.533s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239200/600000: episode: 1196, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239400/600000: episode: 1197, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239600/600000: episode: 1198, duration: 1.502s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 239800/600000: episode: 1199, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240000/600000: episode: 1200, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240200/600000: episode: 1201, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240400/600000: episode: 1202, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240600/600000: episode: 1203, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 240800/600000: episode: 1204, duration: 2.035s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241000/600000: episode: 1205, duration: 1.534s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241200/600000: episode: 1206, duration: 1.459s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241400/600000: episode: 1207, duration: 1.471s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241600/600000: episode: 1208, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 241800/600000: episode: 1209, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242000/600000: episode: 1210, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242200/600000: episode: 1211, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242400/600000: episode: 1212, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242600/600000: episode: 1213, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 242800/600000: episode: 1214, duration: 1.514s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243000/600000: episode: 1215, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243200/600000: episode: 1216, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243400/600000: episode: 1217, duration: 1.458s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243600/600000: episode: 1218, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 243800/600000: episode: 1219, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244000/600000: episode: 1220, duration: 1.858s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244200/600000: episode: 1221, duration: 1.983s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244400/600000: episode: 1222, duration: 1.633s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244600/600000: episode: 1223, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 244800/600000: episode: 1224, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245000/600000: episode: 1225, duration: 1.384s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245200/600000: episode: 1226, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245400/600000: episode: 1227, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245600/600000: episode: 1228, duration: 1.793s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 245800/600000: episode: 1229, duration: 1.810s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246000/600000: episode: 1230, duration: 1.496s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246200/600000: episode: 1231, duration: 1.430s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246400/600000: episode: 1232, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246600/600000: episode: 1233, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 246800/600000: episode: 1234, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247000/600000: episode: 1235, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247200/600000: episode: 1236, duration: 1.638s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247400/600000: episode: 1237, duration: 1.948s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247600/600000: episode: 1238, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 247800/600000: episode: 1239, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248000/600000: episode: 1240, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248200/600000: episode: 1241, duration: 1.485s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248400/600000: episode: 1242, duration: 1.445s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248600/600000: episode: 1243, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 248800/600000: episode: 1244, duration: 1.598s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249000/600000: episode: 1245, duration: 2.064s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249200/600000: episode: 1246, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249400/600000: episode: 1247, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249600/600000: episode: 1248, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 249800/600000: episode: 1249, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250000/600000: episode: 1250, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250200/600000: episode: 1251, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250400/600000: episode: 1252, duration: 1.438s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250600/600000: episode: 1253, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 250800/600000: episode: 1254, duration: 1.611s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251000/600000: episode: 1255, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251200/600000: episode: 1256, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251400/600000: episode: 1257, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251600/600000: episode: 1258, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 251800/600000: episode: 1259, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252000/600000: episode: 1260, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252200/600000: episode: 1261, duration: 1.605s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252400/600000: episode: 1262, duration: 2.024s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252600/600000: episode: 1263, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 252800/600000: episode: 1264, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253000/600000: episode: 1265, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253200/600000: episode: 1266, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253400/600000: episode: 1267, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253600/600000: episode: 1268, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 253800/600000: episode: 1269, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254000/600000: episode: 1270, duration: 1.782s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254200/600000: episode: 1271, duration: 1.733s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254400/600000: episode: 1272, duration: 1.365s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254600/600000: episode: 1273, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 254800/600000: episode: 1274, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255000/600000: episode: 1275, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255200/600000: episode: 1276, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255400/600000: episode: 1277, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255600/600000: episode: 1278, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 255800/600000: episode: 1279, duration: 1.896s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256000/600000: episode: 1280, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256200/600000: episode: 1281, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256400/600000: episode: 1282, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256600/600000: episode: 1283, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 256800/600000: episode: 1284, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257000/600000: episode: 1285, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257200/600000: episode: 1286, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257400/600000: episode: 1287, duration: 1.648s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257600/600000: episode: 1288, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 257800/600000: episode: 1289, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258000/600000: episode: 1290, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258200/600000: episode: 1291, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258400/600000: episode: 1292, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258600/600000: episode: 1293, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 258800/600000: episode: 1294, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259000/600000: episode: 1295, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259200/600000: episode: 1296, duration: 1.690s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259400/600000: episode: 1297, duration: 1.686s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259600/600000: episode: 1298, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 259800/600000: episode: 1299, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260000/600000: episode: 1300, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260200/600000: episode: 1301, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260400/600000: episode: 1302, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260600/600000: episode: 1303, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 260800/600000: episode: 1304, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261000/600000: episode: 1305, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261200/600000: episode: 1306, duration: 1.535s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261400/600000: episode: 1307, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261600/600000: episode: 1308, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 261800/600000: episode: 1309, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262000/600000: episode: 1310, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262200/600000: episode: 1311, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262400/600000: episode: 1312, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262600/600000: episode: 1313, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 262800/600000: episode: 1314, duration: 2.004s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263000/600000: episode: 1315, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263200/600000: episode: 1316, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263400/600000: episode: 1317, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263600/600000: episode: 1318, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 263800/600000: episode: 1319, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264000/600000: episode: 1320, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264200/600000: episode: 1321, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264400/600000: episode: 1322, duration: 1.631s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264600/600000: episode: 1323, duration: 1.769s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 264800/600000: episode: 1324, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265000/600000: episode: 1325, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265200/600000: episode: 1326, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265400/600000: episode: 1327, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265600/600000: episode: 1328, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 265800/600000: episode: 1329, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266000/600000: episode: 1330, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266200/600000: episode: 1331, duration: 1.686s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266400/600000: episode: 1332, duration: 1.715s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266600/600000: episode: 1333, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 266800/600000: episode: 1334, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267000/600000: episode: 1335, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267200/600000: episode: 1336, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267400/600000: episode: 1337, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267600/600000: episode: 1338, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 267800/600000: episode: 1339, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268000/600000: episode: 1340, duration: 1.911s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268200/600000: episode: 1341, duration: 1.522s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268400/600000: episode: 1342, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268600/600000: episode: 1343, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 268800/600000: episode: 1344, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269000/600000: episode: 1345, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269200/600000: episode: 1346, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269400/600000: episode: 1347, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269600/600000: episode: 1348, duration: 1.483s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 269800/600000: episode: 1349, duration: 1.945s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270000/600000: episode: 1350, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270200/600000: episode: 1351, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270400/600000: episode: 1352, duration: 1.270s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270600/600000: episode: 1353, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 270800/600000: episode: 1354, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271000/600000: episode: 1355, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271200/600000: episode: 1356, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271400/600000: episode: 1357, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271600/600000: episode: 1358, duration: 1.757s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 271800/600000: episode: 1359, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272000/600000: episode: 1360, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272200/600000: episode: 1361, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272400/600000: episode: 1362, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272600/600000: episode: 1363, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 272800/600000: episode: 1364, duration: 1.286s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273000/600000: episode: 1365, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273200/600000: episode: 1366, duration: 1.729s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273400/600000: episode: 1367, duration: 1.653s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273600/600000: episode: 1368, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 273800/600000: episode: 1369, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274000/600000: episode: 1370, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274200/600000: episode: 1371, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274400/600000: episode: 1372, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274600/600000: episode: 1373, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 274800/600000: episode: 1374, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275000/600000: episode: 1375, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275200/600000: episode: 1376, duration: 1.574s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275400/600000: episode: 1377, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275600/600000: episode: 1378, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 275800/600000: episode: 1379, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276000/600000: episode: 1380, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276200/600000: episode: 1381, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276400/600000: episode: 1382, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276600/600000: episode: 1383, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 276800/600000: episode: 1384, duration: 1.784s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277000/600000: episode: 1385, duration: 2.043s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277200/600000: episode: 1386, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277400/600000: episode: 1387, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277600/600000: episode: 1388, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 277800/600000: episode: 1389, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278000/600000: episode: 1390, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278200/600000: episode: 1391, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278400/600000: episode: 1392, duration: 1.347s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278600/600000: episode: 1393, duration: 1.922s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 278800/600000: episode: 1394, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279000/600000: episode: 1395, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279200/600000: episode: 1396, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279400/600000: episode: 1397, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279600/600000: episode: 1398, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 279800/600000: episode: 1399, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280000/600000: episode: 1400, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280200/600000: episode: 1401, duration: 1.552s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280400/600000: episode: 1402, duration: 1.751s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280600/600000: episode: 1403, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 280800/600000: episode: 1404, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281000/600000: episode: 1405, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281200/600000: episode: 1406, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281400/600000: episode: 1407, duration: 1.574s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281600/600000: episode: 1408, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 281800/600000: episode: 1409, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282000/600000: episode: 1410, duration: 1.883s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282200/600000: episode: 1411, duration: 1.541s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282400/600000: episode: 1412, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282600/600000: episode: 1413, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 282800/600000: episode: 1414, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283000/600000: episode: 1415, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283200/600000: episode: 1416, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283400/600000: episode: 1417, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283600/600000: episode: 1418, duration: 1.465s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 283800/600000: episode: 1419, duration: 1.891s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284000/600000: episode: 1420, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284200/600000: episode: 1421, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284400/600000: episode: 1422, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284600/600000: episode: 1423, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 284800/600000: episode: 1424, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285000/600000: episode: 1425, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285200/600000: episode: 1426, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285400/600000: episode: 1427, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285600/600000: episode: 1428, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 285800/600000: episode: 1429, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286000/600000: episode: 1430, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286200/600000: episode: 1431, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286400/600000: episode: 1432, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286600/600000: episode: 1433, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 286800/600000: episode: 1434, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287000/600000: episode: 1435, duration: 1.616s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287200/600000: episode: 1436, duration: 2.071s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287400/600000: episode: 1437, duration: 1.525s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287600/600000: episode: 1438, duration: 1.653s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 287800/600000: episode: 1439, duration: 1.501s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288000/600000: episode: 1440, duration: 1.494s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288200/600000: episode: 1441, duration: 1.383s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288400/600000: episode: 1442, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288600/600000: episode: 1443, duration: 1.506s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 288800/600000: episode: 1444, duration: 1.928s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289000/600000: episode: 1445, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289200/600000: episode: 1446, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289400/600000: episode: 1447, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289600/600000: episode: 1448, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 289800/600000: episode: 1449, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290000/600000: episode: 1450, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290200/600000: episode: 1451, duration: 1.442s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290400/600000: episode: 1452, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290600/600000: episode: 1453, duration: 1.917s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 290800/600000: episode: 1454, duration: 1.515s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291000/600000: episode: 1455, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291200/600000: episode: 1456, duration: 1.394s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291400/600000: episode: 1457, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291600/600000: episode: 1458, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 291800/600000: episode: 1459, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292000/600000: episode: 1460, duration: 1.661s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292200/600000: episode: 1461, duration: 1.806s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292400/600000: episode: 1462, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292600/600000: episode: 1463, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 292800/600000: episode: 1464, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293000/600000: episode: 1465, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293200/600000: episode: 1466, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293400/600000: episode: 1467, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293600/600000: episode: 1468, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 293800/600000: episode: 1469, duration: 1.863s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294000/600000: episode: 1470, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294200/600000: episode: 1471, duration: 1.465s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294400/600000: episode: 1472, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294600/600000: episode: 1473, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 294800/600000: episode: 1474, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295000/600000: episode: 1475, duration: 1.399s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295200/600000: episode: 1476, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295400/600000: episode: 1477, duration: 1.693s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295600/600000: episode: 1478, duration: 1.963s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 295800/600000: episode: 1479, duration: 1.515s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296000/600000: episode: 1480, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296200/600000: episode: 1481, duration: 1.453s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296400/600000: episode: 1482, duration: 1.465s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296600/600000: episode: 1483, duration: 1.428s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 296800/600000: episode: 1484, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297000/600000: episode: 1485, duration: 1.656s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297200/600000: episode: 1486, duration: 1.979s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297400/600000: episode: 1487, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297600/600000: episode: 1488, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 297800/600000: episode: 1489, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298000/600000: episode: 1490, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298200/600000: episode: 1491, duration: 1.427s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298400/600000: episode: 1492, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298600/600000: episode: 1493, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 298800/600000: episode: 1494, duration: 2.025s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299000/600000: episode: 1495, duration: 1.398s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299200/600000: episode: 1496, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299400/600000: episode: 1497, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299600/600000: episode: 1498, duration: 1.432s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 299800/600000: episode: 1499, duration: 1.436s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300000/600000: episode: 1500, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300200/600000: episode: 1501, duration: 1.418s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300400/600000: episode: 1502, duration: 1.978s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300600/600000: episode: 1503, duration: 1.574s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 300800/600000: episode: 1504, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301000/600000: episode: 1505, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301200/600000: episode: 1506, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301400/600000: episode: 1507, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301600/600000: episode: 1508, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 301800/600000: episode: 1509, duration: 1.541s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302000/600000: episode: 1510, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302200/600000: episode: 1511, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302400/600000: episode: 1512, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302600/600000: episode: 1513, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 302800/600000: episode: 1514, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303000/600000: episode: 1515, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303200/600000: episode: 1516, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303400/600000: episode: 1517, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303600/600000: episode: 1518, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 303800/600000: episode: 1519, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304000/600000: episode: 1520, duration: 1.486s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304200/600000: episode: 1521, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304400/600000: episode: 1522, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304600/600000: episode: 1523, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 304800/600000: episode: 1524, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305000/600000: episode: 1525, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305200/600000: episode: 1526, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305400/600000: episode: 1527, duration: 1.483s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305600/600000: episode: 1528, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 305800/600000: episode: 1529, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306000/600000: episode: 1530, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306200/600000: episode: 1531, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306400/600000: episode: 1532, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306600/600000: episode: 1533, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 306800/600000: episode: 1534, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307000/600000: episode: 1535, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307200/600000: episode: 1536, duration: 1.684s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307400/600000: episode: 1537, duration: 1.911s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307600/600000: episode: 1538, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 307800/600000: episode: 1539, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308000/600000: episode: 1540, duration: 1.487s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308200/600000: episode: 1541, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308400/600000: episode: 1542, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308600/600000: episode: 1543, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 308800/600000: episode: 1544, duration: 1.581s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309000/600000: episode: 1545, duration: 1.997s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309200/600000: episode: 1546, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309400/600000: episode: 1547, duration: 1.439s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309600/600000: episode: 1548, duration: 1.384s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 309800/600000: episode: 1549, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310000/600000: episode: 1550, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310200/600000: episode: 1551, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310400/600000: episode: 1552, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310600/600000: episode: 1553, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 310800/600000: episode: 1554, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311000/600000: episode: 1555, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311200/600000: episode: 1556, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311400/600000: episode: 1557, duration: 1.383s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311600/600000: episode: 1558, duration: 1.697s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 311800/600000: episode: 1559, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312000/600000: episode: 1560, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312200/600000: episode: 1561, duration: 1.516s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312400/600000: episode: 1562, duration: 1.877s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312600/600000: episode: 1563, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 312800/600000: episode: 1564, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313000/600000: episode: 1565, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313200/600000: episode: 1566, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313400/600000: episode: 1567, duration: 1.286s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313600/600000: episode: 1568, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 313800/600000: episode: 1569, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314000/600000: episode: 1570, duration: 1.575s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314200/600000: episode: 1571, duration: 1.770s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314400/600000: episode: 1572, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314600/600000: episode: 1573, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 314800/600000: episode: 1574, duration: 1.286s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315000/600000: episode: 1575, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315200/600000: episode: 1576, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315400/600000: episode: 1577, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315600/600000: episode: 1578, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 315800/600000: episode: 1579, duration: 1.837s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316000/600000: episode: 1580, duration: 1.664s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316200/600000: episode: 1581, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316400/600000: episode: 1582, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316600/600000: episode: 1583, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 316800/600000: episode: 1584, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317000/600000: episode: 1585, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317200/600000: episode: 1586, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317400/600000: episode: 1587, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317600/600000: episode: 1588, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 317800/600000: episode: 1589, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318000/600000: episode: 1590, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318200/600000: episode: 1591, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318400/600000: episode: 1592, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318600/600000: episode: 1593, duration: 1.418s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 318800/600000: episode: 1594, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319000/600000: episode: 1595, duration: 1.467s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319200/600000: episode: 1596, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319400/600000: episode: 1597, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319600/600000: episode: 1598, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 319800/600000: episode: 1599, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320000/600000: episode: 1600, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320200/600000: episode: 1601, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320400/600000: episode: 1602, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320600/600000: episode: 1603, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 320800/600000: episode: 1604, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321000/600000: episode: 1605, duration: 1.959s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321200/600000: episode: 1606, duration: 1.567s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321400/600000: episode: 1607, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321600/600000: episode: 1608, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 321800/600000: episode: 1609, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322000/600000: episode: 1610, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322200/600000: episode: 1611, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322400/600000: episode: 1612, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322600/600000: episode: 1613, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 322800/600000: episode: 1614, duration: 2.034s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323000/600000: episode: 1615, duration: 1.428s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323200/600000: episode: 1616, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323400/600000: episode: 1617, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323600/600000: episode: 1618, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 323800/600000: episode: 1619, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324000/600000: episode: 1620, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324200/600000: episode: 1621, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324400/600000: episode: 1622, duration: 1.719s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324600/600000: episode: 1623, duration: 1.739s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 324800/600000: episode: 1624, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325000/600000: episode: 1625, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325200/600000: episode: 1626, duration: 1.383s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325400/600000: episode: 1627, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325600/600000: episode: 1628, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 325800/600000: episode: 1629, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326000/600000: episode: 1630, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326200/600000: episode: 1631, duration: 1.834s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326400/600000: episode: 1632, duration: 1.498s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326600/600000: episode: 1633, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 326800/600000: episode: 1634, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327000/600000: episode: 1635, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327200/600000: episode: 1636, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327400/600000: episode: 1637, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327600/600000: episode: 1638, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 327800/600000: episode: 1639, duration: 1.483s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328000/600000: episode: 1640, duration: 2.023s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328200/600000: episode: 1641, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328400/600000: episode: 1642, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328600/600000: episode: 1643, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 328800/600000: episode: 1644, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329000/600000: episode: 1645, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329200/600000: episode: 1646, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329400/600000: episode: 1647, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329600/600000: episode: 1648, duration: 1.702s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 329800/600000: episode: 1649, duration: 1.758s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330000/600000: episode: 1650, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330200/600000: episode: 1651, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330400/600000: episode: 1652, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330600/600000: episode: 1653, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 330800/600000: episode: 1654, duration: 1.401s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331000/600000: episode: 1655, duration: 1.388s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331200/600000: episode: 1656, duration: 1.509s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331400/600000: episode: 1657, duration: 2.016s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331600/600000: episode: 1658, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 331800/600000: episode: 1659, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332000/600000: episode: 1660, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332200/600000: episode: 1661, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332400/600000: episode: 1662, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332600/600000: episode: 1663, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 332800/600000: episode: 1664, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333000/600000: episode: 1665, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333200/600000: episode: 1666, duration: 1.761s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333400/600000: episode: 1667, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333600/600000: episode: 1668, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 333800/600000: episode: 1669, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334000/600000: episode: 1670, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334200/600000: episode: 1671, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334400/600000: episode: 1672, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334600/600000: episode: 1673, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 334800/600000: episode: 1674, duration: 2.441s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335000/600000: episode: 1675, duration: 1.910s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335200/600000: episode: 1676, duration: 1.637s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335400/600000: episode: 1677, duration: 1.552s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335600/600000: episode: 1678, duration: 1.979s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 335800/600000: episode: 1679, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336000/600000: episode: 1680, duration: 1.680s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336200/600000: episode: 1681, duration: 2.067s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336400/600000: episode: 1682, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336600/600000: episode: 1683, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 336800/600000: episode: 1684, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337000/600000: episode: 1685, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337200/600000: episode: 1686, duration: 1.439s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337400/600000: episode: 1687, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337600/600000: episode: 1688, duration: 2.053s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 337800/600000: episode: 1689, duration: 1.798s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338000/600000: episode: 1690, duration: 1.986s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338200/600000: episode: 1691, duration: 1.480s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338400/600000: episode: 1692, duration: 1.706s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338600/600000: episode: 1693, duration: 1.755s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 338800/600000: episode: 1694, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339000/600000: episode: 1695, duration: 2.503s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339200/600000: episode: 1696, duration: 2.468s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339400/600000: episode: 1697, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339600/600000: episode: 1698, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 339800/600000: episode: 1699, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340000/600000: episode: 1700, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340200/600000: episode: 1701, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340400/600000: episode: 1702, duration: 1.572s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340600/600000: episode: 1703, duration: 2.572s, episode steps: 200, steps per second:  78, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 340800/600000: episode: 1704, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341000/600000: episode: 1705, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341200/600000: episode: 1706, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341400/600000: episode: 1707, duration: 1.477s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341600/600000: episode: 1708, duration: 1.534s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 341800/600000: episode: 1709, duration: 1.596s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342000/600000: episode: 1710, duration: 1.398s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342200/600000: episode: 1711, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342400/600000: episode: 1712, duration: 1.516s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342600/600000: episode: 1713, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 342800/600000: episode: 1714, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343000/600000: episode: 1715, duration: 1.453s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343200/600000: episode: 1716, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343400/600000: episode: 1717, duration: 1.852s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343600/600000: episode: 1718, duration: 2.299s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 343800/600000: episode: 1719, duration: 1.715s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344000/600000: episode: 1720, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344200/600000: episode: 1721, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344400/600000: episode: 1722, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344600/600000: episode: 1723, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 344800/600000: episode: 1724, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345000/600000: episode: 1725, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345200/600000: episode: 1726, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345400/600000: episode: 1727, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345600/600000: episode: 1728, duration: 1.477s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 345800/600000: episode: 1729, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346000/600000: episode: 1730, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346200/600000: episode: 1731, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346400/600000: episode: 1732, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346600/600000: episode: 1733, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 346800/600000: episode: 1734, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347000/600000: episode: 1735, duration: 1.774s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347200/600000: episode: 1736, duration: 1.738s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347400/600000: episode: 1737, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347600/600000: episode: 1738, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 347800/600000: episode: 1739, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348000/600000: episode: 1740, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348200/600000: episode: 1741, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348400/600000: episode: 1742, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348600/600000: episode: 1743, duration: 1.515s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 348800/600000: episode: 1744, duration: 1.927s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349000/600000: episode: 1745, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349200/600000: episode: 1746, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349400/600000: episode: 1747, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349600/600000: episode: 1748, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 349800/600000: episode: 1749, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350000/600000: episode: 1750, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350200/600000: episode: 1751, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350400/600000: episode: 1752, duration: 1.587s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350600/600000: episode: 1753, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 350800/600000: episode: 1754, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351000/600000: episode: 1755, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351200/600000: episode: 1756, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351400/600000: episode: 1757, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351600/600000: episode: 1758, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 351800/600000: episode: 1759, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352000/600000: episode: 1760, duration: 1.396s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352200/600000: episode: 1761, duration: 2.024s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352400/600000: episode: 1762, duration: 2.086s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352600/600000: episode: 1763, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 352800/600000: episode: 1764, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353000/600000: episode: 1765, duration: 2.036s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353200/600000: episode: 1766, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353400/600000: episode: 1767, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353600/600000: episode: 1768, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 353800/600000: episode: 1769, duration: 2.314s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354000/600000: episode: 1770, duration: 1.626s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354200/600000: episode: 1771, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354400/600000: episode: 1772, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354600/600000: episode: 1773, duration: 1.723s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 354800/600000: episode: 1774, duration: 1.719s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355000/600000: episode: 1775, duration: 3.547s, episode steps: 200, steps per second:  56, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355200/600000: episode: 1776, duration: 2.400s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355400/600000: episode: 1777, duration: 2.294s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355600/600000: episode: 1778, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 355800/600000: episode: 1779, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356000/600000: episode: 1780, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356200/600000: episode: 1781, duration: 2.361s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356400/600000: episode: 1782, duration: 2.478s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356600/600000: episode: 1783, duration: 1.563s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 356800/600000: episode: 1784, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357000/600000: episode: 1785, duration: 2.463s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357200/600000: episode: 1786, duration: 1.629s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357400/600000: episode: 1787, duration: 2.255s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357600/600000: episode: 1788, duration: 2.639s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 357800/600000: episode: 1789, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358000/600000: episode: 1790, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358200/600000: episode: 1791, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358400/600000: episode: 1792, duration: 1.572s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358600/600000: episode: 1793, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 358800/600000: episode: 1794, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359000/600000: episode: 1795, duration: 2.510s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359200/600000: episode: 1796, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359400/600000: episode: 1797, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359600/600000: episode: 1798, duration: 1.516s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 359800/600000: episode: 1799, duration: 1.510s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360000/600000: episode: 1800, duration: 2.101s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360200/600000: episode: 1801, duration: 1.518s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360400/600000: episode: 1802, duration: 2.207s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360600/600000: episode: 1803, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 360800/600000: episode: 1804, duration: 1.430s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361000/600000: episode: 1805, duration: 1.439s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361200/600000: episode: 1806, duration: 1.365s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361400/600000: episode: 1807, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361600/600000: episode: 1808, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 361800/600000: episode: 1809, duration: 1.918s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362000/600000: episode: 1810, duration: 1.804s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362200/600000: episode: 1811, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362400/600000: episode: 1812, duration: 1.978s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362600/600000: episode: 1813, duration: 1.442s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 362800/600000: episode: 1814, duration: 1.401s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363000/600000: episode: 1815, duration: 1.493s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363200/600000: episode: 1816, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363400/600000: episode: 1817, duration: 1.966s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363600/600000: episode: 1818, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 363800/600000: episode: 1819, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364000/600000: episode: 1820, duration: 1.454s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364200/600000: episode: 1821, duration: 1.811s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364400/600000: episode: 1822, duration: 1.440s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364600/600000: episode: 1823, duration: 1.453s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 364800/600000: episode: 1824, duration: 1.616s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365000/600000: episode: 1825, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365200/600000: episode: 1826, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365400/600000: episode: 1827, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365600/600000: episode: 1828, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 365800/600000: episode: 1829, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366000/600000: episode: 1830, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366200/600000: episode: 1831, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366400/600000: episode: 1832, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366600/600000: episode: 1833, duration: 1.928s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 366800/600000: episode: 1834, duration: 1.513s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367000/600000: episode: 1835, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367200/600000: episode: 1836, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367400/600000: episode: 1837, duration: 1.694s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367600/600000: episode: 1838, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 367800/600000: episode: 1839, duration: 1.455s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368000/600000: episode: 1840, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368200/600000: episode: 1841, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368400/600000: episode: 1842, duration: 1.719s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368600/600000: episode: 1843, duration: 1.584s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 368800/600000: episode: 1844, duration: 2.682s, episode steps: 200, steps per second:  75, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369000/600000: episode: 1845, duration: 1.477s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369200/600000: episode: 1846, duration: 2.350s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369400/600000: episode: 1847, duration: 2.290s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369600/600000: episode: 1848, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 369800/600000: episode: 1849, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370000/600000: episode: 1850, duration: 1.384s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370200/600000: episode: 1851, duration: 1.416s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370400/600000: episode: 1852, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370600/600000: episode: 1853, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 370800/600000: episode: 1854, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371000/600000: episode: 1855, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371200/600000: episode: 1856, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371400/600000: episode: 1857, duration: 1.485s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371600/600000: episode: 1858, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 371800/600000: episode: 1859, duration: 1.830s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372000/600000: episode: 1860, duration: 2.430s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372200/600000: episode: 1861, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372400/600000: episode: 1862, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372600/600000: episode: 1863, duration: 1.766s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 372800/600000: episode: 1864, duration: 1.990s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373000/600000: episode: 1865, duration: 1.514s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373200/600000: episode: 1866, duration: 1.498s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373400/600000: episode: 1867, duration: 1.508s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373600/600000: episode: 1868, duration: 1.492s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 373800/600000: episode: 1869, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374000/600000: episode: 1870, duration: 1.443s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374200/600000: episode: 1871, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374400/600000: episode: 1872, duration: 1.884s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374600/600000: episode: 1873, duration: 1.494s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 374800/600000: episode: 1874, duration: 1.463s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375000/600000: episode: 1875, duration: 1.455s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375200/600000: episode: 1876, duration: 1.427s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375400/600000: episode: 1877, duration: 1.438s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375600/600000: episode: 1878, duration: 1.504s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 375800/600000: episode: 1879, duration: 1.971s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376000/600000: episode: 1880, duration: 1.724s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376200/600000: episode: 1881, duration: 1.442s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376400/600000: episode: 1882, duration: 1.394s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376600/600000: episode: 1883, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 376800/600000: episode: 1884, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377000/600000: episode: 1885, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377200/600000: episode: 1886, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.795 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377400/600000: episode: 1887, duration: 1.557s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377600/600000: episode: 1888, duration: 1.890s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 377800/600000: episode: 1889, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378000/600000: episode: 1890, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378200/600000: episode: 1891, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378400/600000: episode: 1892, duration: 1.962s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378600/600000: episode: 1893, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 378800/600000: episode: 1894, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379000/600000: episode: 1895, duration: 2.713s, episode steps: 200, steps per second:  74, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379200/600000: episode: 1896, duration: 1.462s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379400/600000: episode: 1897, duration: 1.495s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379600/600000: episode: 1898, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 379800/600000: episode: 1899, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380000/600000: episode: 1900, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380200/600000: episode: 1901, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380400/600000: episode: 1902, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380600/600000: episode: 1903, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 380800/600000: episode: 1904, duration: 1.652s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381000/600000: episode: 1905, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381200/600000: episode: 1906, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381400/600000: episode: 1907, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381600/600000: episode: 1908, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 381800/600000: episode: 1909, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382000/600000: episode: 1910, duration: 1.278s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382200/600000: episode: 1911, duration: 1.276s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382400/600000: episode: 1912, duration: 1.883s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382600/600000: episode: 1913, duration: 1.535s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 382800/600000: episode: 1914, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383000/600000: episode: 1915, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383200/600000: episode: 1916, duration: 1.587s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383400/600000: episode: 1917, duration: 1.484s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383600/600000: episode: 1918, duration: 1.510s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 383800/600000: episode: 1919, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384000/600000: episode: 1920, duration: 1.622s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384200/600000: episode: 1921, duration: 1.776s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384400/600000: episode: 1922, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384600/600000: episode: 1923, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 384800/600000: episode: 1924, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385000/600000: episode: 1925, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385200/600000: episode: 1926, duration: 1.449s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385400/600000: episode: 1927, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385600/600000: episode: 1928, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 385800/600000: episode: 1929, duration: 2.927s, episode steps: 200, steps per second:  68, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386000/600000: episode: 1930, duration: 1.612s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386200/600000: episode: 1931, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386400/600000: episode: 1932, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386600/600000: episode: 1933, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 386800/600000: episode: 1934, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387000/600000: episode: 1935, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387200/600000: episode: 1936, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387400/600000: episode: 1937, duration: 1.994s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387600/600000: episode: 1938, duration: 1.604s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 387800/600000: episode: 1939, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388000/600000: episode: 1940, duration: 1.547s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388200/600000: episode: 1941, duration: 1.774s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388400/600000: episode: 1942, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388600/600000: episode: 1943, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 388800/600000: episode: 1944, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389000/600000: episode: 1945, duration: 1.925s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389200/600000: episode: 1946, duration: 1.922s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389400/600000: episode: 1947, duration: 1.511s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389600/600000: episode: 1948, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 389800/600000: episode: 1949, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390000/600000: episode: 1950, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390200/600000: episode: 1951, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390400/600000: episode: 1952, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390600/600000: episode: 1953, duration: 2.610s, episode steps: 200, steps per second:  77, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 390800/600000: episode: 1954, duration: 1.462s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391000/600000: episode: 1955, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391200/600000: episode: 1956, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391400/600000: episode: 1957, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391600/600000: episode: 1958, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 391800/600000: episode: 1959, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392000/600000: episode: 1960, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392200/600000: episode: 1961, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392400/600000: episode: 1962, duration: 1.802s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392600/600000: episode: 1963, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 392800/600000: episode: 1964, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393000/600000: episode: 1965, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393200/600000: episode: 1966, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393400/600000: episode: 1967, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393600/600000: episode: 1968, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 393800/600000: episode: 1969, duration: 1.630s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394000/600000: episode: 1970, duration: 1.990s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394200/600000: episode: 1971, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394400/600000: episode: 1972, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394600/600000: episode: 1973, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 394800/600000: episode: 1974, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395000/600000: episode: 1975, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395200/600000: episode: 1976, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395400/600000: episode: 1977, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395600/600000: episode: 1978, duration: 1.943s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 395800/600000: episode: 1979, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396000/600000: episode: 1980, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396200/600000: episode: 1981, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396400/600000: episode: 1982, duration: 1.432s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396600/600000: episode: 1983, duration: 1.449s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 396800/600000: episode: 1984, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397000/600000: episode: 1985, duration: 1.622s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397200/600000: episode: 1986, duration: 2.173s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397400/600000: episode: 1987, duration: 2.884s, episode steps: 200, steps per second:  69, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397600/600000: episode: 1988, duration: 1.993s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 397800/600000: episode: 1989, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398000/600000: episode: 1990, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398200/600000: episode: 1991, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398400/600000: episode: 1992, duration: 1.702s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398600/600000: episode: 1993, duration: 1.759s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 398800/600000: episode: 1994, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399000/600000: episode: 1995, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399200/600000: episode: 1996, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399400/600000: episode: 1997, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399600/600000: episode: 1998, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 399800/600000: episode: 1999, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400000/600000: episode: 2000, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400200/600000: episode: 2001, duration: 1.880s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400400/600000: episode: 2002, duration: 1.580s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400600/600000: episode: 2003, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 400800/600000: episode: 2004, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401000/600000: episode: 2005, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401200/600000: episode: 2006, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401400/600000: episode: 2007, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401600/600000: episode: 2008, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 401800/600000: episode: 2009, duration: 2.206s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402000/600000: episode: 2010, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402200/600000: episode: 2011, duration: 1.858s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402400/600000: episode: 2012, duration: 1.980s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402600/600000: episode: 2013, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 402800/600000: episode: 2014, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403000/600000: episode: 2015, duration: 1.396s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403200/600000: episode: 2016, duration: 2.694s, episode steps: 200, steps per second:  74, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403400/600000: episode: 2017, duration: 1.664s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403600/600000: episode: 2018, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 403800/600000: episode: 2019, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404000/600000: episode: 2020, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404200/600000: episode: 2021, duration: 1.384s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404400/600000: episode: 2022, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404600/600000: episode: 2023, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 404800/600000: episode: 2024, duration: 1.493s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405000/600000: episode: 2025, duration: 1.981s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405200/600000: episode: 2026, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405400/600000: episode: 2027, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405600/600000: episode: 2028, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 405800/600000: episode: 2029, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406000/600000: episode: 2030, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406200/600000: episode: 2031, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406400/600000: episode: 2032, duration: 1.401s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406600/600000: episode: 2033, duration: 2.845s, episode steps: 200, steps per second:  70, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 406800/600000: episode: 2034, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407000/600000: episode: 2035, duration: 2.057s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407200/600000: episode: 2036, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407400/600000: episode: 2037, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407600/600000: episode: 2038, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 407800/600000: episode: 2039, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408000/600000: episode: 2040, duration: 2.034s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408200/600000: episode: 2041, duration: 2.048s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408400/600000: episode: 2042, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408600/600000: episode: 2043, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 408800/600000: episode: 2044, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409000/600000: episode: 2045, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409200/600000: episode: 2046, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409400/600000: episode: 2047, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409600/600000: episode: 2048, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 409800/600000: episode: 2049, duration: 2.038s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410000/600000: episode: 2050, duration: 1.518s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410200/600000: episode: 2051, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410400/600000: episode: 2052, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410600/600000: episode: 2053, duration: 1.773s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 410800/600000: episode: 2054, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411000/600000: episode: 2055, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411200/600000: episode: 2056, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411400/600000: episode: 2057, duration: 1.808s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411600/600000: episode: 2058, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 411800/600000: episode: 2059, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412000/600000: episode: 2060, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412200/600000: episode: 2061, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412400/600000: episode: 2062, duration: 1.527s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412600/600000: episode: 2063, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 412800/600000: episode: 2064, duration: 1.481s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413000/600000: episode: 2065, duration: 2.924s, episode steps: 200, steps per second:  68, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413200/600000: episode: 2066, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413400/600000: episode: 2067, duration: 1.509s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413600/600000: episode: 2068, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 413800/600000: episode: 2069, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414000/600000: episode: 2070, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414200/600000: episode: 2071, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414400/600000: episode: 2072, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414600/600000: episode: 2073, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 414800/600000: episode: 2074, duration: 1.570s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415000/600000: episode: 2075, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415200/600000: episode: 2076, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415400/600000: episode: 2077, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415600/600000: episode: 2078, duration: 1.443s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 415800/600000: episode: 2079, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416000/600000: episode: 2080, duration: 1.421s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416200/600000: episode: 2081, duration: 1.647s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416400/600000: episode: 2082, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416600/600000: episode: 2083, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 416800/600000: episode: 2084, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417000/600000: episode: 2085, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417200/600000: episode: 2086, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417400/600000: episode: 2087, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417600/600000: episode: 2088, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 417800/600000: episode: 2089, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418000/600000: episode: 2090, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418200/600000: episode: 2091, duration: 1.639s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418400/600000: episode: 2092, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418600/600000: episode: 2093, duration: 1.433s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 418800/600000: episode: 2094, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419000/600000: episode: 2095, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419200/600000: episode: 2096, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419400/600000: episode: 2097, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419600/600000: episode: 2098, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 419800/600000: episode: 2099, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420000/600000: episode: 2100, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420200/600000: episode: 2101, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420400/600000: episode: 2102, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420600/600000: episode: 2103, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 420800/600000: episode: 2104, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421000/600000: episode: 2105, duration: 1.329s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421200/600000: episode: 2106, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421400/600000: episode: 2107, duration: 1.700s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421600/600000: episode: 2108, duration: 1.675s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 421800/600000: episode: 2109, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422000/600000: episode: 2110, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422200/600000: episode: 2111, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422400/600000: episode: 2112, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422600/600000: episode: 2113, duration: 1.303s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 422800/600000: episode: 2114, duration: 1.384s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423000/600000: episode: 2115, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423200/600000: episode: 2116, duration: 1.936s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423400/600000: episode: 2117, duration: 1.458s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423600/600000: episode: 2118, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 423800/600000: episode: 2119, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424000/600000: episode: 2120, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424200/600000: episode: 2121, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424400/600000: episode: 2122, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424600/600000: episode: 2123, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 424800/600000: episode: 2124, duration: 1.488s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425000/600000: episode: 2125, duration: 1.946s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425200/600000: episode: 2126, duration: 1.588s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425400/600000: episode: 2127, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425600/600000: episode: 2128, duration: 1.733s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 425800/600000: episode: 2129, duration: 2.057s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426000/600000: episode: 2130, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426200/600000: episode: 2131, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426400/600000: episode: 2132, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426600/600000: episode: 2133, duration: 1.559s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 426800/600000: episode: 2134, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427000/600000: episode: 2135, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427200/600000: episode: 2136, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427400/600000: episode: 2137, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427600/600000: episode: 2138, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 427800/600000: episode: 2139, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428000/600000: episode: 2140, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428200/600000: episode: 2141, duration: 1.914s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428400/600000: episode: 2142, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428600/600000: episode: 2143, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 428800/600000: episode: 2144, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429000/600000: episode: 2145, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429200/600000: episode: 2146, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429400/600000: episode: 2147, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429600/600000: episode: 2148, duration: 1.418s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 429800/600000: episode: 2149, duration: 2.005s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430000/600000: episode: 2150, duration: 2.252s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430200/600000: episode: 2151, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430400/600000: episode: 2152, duration: 1.770s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430600/600000: episode: 2153, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 430800/600000: episode: 2154, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431000/600000: episode: 2155, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431200/600000: episode: 2156, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431400/600000: episode: 2157, duration: 2.348s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431600/600000: episode: 2158, duration: 2.211s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 431800/600000: episode: 2159, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432000/600000: episode: 2160, duration: 1.265s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432200/600000: episode: 2161, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432400/600000: episode: 2162, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432600/600000: episode: 2163, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 432800/600000: episode: 2164, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433000/600000: episode: 2165, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433200/600000: episode: 2166, duration: 1.694s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433400/600000: episode: 2167, duration: 1.399s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433600/600000: episode: 2168, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 433800/600000: episode: 2169, duration: 1.449s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434000/600000: episode: 2170, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434200/600000: episode: 2171, duration: 1.697s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434400/600000: episode: 2172, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434600/600000: episode: 2173, duration: 1.961s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 434800/600000: episode: 2174, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435000/600000: episode: 2175, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435200/600000: episode: 2176, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435400/600000: episode: 2177, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435600/600000: episode: 2178, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 435800/600000: episode: 2179, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436000/600000: episode: 2180, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436200/600000: episode: 2181, duration: 2.010s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436400/600000: episode: 2182, duration: 1.555s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436600/600000: episode: 2183, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 436800/600000: episode: 2184, duration: 1.356s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437000/600000: episode: 2185, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437200/600000: episode: 2186, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437400/600000: episode: 2187, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437600/600000: episode: 2188, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 437800/600000: episode: 2189, duration: 1.719s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438000/600000: episode: 2190, duration: 2.360s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438200/600000: episode: 2191, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438400/600000: episode: 2192, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438600/600000: episode: 2193, duration: 1.618s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 438800/600000: episode: 2194, duration: 1.470s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439000/600000: episode: 2195, duration: 1.700s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439200/600000: episode: 2196, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439400/600000: episode: 2197, duration: 1.734s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439600/600000: episode: 2198, duration: 1.663s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 439800/600000: episode: 2199, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440000/600000: episode: 2200, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440200/600000: episode: 2201, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440400/600000: episode: 2202, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440600/600000: episode: 2203, duration: 1.489s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 440800/600000: episode: 2204, duration: 1.992s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441000/600000: episode: 2205, duration: 3.617s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441200/600000: episode: 2206, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441400/600000: episode: 2207, duration: 1.452s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441600/600000: episode: 2208, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 441800/600000: episode: 2209, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442000/600000: episode: 2210, duration: 1.747s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442200/600000: episode: 2211, duration: 1.602s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442400/600000: episode: 2212, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442600/600000: episode: 2213, duration: 1.660s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 442800/600000: episode: 2214, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443000/600000: episode: 2215, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443200/600000: episode: 2216, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443400/600000: episode: 2217, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443600/600000: episode: 2218, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 443800/600000: episode: 2219, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444000/600000: episode: 2220, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444200/600000: episode: 2221, duration: 2.007s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444400/600000: episode: 2222, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444600/600000: episode: 2223, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 444800/600000: episode: 2224, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445000/600000: episode: 2225, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445200/600000: episode: 2226, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445400/600000: episode: 2227, duration: 1.639s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445600/600000: episode: 2228, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 445800/600000: episode: 2229, duration: 1.654s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446000/600000: episode: 2230, duration: 1.710s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446200/600000: episode: 2231, duration: 1.706s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446400/600000: episode: 2232, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446600/600000: episode: 2233, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 446800/600000: episode: 2234, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447000/600000: episode: 2235, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447200/600000: episode: 2236, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447400/600000: episode: 2237, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447600/600000: episode: 2238, duration: 2.256s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 447800/600000: episode: 2239, duration: 1.966s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448000/600000: episode: 2240, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448200/600000: episode: 2241, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448400/600000: episode: 2242, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448600/600000: episode: 2243, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 448800/600000: episode: 2244, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449000/600000: episode: 2245, duration: 2.318s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449200/600000: episode: 2246, duration: 2.657s, episode steps: 200, steps per second:  75, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449400/600000: episode: 2247, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449600/600000: episode: 2248, duration: 1.492s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 449800/600000: episode: 2249, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450000/600000: episode: 2250, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450200/600000: episode: 2251, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450400/600000: episode: 2252, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450600/600000: episode: 2253, duration: 1.680s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 450800/600000: episode: 2254, duration: 1.781s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451000/600000: episode: 2255, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451200/600000: episode: 2256, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451400/600000: episode: 2257, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451600/600000: episode: 2258, duration: 1.775s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 451800/600000: episode: 2259, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452000/600000: episode: 2260, duration: 1.469s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452200/600000: episode: 2261, duration: 1.554s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452400/600000: episode: 2262, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452600/600000: episode: 2263, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 452800/600000: episode: 2264, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453000/600000: episode: 2265, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453200/600000: episode: 2266, duration: 1.271s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453400/600000: episode: 2267, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453600/600000: episode: 2268, duration: 1.365s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 453800/600000: episode: 2269, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454000/600000: episode: 2270, duration: 2.132s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454200/600000: episode: 2271, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454400/600000: episode: 2272, duration: 1.695s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454600/600000: episode: 2273, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 454800/600000: episode: 2274, duration: 2.241s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455000/600000: episode: 2275, duration: 1.728s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455200/600000: episode: 2276, duration: 2.231s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455400/600000: episode: 2277, duration: 1.938s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455600/600000: episode: 2278, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 455800/600000: episode: 2279, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456000/600000: episode: 2280, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456200/600000: episode: 2281, duration: 1.279s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456400/600000: episode: 2282, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456600/600000: episode: 2283, duration: 1.664s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 456800/600000: episode: 2284, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457000/600000: episode: 2285, duration: 1.830s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457200/600000: episode: 2286, duration: 1.516s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457400/600000: episode: 2287, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457600/600000: episode: 2288, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 457800/600000: episode: 2289, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458000/600000: episode: 2290, duration: 1.388s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458200/600000: episode: 2291, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458400/600000: episode: 2292, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458600/600000: episode: 2293, duration: 1.538s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 458800/600000: episode: 2294, duration: 1.957s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459000/600000: episode: 2295, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459200/600000: episode: 2296, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459400/600000: episode: 2297, duration: 1.458s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459600/600000: episode: 2298, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 459800/600000: episode: 2299, duration: 1.457s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460000/600000: episode: 2300, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460200/600000: episode: 2301, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460400/600000: episode: 2302, duration: 1.955s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460600/600000: episode: 2303, duration: 1.481s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 460800/600000: episode: 2304, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461000/600000: episode: 2305, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461200/600000: episode: 2306, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461400/600000: episode: 2307, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461600/600000: episode: 2308, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 461800/600000: episode: 2309, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462000/600000: episode: 2310, duration: 1.548s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462200/600000: episode: 2311, duration: 1.961s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462400/600000: episode: 2312, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462600/600000: episode: 2313, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 462800/600000: episode: 2314, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463000/600000: episode: 2315, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463200/600000: episode: 2316, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463400/600000: episode: 2317, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463600/600000: episode: 2318, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 463800/600000: episode: 2319, duration: 2.856s, episode steps: 200, steps per second:  70, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464000/600000: episode: 2320, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464200/600000: episode: 2321, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464400/600000: episode: 2322, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464600/600000: episode: 2323, duration: 1.527s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 464800/600000: episode: 2324, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465000/600000: episode: 2325, duration: 2.056s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465200/600000: episode: 2326, duration: 2.496s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465400/600000: episode: 2327, duration: 1.669s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465600/600000: episode: 2328, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 465800/600000: episode: 2329, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466000/600000: episode: 2330, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466200/600000: episode: 2331, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466400/600000: episode: 2332, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466600/600000: episode: 2333, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 466800/600000: episode: 2334, duration: 2.310s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467000/600000: episode: 2335, duration: 1.610s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467200/600000: episode: 2336, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467400/600000: episode: 2337, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467600/600000: episode: 2338, duration: 1.347s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 467800/600000: episode: 2339, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468000/600000: episode: 2340, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468200/600000: episode: 2341, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468400/600000: episode: 2342, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468600/600000: episode: 2343, duration: 1.605s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 468800/600000: episode: 2344, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469000/600000: episode: 2345, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469200/600000: episode: 2346, duration: 1.396s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469400/600000: episode: 2347, duration: 1.438s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469600/600000: episode: 2348, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 469800/600000: episode: 2349, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470000/600000: episode: 2350, duration: 1.610s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470200/600000: episode: 2351, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470400/600000: episode: 2352, duration: 1.365s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470600/600000: episode: 2353, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 470800/600000: episode: 2354, duration: 1.737s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471000/600000: episode: 2355, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471200/600000: episode: 2356, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471400/600000: episode: 2357, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471600/600000: episode: 2358, duration: 1.590s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 471800/600000: episode: 2359, duration: 2.361s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472000/600000: episode: 2360, duration: 2.020s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472200/600000: episode: 2361, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472400/600000: episode: 2362, duration: 1.509s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472600/600000: episode: 2363, duration: 1.452s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 472800/600000: episode: 2364, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473000/600000: episode: 2365, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473200/600000: episode: 2366, duration: 3.388s, episode steps: 200, steps per second:  59, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473400/600000: episode: 2367, duration: 1.607s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473600/600000: episode: 2368, duration: 1.526s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 473800/600000: episode: 2369, duration: 1.507s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474000/600000: episode: 2370, duration: 2.029s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474200/600000: episode: 2371, duration: 1.472s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474400/600000: episode: 2372, duration: 1.605s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474600/600000: episode: 2373, duration: 2.131s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 474800/600000: episode: 2374, duration: 1.604s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475000/600000: episode: 2375, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475200/600000: episode: 2376, duration: 1.427s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475400/600000: episode: 2377, duration: 1.401s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475600/600000: episode: 2378, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 475800/600000: episode: 2379, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476000/600000: episode: 2380, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476200/600000: episode: 2381, duration: 1.977s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476400/600000: episode: 2382, duration: 1.489s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476600/600000: episode: 2383, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 476800/600000: episode: 2384, duration: 1.445s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477000/600000: episode: 2385, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477200/600000: episode: 2386, duration: 1.549s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477400/600000: episode: 2387, duration: 1.457s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477600/600000: episode: 2388, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 477800/600000: episode: 2389, duration: 1.979s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478000/600000: episode: 2390, duration: 1.648s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478200/600000: episode: 2391, duration: 1.365s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478400/600000: episode: 2392, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478600/600000: episode: 2393, duration: 1.402s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 478800/600000: episode: 2394, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479000/600000: episode: 2395, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479200/600000: episode: 2396, duration: 1.398s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479400/600000: episode: 2397, duration: 1.752s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479600/600000: episode: 2398, duration: 1.769s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 479800/600000: episode: 2399, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480000/600000: episode: 2400, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480200/600000: episode: 2401, duration: 1.465s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480400/600000: episode: 2402, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480600/600000: episode: 2403, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 480800/600000: episode: 2404, duration: 1.462s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481000/600000: episode: 2405, duration: 1.539s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481200/600000: episode: 2406, duration: 2.028s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481400/600000: episode: 2407, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481600/600000: episode: 2408, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 481800/600000: episode: 2409, duration: 1.439s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482000/600000: episode: 2410, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482200/600000: episode: 2411, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482400/600000: episode: 2412, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482600/600000: episode: 2413, duration: 1.433s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 482800/600000: episode: 2414, duration: 1.988s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483000/600000: episode: 2415, duration: 1.571s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483200/600000: episode: 2416, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483400/600000: episode: 2417, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483600/600000: episode: 2418, duration: 1.723s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 483800/600000: episode: 2419, duration: 1.399s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484000/600000: episode: 2420, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484200/600000: episode: 2421, duration: 1.815s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484400/600000: episode: 2422, duration: 2.970s, episode steps: 200, steps per second:  67, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484600/600000: episode: 2423, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 484800/600000: episode: 2424, duration: 1.929s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485000/600000: episode: 2425, duration: 1.559s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485200/600000: episode: 2426, duration: 1.699s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485400/600000: episode: 2427, duration: 2.081s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485600/600000: episode: 2428, duration: 3.011s, episode steps: 200, steps per second:  66, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 485800/600000: episode: 2429, duration: 1.654s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486000/600000: episode: 2430, duration: 1.954s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486200/600000: episode: 2431, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486400/600000: episode: 2432, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486600/600000: episode: 2433, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 486800/600000: episode: 2434, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487000/600000: episode: 2435, duration: 1.594s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487200/600000: episode: 2436, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487400/600000: episode: 2437, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487600/600000: episode: 2438, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 487800/600000: episode: 2439, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488000/600000: episode: 2440, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488200/600000: episode: 2441, duration: 1.584s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488400/600000: episode: 2442, duration: 1.475s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488600/600000: episode: 2443, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 488800/600000: episode: 2444, duration: 1.973s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489000/600000: episode: 2445, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489200/600000: episode: 2446, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489400/600000: episode: 2447, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489600/600000: episode: 2448, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 489800/600000: episode: 2449, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490000/600000: episode: 2450, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490200/600000: episode: 2451, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490400/600000: episode: 2452, duration: 1.679s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490600/600000: episode: 2453, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 490800/600000: episode: 2454, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491000/600000: episode: 2455, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491200/600000: episode: 2456, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491400/600000: episode: 2457, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491600/600000: episode: 2458, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 491800/600000: episode: 2459, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492000/600000: episode: 2460, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492200/600000: episode: 2461, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492400/600000: episode: 2462, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492600/600000: episode: 2463, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 492800/600000: episode: 2464, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493000/600000: episode: 2465, duration: 1.393s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493200/600000: episode: 2466, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493400/600000: episode: 2467, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493600/600000: episode: 2468, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 493800/600000: episode: 2469, duration: 1.492s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494000/600000: episode: 2470, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494200/600000: episode: 2471, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494400/600000: episode: 2472, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494600/600000: episode: 2473, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 494800/600000: episode: 2474, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495000/600000: episode: 2475, duration: 1.297s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495200/600000: episode: 2476, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495400/600000: episode: 2477, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495600/600000: episode: 2478, duration: 1.549s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 495800/600000: episode: 2479, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496000/600000: episode: 2480, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496200/600000: episode: 2481, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496400/600000: episode: 2482, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496600/600000: episode: 2483, duration: 1.684s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 496800/600000: episode: 2484, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497000/600000: episode: 2485, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497200/600000: episode: 2486, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497400/600000: episode: 2487, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497600/600000: episode: 2488, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 497800/600000: episode: 2489, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498000/600000: episode: 2490, duration: 1.585s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498200/600000: episode: 2491, duration: 3.166s, episode steps: 200, steps per second:  63, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498400/600000: episode: 2492, duration: 1.933s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498600/600000: episode: 2493, duration: 2.741s, episode steps: 200, steps per second:  73, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 498800/600000: episode: 2494, duration: 1.946s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499000/600000: episode: 2495, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499200/600000: episode: 2496, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499400/600000: episode: 2497, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499600/600000: episode: 2498, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 499800/600000: episode: 2499, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500000/600000: episode: 2500, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500200/600000: episode: 2501, duration: 1.586s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500400/600000: episode: 2502, duration: 2.090s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500600/600000: episode: 2503, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 500800/600000: episode: 2504, duration: 1.423s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501000/600000: episode: 2505, duration: 1.499s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501200/600000: episode: 2506, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501400/600000: episode: 2507, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501600/600000: episode: 2508, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 501800/600000: episode: 2509, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502000/600000: episode: 2510, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502200/600000: episode: 2511, duration: 2.011s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502400/600000: episode: 2512, duration: 1.784s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502600/600000: episode: 2513, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 502800/600000: episode: 2514, duration: 1.521s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503000/600000: episode: 2515, duration: 1.668s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503200/600000: episode: 2516, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503400/600000: episode: 2517, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503600/600000: episode: 2518, duration: 1.698s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 503800/600000: episode: 2519, duration: 1.746s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504000/600000: episode: 2520, duration: 2.019s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504200/600000: episode: 2521, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504400/600000: episode: 2522, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504600/600000: episode: 2523, duration: 1.727s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 504800/600000: episode: 2524, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505000/600000: episode: 2525, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505200/600000: episode: 2526, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505400/600000: episode: 2527, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505600/600000: episode: 2528, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 505800/600000: episode: 2529, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506000/600000: episode: 2530, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506200/600000: episode: 2531, duration: 1.268s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506400/600000: episode: 2532, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506600/600000: episode: 2533, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 506800/600000: episode: 2534, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507000/600000: episode: 2535, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507200/600000: episode: 2536, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507400/600000: episode: 2537, duration: 1.294s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507600/600000: episode: 2538, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 507800/600000: episode: 2539, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508000/600000: episode: 2540, duration: 1.489s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508200/600000: episode: 2541, duration: 1.938s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508400/600000: episode: 2542, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508600/600000: episode: 2543, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 508800/600000: episode: 2544, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509000/600000: episode: 2545, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509200/600000: episode: 2546, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509400/600000: episode: 2547, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509600/600000: episode: 2548, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 509800/600000: episode: 2549, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510000/600000: episode: 2550, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510200/600000: episode: 2551, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510400/600000: episode: 2552, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510600/600000: episode: 2553, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 510800/600000: episode: 2554, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511000/600000: episode: 2555, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511200/600000: episode: 2556, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511400/600000: episode: 2557, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511600/600000: episode: 2558, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 511800/600000: episode: 2559, duration: 1.880s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512000/600000: episode: 2560, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512200/600000: episode: 2561, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512400/600000: episode: 2562, duration: 1.256s, episode steps: 200, steps per second: 159, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512600/600000: episode: 2563, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 512800/600000: episode: 2564, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513000/600000: episode: 2565, duration: 1.492s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513200/600000: episode: 2566, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513400/600000: episode: 2567, duration: 3.015s, episode steps: 200, steps per second:  66, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513600/600000: episode: 2568, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 513800/600000: episode: 2569, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514000/600000: episode: 2570, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514200/600000: episode: 2571, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514400/600000: episode: 2572, duration: 2.048s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514600/600000: episode: 2573, duration: 2.310s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 514800/600000: episode: 2574, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515000/600000: episode: 2575, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515200/600000: episode: 2576, duration: 1.293s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515400/600000: episode: 2577, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515600/600000: episode: 2578, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 515800/600000: episode: 2579, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516000/600000: episode: 2580, duration: 1.505s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516200/600000: episode: 2581, duration: 1.357s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516400/600000: episode: 2582, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516600/600000: episode: 2583, duration: 1.656s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 516800/600000: episode: 2584, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517000/600000: episode: 2585, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517200/600000: episode: 2586, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517400/600000: episode: 2587, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517600/600000: episode: 2588, duration: 1.489s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 517800/600000: episode: 2589, duration: 1.785s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518000/600000: episode: 2590, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518200/600000: episode: 2591, duration: 2.527s, episode steps: 200, steps per second:  79, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518400/600000: episode: 2592, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518600/600000: episode: 2593, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 518800/600000: episode: 2594, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519000/600000: episode: 2595, duration: 1.388s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519200/600000: episode: 2596, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519400/600000: episode: 2597, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519600/600000: episode: 2598, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 519800/600000: episode: 2599, duration: 2.421s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520000/600000: episode: 2600, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520200/600000: episode: 2601, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520400/600000: episode: 2602, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520600/600000: episode: 2603, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 520800/600000: episode: 2604, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521000/600000: episode: 2605, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521200/600000: episode: 2606, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521400/600000: episode: 2607, duration: 1.982s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521600/600000: episode: 2608, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 521800/600000: episode: 2609, duration: 1.717s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522000/600000: episode: 2610, duration: 1.724s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522200/600000: episode: 2611, duration: 1.320s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522400/600000: episode: 2612, duration: 1.556s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522600/600000: episode: 2613, duration: 2.074s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 522800/600000: episode: 2614, duration: 1.716s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523000/600000: episode: 2615, duration: 2.234s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523200/600000: episode: 2616, duration: 1.820s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523400/600000: episode: 2617, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523600/600000: episode: 2618, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 523800/600000: episode: 2619, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524000/600000: episode: 2620, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524200/600000: episode: 2621, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524400/600000: episode: 2622, duration: 2.655s, episode steps: 200, steps per second:  75, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524600/600000: episode: 2623, duration: 1.620s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 524800/600000: episode: 2624, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525000/600000: episode: 2625, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525200/600000: episode: 2626, duration: 1.394s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525400/600000: episode: 2627, duration: 1.589s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525600/600000: episode: 2628, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 525800/600000: episode: 2629, duration: 1.830s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526000/600000: episode: 2630, duration: 2.965s, episode steps: 200, steps per second:  67, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526200/600000: episode: 2631, duration: 1.906s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526400/600000: episode: 2632, duration: 1.997s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526600/600000: episode: 2633, duration: 1.684s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 526800/600000: episode: 2634, duration: 1.329s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527000/600000: episode: 2635, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527200/600000: episode: 2636, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527400/600000: episode: 2637, duration: 2.497s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527600/600000: episode: 2638, duration: 1.750s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 527800/600000: episode: 2639, duration: 1.320s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528000/600000: episode: 2640, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528200/600000: episode: 2641, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528400/600000: episode: 2642, duration: 1.493s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528600/600000: episode: 2643, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 528800/600000: episode: 2644, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529000/600000: episode: 2645, duration: 2.563s, episode steps: 200, steps per second:  78, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529200/600000: episode: 2646, duration: 1.617s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529400/600000: episode: 2647, duration: 1.626s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529600/600000: episode: 2648, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 529800/600000: episode: 2649, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530000/600000: episode: 2650, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530200/600000: episode: 2651, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530400/600000: episode: 2652, duration: 2.718s, episode steps: 200, steps per second:  74, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530600/600000: episode: 2653, duration: 2.188s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 530800/600000: episode: 2654, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531000/600000: episode: 2655, duration: 1.694s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531200/600000: episode: 2656, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531400/600000: episode: 2657, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531600/600000: episode: 2658, duration: 1.833s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 531800/600000: episode: 2659, duration: 1.612s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532000/600000: episode: 2660, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532200/600000: episode: 2661, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532400/600000: episode: 2662, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532600/600000: episode: 2663, duration: 1.767s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 532800/600000: episode: 2664, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533000/600000: episode: 2665, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533200/600000: episode: 2666, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533400/600000: episode: 2667, duration: 1.467s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533600/600000: episode: 2668, duration: 1.883s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 533800/600000: episode: 2669, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534000/600000: episode: 2670, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534200/600000: episode: 2671, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534400/600000: episode: 2672, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534600/600000: episode: 2673, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 534800/600000: episode: 2674, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535000/600000: episode: 2675, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535200/600000: episode: 2676, duration: 1.552s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535400/600000: episode: 2677, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535600/600000: episode: 2678, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 535800/600000: episode: 2679, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536000/600000: episode: 2680, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536200/600000: episode: 2681, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536400/600000: episode: 2682, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536600/600000: episode: 2683, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 536800/600000: episode: 2684, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537000/600000: episode: 2685, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537200/600000: episode: 2686, duration: 1.758s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537400/600000: episode: 2687, duration: 1.624s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537600/600000: episode: 2688, duration: 1.288s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 537800/600000: episode: 2689, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538000/600000: episode: 2690, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538200/600000: episode: 2691, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538400/600000: episode: 2692, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538600/600000: episode: 2693, duration: 1.384s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 538800/600000: episode: 2694, duration: 1.934s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539000/600000: episode: 2695, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539200/600000: episode: 2696, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539400/600000: episode: 2697, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539600/600000: episode: 2698, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 539800/600000: episode: 2699, duration: 1.494s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540000/600000: episode: 2700, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540200/600000: episode: 2701, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540400/600000: episode: 2702, duration: 1.645s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540600/600000: episode: 2703, duration: 1.767s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 540800/600000: episode: 2704, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541000/600000: episode: 2705, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541200/600000: episode: 2706, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541400/600000: episode: 2707, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541600/600000: episode: 2708, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 541800/600000: episode: 2709, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542000/600000: episode: 2710, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542200/600000: episode: 2711, duration: 1.753s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542400/600000: episode: 2712, duration: 1.657s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542600/600000: episode: 2713, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 542800/600000: episode: 2714, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543000/600000: episode: 2715, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543200/600000: episode: 2716, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543400/600000: episode: 2717, duration: 1.432s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543600/600000: episode: 2718, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 543800/600000: episode: 2719, duration: 1.966s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544000/600000: episode: 2720, duration: 1.991s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544200/600000: episode: 2721, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544400/600000: episode: 2722, duration: 1.553s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544600/600000: episode: 2723, duration: 2.055s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 544800/600000: episode: 2724, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545000/600000: episode: 2725, duration: 1.566s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545200/600000: episode: 2726, duration: 3.040s, episode steps: 200, steps per second:  66, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545400/600000: episode: 2727, duration: 1.520s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545600/600000: episode: 2728, duration: 1.488s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 545800/600000: episode: 2729, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546000/600000: episode: 2730, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546200/600000: episode: 2731, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546400/600000: episode: 2732, duration: 1.481s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546600/600000: episode: 2733, duration: 1.570s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 546800/600000: episode: 2734, duration: 1.973s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547000/600000: episode: 2735, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547200/600000: episode: 2736, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547400/600000: episode: 2737, duration: 1.717s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547600/600000: episode: 2738, duration: 1.394s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 547800/600000: episode: 2739, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548000/600000: episode: 2740, duration: 1.844s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548200/600000: episode: 2741, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548400/600000: episode: 2742, duration: 1.708s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548600/600000: episode: 2743, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 548800/600000: episode: 2744, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549000/600000: episode: 2745, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549200/600000: episode: 2746, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549400/600000: episode: 2747, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549600/600000: episode: 2748, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 549800/600000: episode: 2749, duration: 1.537s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550000/600000: episode: 2750, duration: 1.942s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550200/600000: episode: 2751, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550400/600000: episode: 2752, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550600/600000: episode: 2753, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 550800/600000: episode: 2754, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551000/600000: episode: 2755, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551200/600000: episode: 2756, duration: 1.717s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551400/600000: episode: 2757, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551600/600000: episode: 2758, duration: 2.018s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 551800/600000: episode: 2759, duration: 1.926s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552000/600000: episode: 2760, duration: 1.552s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552200/600000: episode: 2761, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552400/600000: episode: 2762, duration: 1.484s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552600/600000: episode: 2763, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 552800/600000: episode: 2764, duration: 1.339s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553000/600000: episode: 2765, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553200/600000: episode: 2766, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553400/600000: episode: 2767, duration: 1.693s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553600/600000: episode: 2768, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 553800/600000: episode: 2769, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554000/600000: episode: 2770, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554200/600000: episode: 2771, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554400/600000: episode: 2772, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554600/600000: episode: 2773, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 554800/600000: episode: 2774, duration: 1.445s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555000/600000: episode: 2775, duration: 1.975s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555200/600000: episode: 2776, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555400/600000: episode: 2777, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555600/600000: episode: 2778, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 555800/600000: episode: 2779, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556000/600000: episode: 2780, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556200/600000: episode: 2781, duration: 1.547s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556400/600000: episode: 2782, duration: 1.600s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556600/600000: episode: 2783, duration: 1.999s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 556800/600000: episode: 2784, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557000/600000: episode: 2785, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557200/600000: episode: 2786, duration: 1.375s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557400/600000: episode: 2787, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557600/600000: episode: 2788, duration: 1.387s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 557800/600000: episode: 2789, duration: 1.784s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558000/600000: episode: 2790, duration: 1.916s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558200/600000: episode: 2791, duration: 1.989s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558400/600000: episode: 2792, duration: 1.398s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558600/600000: episode: 2793, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 558800/600000: episode: 2794, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559000/600000: episode: 2795, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559200/600000: episode: 2796, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559400/600000: episode: 2797, duration: 1.654s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559600/600000: episode: 2798, duration: 1.713s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 559800/600000: episode: 2799, duration: 2.570s, episode steps: 200, steps per second:  78, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560000/600000: episode: 2800, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560200/600000: episode: 2801, duration: 1.438s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560400/600000: episode: 2802, duration: 1.714s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560600/600000: episode: 2803, duration: 1.717s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 560800/600000: episode: 2804, duration: 1.676s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561000/600000: episode: 2805, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561200/600000: episode: 2806, duration: 2.485s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561400/600000: episode: 2807, duration: 2.052s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561600/600000: episode: 2808, duration: 1.436s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 561800/600000: episode: 2809, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562000/600000: episode: 2810, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562200/600000: episode: 2811, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562400/600000: episode: 2812, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562600/600000: episode: 2813, duration: 1.342s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 562800/600000: episode: 2814, duration: 1.788s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563000/600000: episode: 2815, duration: 1.676s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563200/600000: episode: 2816, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563400/600000: episode: 2817, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563600/600000: episode: 2818, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 563800/600000: episode: 2819, duration: 1.344s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564000/600000: episode: 2820, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564200/600000: episode: 2821, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564400/600000: episode: 2822, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564600/600000: episode: 2823, duration: 1.974s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 564800/600000: episode: 2824, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565000/600000: episode: 2825, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565200/600000: episode: 2826, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565400/600000: episode: 2827, duration: 1.620s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565600/600000: episode: 2828, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 565800/600000: episode: 2829, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566000/600000: episode: 2830, duration: 1.474s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566200/600000: episode: 2831, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566400/600000: episode: 2832, duration: 1.658s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566600/600000: episode: 2833, duration: 1.394s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 566800/600000: episode: 2834, duration: 1.465s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567000/600000: episode: 2835, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567200/600000: episode: 2836, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567400/600000: episode: 2837, duration: 1.613s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567600/600000: episode: 2838, duration: 1.921s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 567800/600000: episode: 2839, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568000/600000: episode: 2840, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568200/600000: episode: 2841, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568400/600000: episode: 2842, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568600/600000: episode: 2843, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 568800/600000: episode: 2844, duration: 1.305s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569000/600000: episode: 2845, duration: 1.826s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569200/600000: episode: 2846, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569400/600000: episode: 2847, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569600/600000: episode: 2848, duration: 1.634s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 569800/600000: episode: 2849, duration: 1.537s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570000/600000: episode: 2850, duration: 1.570s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570200/600000: episode: 2851, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570400/600000: episode: 2852, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570600/600000: episode: 2853, duration: 1.384s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 570800/600000: episode: 2854, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571000/600000: episode: 2855, duration: 2.275s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571200/600000: episode: 2856, duration: 2.027s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571400/600000: episode: 2857, duration: 1.945s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571600/600000: episode: 2858, duration: 1.945s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 571800/600000: episode: 2859, duration: 1.878s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572000/600000: episode: 2860, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572200/600000: episode: 2861, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572400/600000: episode: 2862, duration: 1.790s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572600/600000: episode: 2863, duration: 1.715s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 572800/600000: episode: 2864, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573000/600000: episode: 2865, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573200/600000: episode: 2866, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573400/600000: episode: 2867, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573600/600000: episode: 2868, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 573800/600000: episode: 2869, duration: 1.453s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574000/600000: episode: 2870, duration: 2.103s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574200/600000: episode: 2871, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574400/600000: episode: 2872, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574600/600000: episode: 2873, duration: 1.455s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 574800/600000: episode: 2874, duration: 1.987s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575000/600000: episode: 2875, duration: 1.452s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575200/600000: episode: 2876, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575400/600000: episode: 2877, duration: 1.586s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575600/600000: episode: 2878, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 575800/600000: episode: 2879, duration: 1.376s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576000/600000: episode: 2880, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576200/600000: episode: 2881, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576400/600000: episode: 2882, duration: 1.399s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576600/600000: episode: 2883, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 576800/600000: episode: 2884, duration: 1.330s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577000/600000: episode: 2885, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577200/600000: episode: 2886, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577400/600000: episode: 2887, duration: 1.675s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577600/600000: episode: 2888, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 577800/600000: episode: 2889, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578000/600000: episode: 2890, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578200/600000: episode: 2891, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578400/600000: episode: 2892, duration: 1.396s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578600/600000: episode: 2893, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 578800/600000: episode: 2894, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579000/600000: episode: 2895, duration: 1.938s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579200/600000: episode: 2896, duration: 1.472s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579400/600000: episode: 2897, duration: 1.484s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579600/600000: episode: 2898, duration: 1.669s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 579800/600000: episode: 2899, duration: 1.535s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580000/600000: episode: 2900, duration: 1.563s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580200/600000: episode: 2901, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580400/600000: episode: 2902, duration: 2.143s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580600/600000: episode: 2903, duration: 2.261s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 580800/600000: episode: 2904, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581000/600000: episode: 2905, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581200/600000: episode: 2906, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581400/600000: episode: 2907, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581600/600000: episode: 2908, duration: 1.283s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 581800/600000: episode: 2909, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582000/600000: episode: 2910, duration: 1.490s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582200/600000: episode: 2911, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582400/600000: episode: 2912, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582600/600000: episode: 2913, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 582800/600000: episode: 2914, duration: 1.906s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583000/600000: episode: 2915, duration: 1.477s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583200/600000: episode: 2916, duration: 1.689s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583400/600000: episode: 2917, duration: 1.684s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583600/600000: episode: 2918, duration: 1.944s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 583800/600000: episode: 2919, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584000/600000: episode: 2920, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584200/600000: episode: 2921, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584400/600000: episode: 2922, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584600/600000: episode: 2923, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 584800/600000: episode: 2924, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585000/600000: episode: 2925, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585200/600000: episode: 2926, duration: 1.601s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585400/600000: episode: 2927, duration: 1.909s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585600/600000: episode: 2928, duration: 1.337s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 585800/600000: episode: 2929, duration: 1.730s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586000/600000: episode: 2930, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586200/600000: episode: 2931, duration: 1.306s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586400/600000: episode: 2932, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586600/600000: episode: 2933, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 586800/600000: episode: 2934, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587000/600000: episode: 2935, duration: 1.992s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587200/600000: episode: 2936, duration: 1.577s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587400/600000: episode: 2937, duration: 1.365s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587600/600000: episode: 2938, duration: 1.366s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 587800/600000: episode: 2939, duration: 1.280s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588000/600000: episode: 2940, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588200/600000: episode: 2941, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588400/600000: episode: 2942, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588600/600000: episode: 2943, duration: 1.485s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 588800/600000: episode: 2944, duration: 1.940s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589000/600000: episode: 2945, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589200/600000: episode: 2946, duration: 1.353s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589400/600000: episode: 2947, duration: 1.347s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589600/600000: episode: 2948, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 589800/600000: episode: 2949, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590000/600000: episode: 2950, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590200/600000: episode: 2951, duration: 1.346s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590400/600000: episode: 2952, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590600/600000: episode: 2953, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 590800/600000: episode: 2954, duration: 1.514s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591000/600000: episode: 2955, duration: 1.530s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591200/600000: episode: 2956, duration: 1.467s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591400/600000: episode: 2957, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591600/600000: episode: 2958, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 591800/600000: episode: 2959, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592000/600000: episode: 2960, duration: 1.781s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592200/600000: episode: 2961, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592400/600000: episode: 2962, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592600/600000: episode: 2963, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 592800/600000: episode: 2964, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593000/600000: episode: 2965, duration: 1.289s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593200/600000: episode: 2966, duration: 1.356s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593400/600000: episode: 2967, duration: 1.334s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593600/600000: episode: 2968, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 593800/600000: episode: 2969, duration: 1.948s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594000/600000: episode: 2970, duration: 1.530s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594200/600000: episode: 2971, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594400/600000: episode: 2972, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594600/600000: episode: 2973, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 594800/600000: episode: 2974, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595000/600000: episode: 2975, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595200/600000: episode: 2976, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595400/600000: episode: 2977, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595600/600000: episode: 2978, duration: 1.960s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 595800/600000: episode: 2979, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596000/600000: episode: 2980, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596200/600000: episode: 2981, duration: 1.381s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596400/600000: episode: 2982, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596600/600000: episode: 2983, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 596800/600000: episode: 2984, duration: 1.390s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597000/600000: episode: 2985, duration: 1.512s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597200/600000: episode: 2986, duration: 1.779s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597400/600000: episode: 2987, duration: 1.736s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597600/600000: episode: 2988, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 597800/600000: episode: 2989, duration: 1.292s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598000/600000: episode: 2990, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598200/600000: episode: 2991, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598400/600000: episode: 2992, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598600/600000: episode: 2993, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 598800/600000: episode: 2994, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599000/600000: episode: 2995, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599200/600000: episode: 2996, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599400/600000: episode: 2997, duration: 1.336s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599600/600000: episode: 2998, duration: 1.338s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 599800/600000: episode: 2999, duration: 1.327s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            " 600000/600000: episode: 3000, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "done, took 4686.903 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejemplo de construcción de una red neuronal y entrenamiento con SARSA\n",
        "\n",
        "########################## Creación del Ambiente ##########################\n",
        "env = gym.make('MountainCar-v0') #crea el ambiente utilizando gym.\n",
        "np.random.seed(613) #semilla aleatoria\n",
        "env.seed(613)\n",
        "nb_actions = env.action_space.n #obtiene la cantidad de acciones (3)\n",
        "\n",
        "###################### Creación de la Red Neuronal ########################\n",
        "# Modelo secuencial, una capa después de otra\n",
        "model = Sequential()\n",
        "\n",
        "#Capa de Entrada\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "\n",
        "#Capas \"Escondidas\". Aquí se pueden añadir varias capas con distinta cantidad de neuronas\n",
        "model.add(Dense(200,activation='relu'))\n",
        "\n",
        "#Capa de Salida\n",
        "model.add(Dense(nb_actions,activation='relu'))\n",
        "\n",
        "print(model.summary()) #muestra las neuronas y parámetros de la red\n",
        "\n",
        "###################### Configuración del Modelo ##########################\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1) #límite de la memoria\n",
        "policy = BoltzmannQPolicy() #política inicial\n",
        "\n",
        "#se crea el agente de SARSA\n",
        "sarsa_solucion_5 = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=100, policy=policy)\n",
        "sarsa_solucion_5.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "\n",
        "model.save_weights('model_sarsa_solucion_5_valoresiniciales.h5') #guarda los valores iniciales\n",
        "\n",
        "############################ Entrenamiento ###############################\n",
        "#Se entrena el agente por 10,000 pasos de tiempo y se guardan los resultados\n",
        "\n",
        "#carga los pesos iniciales para entrenar desde 0 (buena practica)\n",
        "sarsa_solucion_5.load_weights('model_sarsa_solucion_5_valoresiniciales.h5')\n",
        "\n",
        "#Comienza a entrenar\n",
        "#Guarda el historial y muestra progreso en pantalla\n",
        "historial_solucion_sarsa_ejemplo_5 = sarsa_solucion_5.fit(env, nb_steps=600000, visualize=False, verbose=2)\n",
        "\n",
        "#guarda los pesos finales (buena practica)\n",
        "sarsa_solucion_5.save_weights('model_sarsa_solucion_5_valoresfinales.h5f', overwrite=True)"
      ],
      "metadata": {
        "id": "kG02Nr6a5aWz",
        "outputId": "45ddc0a8-4deb-44de-e3fc-63335e52daa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_11 (Flatten)        (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 200)               600       \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 3)                 603       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1203 (4.70 KB)\n",
            "Trainable params: 1203 (4.70 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 600000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    200/600000: episode: 1, duration: 3.202s, episode steps: 200, steps per second:  62, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "    400/600000: episode: 2, duration: 1.595s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "    600/600000: episode: 3, duration: 1.321s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "    800/600000: episode: 4, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1000/600000: episode: 5, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1200/600000: episode: 6, duration: 1.300s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1400/600000: episode: 7, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1600/600000: episode: 8, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   1800/600000: episode: 9, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2000/600000: episode: 10, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2200/600000: episode: 11, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2400/600000: episode: 12, duration: 1.383s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2600/600000: episode: 13, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   2800/600000: episode: 14, duration: 1.399s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3000/600000: episode: 15, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3200/600000: episode: 16, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3400/600000: episode: 17, duration: 1.452s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3600/600000: episode: 18, duration: 1.810s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   3800/600000: episode: 19, duration: 1.669s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4000/600000: episode: 20, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4200/600000: episode: 21, duration: 1.416s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4400/600000: episode: 22, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4600/600000: episode: 23, duration: 1.386s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   4800/600000: episode: 24, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5000/600000: episode: 25, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5200/600000: episode: 26, duration: 1.667s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5400/600000: episode: 27, duration: 1.820s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5600/600000: episode: 28, duration: 1.278s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   5800/600000: episode: 29, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6000/600000: episode: 30, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6200/600000: episode: 31, duration: 1.331s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6400/600000: episode: 32, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6600/600000: episode: 33, duration: 1.362s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   6800/600000: episode: 34, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7000/600000: episode: 35, duration: 1.737s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7200/600000: episode: 36, duration: 1.672s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7400/600000: episode: 37, duration: 1.398s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7600/600000: episode: 38, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   7800/600000: episode: 39, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8000/600000: episode: 40, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8200/600000: episode: 41, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8400/600000: episode: 42, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8600/600000: episode: 43, duration: 1.512s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   8800/600000: episode: 44, duration: 2.026s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9000/600000: episode: 45, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9200/600000: episode: 46, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9400/600000: episode: 47, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9600/600000: episode: 48, duration: 1.445s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "   9800/600000: episode: 49, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10000/600000: episode: 50, duration: 1.475s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10200/600000: episode: 51, duration: 1.378s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10400/600000: episode: 52, duration: 2.038s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10600/600000: episode: 53, duration: 1.500s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  10800/600000: episode: 54, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11000/600000: episode: 55, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11200/600000: episode: 56, duration: 1.440s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11400/600000: episode: 57, duration: 1.401s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11600/600000: episode: 58, duration: 1.454s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  11800/600000: episode: 59, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12000/600000: episode: 60, duration: 1.808s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12200/600000: episode: 61, duration: 1.683s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12400/600000: episode: 62, duration: 1.377s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12600/600000: episode: 63, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  12800/600000: episode: 64, duration: 1.371s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13000/600000: episode: 65, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13200/600000: episode: 66, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13400/600000: episode: 67, duration: 1.458s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13600/600000: episode: 68, duration: 1.659s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  13800/600000: episode: 69, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14000/600000: episode: 70, duration: 1.338s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14200/600000: episode: 71, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14400/600000: episode: 72, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14600/600000: episode: 73, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  14800/600000: episode: 74, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15000/600000: episode: 75, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15200/600000: episode: 76, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15400/600000: episode: 77, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15600/600000: episode: 78, duration: 1.715s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  15800/600000: episode: 79, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16000/600000: episode: 80, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16200/600000: episode: 81, duration: 1.315s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16400/600000: episode: 82, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16600/600000: episode: 83, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  16800/600000: episode: 84, duration: 1.507s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17000/600000: episode: 85, duration: 1.624s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17200/600000: episode: 86, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17400/600000: episode: 87, duration: 1.455s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17600/600000: episode: 88, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  17800/600000: episode: 89, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18000/600000: episode: 90, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18200/600000: episode: 91, duration: 1.436s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18400/600000: episode: 92, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18600/600000: episode: 93, duration: 1.318s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  18800/600000: episode: 94, duration: 1.968s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19000/600000: episode: 95, duration: 1.583s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19200/600000: episode: 96, duration: 1.325s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19400/600000: episode: 97, duration: 1.355s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19600/600000: episode: 98, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  19800/600000: episode: 99, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20000/600000: episode: 100, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20200/600000: episode: 101, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20400/600000: episode: 102, duration: 1.536s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20600/600000: episode: 103, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  20800/600000: episode: 104, duration: 1.299s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21000/600000: episode: 105, duration: 1.308s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21200/600000: episode: 106, duration: 1.364s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21400/600000: episode: 107, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21600/600000: episode: 108, duration: 1.564s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  21800/600000: episode: 109, duration: 1.360s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22000/600000: episode: 110, duration: 1.515s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22200/600000: episode: 111, duration: 2.534s, episode steps: 200, steps per second:  79, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22400/600000: episode: 112, duration: 1.676s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22600/600000: episode: 113, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  22800/600000: episode: 114, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23000/600000: episode: 115, duration: 1.370s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23200/600000: episode: 116, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23400/600000: episode: 117, duration: 1.392s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23600/600000: episode: 118, duration: 1.534s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  23800/600000: episode: 119, duration: 2.067s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24000/600000: episode: 120, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24200/600000: episode: 121, duration: 1.350s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24400/600000: episode: 122, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24600/600000: episode: 123, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  24800/600000: episode: 124, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25000/600000: episode: 125, duration: 1.402s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25200/600000: episode: 126, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25400/600000: episode: 127, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25600/600000: episode: 128, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  25800/600000: episode: 129, duration: 1.543s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26000/600000: episode: 130, duration: 1.384s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26200/600000: episode: 131, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26400/600000: episode: 132, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26600/600000: episode: 133, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  26800/600000: episode: 134, duration: 1.379s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27000/600000: episode: 135, duration: 1.737s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27200/600000: episode: 136, duration: 1.804s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27400/600000: episode: 137, duration: 1.404s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27600/600000: episode: 138, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  27800/600000: episode: 139, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28000/600000: episode: 140, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28200/600000: episode: 141, duration: 1.359s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28400/600000: episode: 142, duration: 1.329s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28600/600000: episode: 143, duration: 1.372s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  28800/600000: episode: 144, duration: 1.926s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29000/600000: episode: 145, duration: 1.487s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29200/600000: episode: 146, duration: 1.774s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29400/600000: episode: 147, duration: 1.520s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29600/600000: episode: 148, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  29800/600000: episode: 149, duration: 1.428s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30000/600000: episode: 150, duration: 1.333s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30200/600000: episode: 151, duration: 1.367s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30400/600000: episode: 152, duration: 2.007s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30600/600000: episode: 153, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  30800/600000: episode: 154, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31000/600000: episode: 155, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31200/600000: episode: 156, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31400/600000: episode: 157, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31600/600000: episode: 158, duration: 1.323s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  31800/600000: episode: 159, duration: 1.361s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32000/600000: episode: 160, duration: 1.686s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32200/600000: episode: 161, duration: 2.395s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32400/600000: episode: 162, duration: 1.316s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32600/600000: episode: 163, duration: 1.985s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  32800/600000: episode: 164, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33000/600000: episode: 165, duration: 1.281s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33200/600000: episode: 166, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33400/600000: episode: 167, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33600/600000: episode: 168, duration: 2.928s, episode steps: 200, steps per second:  68, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  33800/600000: episode: 169, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34000/600000: episode: 170, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34200/600000: episode: 171, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34400/600000: episode: 172, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34600/600000: episode: 173, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  34800/600000: episode: 174, duration: 1.314s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35000/600000: episode: 175, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35200/600000: episode: 176, duration: 1.779s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35400/600000: episode: 177, duration: 1.802s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35600/600000: episode: 178, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  35800/600000: episode: 179, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36000/600000: episode: 180, duration: 1.399s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36200/600000: episode: 181, duration: 1.540s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36400/600000: episode: 182, duration: 1.634s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36600/600000: episode: 183, duration: 1.570s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  36800/600000: episode: 184, duration: 2.280s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37000/600000: episode: 185, duration: 1.911s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37200/600000: episode: 186, duration: 1.768s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37400/600000: episode: 187, duration: 1.561s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37600/600000: episode: 188, duration: 1.501s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  37800/600000: episode: 189, duration: 1.631s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38000/600000: episode: 190, duration: 1.574s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38200/600000: episode: 191, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38400/600000: episode: 192, duration: 2.025s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38600/600000: episode: 193, duration: 1.690s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  38800/600000: episode: 194, duration: 1.539s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39000/600000: episode: 195, duration: 1.816s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39200/600000: episode: 196, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39400/600000: episode: 197, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39600/600000: episode: 198, duration: 1.694s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  39800/600000: episode: 199, duration: 2.089s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40000/600000: episode: 200, duration: 1.517s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40200/600000: episode: 201, duration: 1.508s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40400/600000: episode: 202, duration: 1.486s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40600/600000: episode: 203, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  40800/600000: episode: 204, duration: 1.629s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41000/600000: episode: 205, duration: 1.700s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41200/600000: episode: 206, duration: 2.112s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41400/600000: episode: 207, duration: 1.725s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41600/600000: episode: 208, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  41800/600000: episode: 209, duration: 1.541s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42000/600000: episode: 210, duration: 1.506s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42200/600000: episode: 211, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42400/600000: episode: 212, duration: 1.537s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42600/600000: episode: 213, duration: 1.683s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  42800/600000: episode: 214, duration: 2.264s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43000/600000: episode: 215, duration: 1.516s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43200/600000: episode: 216, duration: 1.441s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43400/600000: episode: 217, duration: 1.345s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43600/600000: episode: 218, duration: 1.343s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  43800/600000: episode: 219, duration: 1.349s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44000/600000: episode: 220, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44200/600000: episode: 221, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44400/600000: episode: 222, duration: 2.074s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44600/600000: episode: 223, duration: 1.525s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  44800/600000: episode: 224, duration: 1.341s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45000/600000: episode: 225, duration: 1.347s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45200/600000: episode: 226, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45400/600000: episode: 227, duration: 1.326s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45600/600000: episode: 228, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  45800/600000: episode: 229, duration: 1.295s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46000/600000: episode: 230, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46200/600000: episode: 231, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46400/600000: episode: 232, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46600/600000: episode: 233, duration: 1.309s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  46800/600000: episode: 234, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47000/600000: episode: 235, duration: 1.354s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47200/600000: episode: 236, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47400/600000: episode: 237, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47600/600000: episode: 238, duration: 1.275s, episode steps: 200, steps per second: 157, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  47800/600000: episode: 239, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48000/600000: episode: 240, duration: 1.814s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48200/600000: episode: 241, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48400/600000: episode: 242, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48600/600000: episode: 243, duration: 1.291s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  48800/600000: episode: 244, duration: 1.351s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49000/600000: episode: 245, duration: 1.310s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49200/600000: episode: 246, duration: 1.328s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49400/600000: episode: 247, duration: 1.324s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49600/600000: episode: 248, duration: 1.617s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  49800/600000: episode: 249, duration: 1.749s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50000/600000: episode: 250, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50200/600000: episode: 251, duration: 1.296s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50400/600000: episode: 252, duration: 1.284s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50600/600000: episode: 253, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  50800/600000: episode: 254, duration: 1.264s, episode steps: 200, steps per second: 158, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51000/600000: episode: 255, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51200/600000: episode: 256, duration: 1.285s, episode steps: 200, steps per second: 156, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51400/600000: episode: 257, duration: 1.693s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51600/600000: episode: 258, duration: 1.756s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  51800/600000: episode: 259, duration: 1.302s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52000/600000: episode: 260, duration: 1.322s, episode steps: 200, steps per second: 151, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52200/600000: episode: 261, duration: 1.319s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52400/600000: episode: 262, duration: 1.311s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52600/600000: episode: 263, duration: 1.313s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  52800/600000: episode: 264, duration: 1.287s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53000/600000: episode: 265, duration: 1.340s, episode steps: 200, steps per second: 149, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53200/600000: episode: 266, duration: 1.759s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53400/600000: episode: 267, duration: 1.639s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53600/600000: episode: 268, duration: 1.301s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  53800/600000: episode: 269, duration: 1.290s, episode steps: 200, steps per second: 155, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54000/600000: episode: 270, duration: 1.307s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54200/600000: episode: 271, duration: 1.303s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54400/600000: episode: 272, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54600/600000: episode: 273, duration: 1.317s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  54800/600000: episode: 274, duration: 1.298s, episode steps: 200, steps per second: 154, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55000/600000: episode: 275, duration: 1.815s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55200/600000: episode: 276, duration: 1.549s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n",
            "  55400/600000: episode: 277, duration: 1.312s, episode steps: 200, steps per second: 152, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.500000, mae: 0.333333, mean_q: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################ Validación ###############################\n",
        "sarsa_solucion_1.load_weights('model_sarsa_solucion_1_valoresfinales.h5f') #Carga los pesos del entrenamiento (buena practica)\n",
        "env.reset() #reinicia el ambiente\n",
        "test_episodes = 5  # Número de episodios de validación\n",
        "sarsa_solucion_1.test(env, nb_episodes=test_episodes, visualize=True) #Simula con la política obtenida\n",
        "\n",
        "observation, info = env_test_render.reset() #inicializa el ambiente\n",
        "\n",
        "while True: #simula\n",
        "  action = sarsa_solucion_1.forward(observation) #elige una acción de acuerdo a sarsa 4 (se usan los resultados de la primera red)\n",
        "  observation, reward, terminated, truncated, info = env_test_render.step(action) #obtiene la observación tras tomar la acción\n",
        "  if terminated or truncated: #si pierde o pasan 200 pasos\n",
        "      break #termina\n",
        "\n",
        "env_test_render.play() #muestra el video\n",
        "\n",
        "############################ Validación ###############################\n",
        "sarsa_solucion_2.load_weights('model_sarsa_solucion_2_valoresfinales.h5f') #Carga los pesos del entrenamiento (buena practica)\n",
        "env.reset() #reinicia el ambiente\n",
        "test_episodes = 5  # Número de episodios de validación\n",
        "sarsa_solucion_2.test(env, nb_episodes=test_episodes, visualize=True) #Simula con la política obtenida\n",
        "\n",
        "observation, info = env_test_render.reset() #inicializa el ambiente\n",
        "\n",
        "while True: #simula\n",
        "  action = sarsa_solucion_2.forward(observation) #elige una acción de acuerdo a sarsa 4 (se usan los resultados de la primera red)\n",
        "  observation, reward, terminated, truncated, info = env_test_render.step(action) #obtiene la observación tras tomar la acción\n",
        "  if terminated or truncated: #si pierde o pasan 200 pasos\n",
        "      break #termina\n",
        "\n",
        "env_test_render.play() #muestra el video\n",
        "\n",
        "\n",
        "############################ Validación ###############################\n",
        "sarsa_solucion_3.load_weights('model_sarsa_solucion_3_valoresfinales.h5f') #Carga los pesos del entrenamiento (buena practica)\n",
        "env.reset() #reinicia el ambiente\n",
        "test_episodes = 5  # Número de episodios de validación\n",
        "sarsa_solucion_3.test(env, nb_episodes=test_episodes, visualize=True) #Simula con la política obtenida\n",
        "\n",
        "observation, info = env_test_render.reset() #inicializa el ambiente\n",
        "\n",
        "while True: #simula\n",
        "  action = sarsa_solucion_3.forward(observation) #elige una acción de acuerdo a sarsa 4 (se usan los resultados de la primera red)\n",
        "  observation, reward, terminated, truncated, info = env_test_render.step(action) #obtiene la observación tras tomar la acción\n",
        "  if terminated or truncated: #si pierde o pasan 200 pasos\n",
        "      break #termina\n",
        "\n",
        "env_test_render.play() #muestra el video\n",
        "\n",
        "############################ Validación ###############################\n",
        "sarsa_solucion_4.load_weights('model_sarsa_solucion_4_valoresfinales.h5f') #Carga los pesos del entrenamiento (buena practica)\n",
        "env.reset() #reinicia el ambiente\n",
        "test_episodes = 5  # Número de episodios de validación\n",
        "sarsa_solucion_4.test(env, nb_episodes=test_episodes, visualize=True) #Simula con la política obtenida\n",
        "\n",
        "observation, info = env_test_render.reset() #inicializa el ambiente\n",
        "\n",
        "while True: #simula\n",
        "  action = sarsa_solucion_4.forward(observation) #elige una acción de acuerdo a sarsa 4 (se usan los resultados de la primera red)\n",
        "  observation, reward, terminated, truncated, info = env_test_render.step(action) #obtiene la observación tras tomar la acción\n",
        "  if terminated or truncated: #si pierde o pasan 200 pasos\n",
        "      break #termina\n",
        "\n",
        "env_test_render.play() #muestra el video\n",
        "\n",
        "############################ Validación ###############################\n",
        "sarsa_solucion_5.load_weights('model_sarsa_solucion_5_valoresfinales.h5f') #Carga los pesos del entrenamiento (buena practica)\n",
        "env.reset() #reinicia el ambiente\n",
        "test_episodes = 5  # Número de episodios de validación\n",
        "sarsa_solucion_5.test(env, nb_episodes=test_episodes, visualize=True) #Simula con la política obtenida\n",
        "\n",
        "observation, info = env_test_render.reset() #inicializa el ambiente\n",
        "\n",
        "while True: #simula\n",
        "  action = sarsa_solucion_5.forward(observation) #elige una acción de acuerdo a sarsa 4 (se usan los resultados de la primera red)\n",
        "  observation, reward, terminated, truncated, info = env_test_render.step(action) #obtiene la observación tras tomar la acción\n",
        "  if terminated or truncated: #si pierde o pasan 200 pasos\n",
        "      break #termina\n",
        "\n",
        "env_test_render.play() #muestra el video"
      ],
      "metadata": {
        "id": "TLS6LxtRztbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3LFlj-fz0SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflexiones Finales\n",
        "\n"
      ],
      "metadata": {
        "id": "G5ZVj1xuPWoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias\n",
        "\n",
        "[1] Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. The MIT Press, second edition.\n",
        "\n",
        "[2]\n",
        "\n",
        "[3] Gym Documentation, Mountain Car. `https://gymnasium.farama.org/environments/classic_control/mountain_car/`\n",
        "\n",
        "\n",
        "[4] keras-rl2 Documentation, SARSA. `https://github.com/inarikami/keras-rl2/blob/master/rl/agents/sarsa.py`"
      ],
      "metadata": {
        "id": "HhVGfkzU6KRH"
      }
    }
  ]
}